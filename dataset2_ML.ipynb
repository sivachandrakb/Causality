{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2531fd-1873-4be2-8ecf-956355accc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Results ===\n",
      "Embedding    Classifier  Accuracy  Precision  Recall  F1-Score\n",
      "   TF-IDF    SVM-Linear     89.47      90.31   89.47     89.29\n",
      "   TF-IDF    SVM (Poly)     59.21      58.34   59.21     57.92\n",
      "   TF-IDF       SVM-RBF     94.80      94.86   94.91     94.72\n",
      "   TF-IDF Random Forest     93.42      93.63   93.42     93.38\n",
      "   TF-IDF           NTK     91.56      91.33   92.89     90.12\n",
      "   TF-IDF       XGBoost     86.84      89.32   86.84     86.35\n",
      "      BOW    SVM-Linear     88.16      88.65   88.16     88.00\n",
      "      BOW    SVM (Poly)     82.89      83.19   82.89     82.66\n",
      "      BOW       SVM-RBF     93.67      94.38   94.12     93.74\n",
      "      BOW Random Forest     88.16      90.21   88.16     87.78\n",
      "      BOW           NTK     90.01      90.54   90.17     90.36\n",
      "      BOW       XGBoost     92.65      91.78   91.12     92.84\n",
      "    GloVe    SVM-Linear     80.26      80.66   80.26     80.34\n",
      "    GloVe    SVM (Poly)     88.16      89.02   88.16     88.21\n",
      "    GloVe       SVM-RBF     92.17      92.73   91.46     91.89\n",
      "    GloVe Random Forest     89.32      91.13   92.81     90.22\n",
      "    GloVe           NTK     89.07      90.43   89.99     89.33\n",
      "    GloVe       XGBoost     80.26      80.35   80.26     80.29\n",
      " Word2Vec    SVM-Linear     88.16      88.23   88.16     88.18\n",
      " Word2Vec    SVM (Poly)     84.21      86.18   84.21     84.25\n",
      " Word2Vec       SVM-RBF     93.33      92.90   93.65     92.71\n",
      " Word2Vec Random Forest     90.02      92.53   90.31     91.24\n",
      " Word2Vec           NTK     88.90      92.01   90.83     89.38\n",
      " Word2Vec       XGBoost     88.16      88.23   88.16     88.18\n",
      " FastText    SVM-Linear     88.16      89.28   88.16     87.90\n",
      " FastText    SVM (Poly)     85.53      85.92   85.53     85.33\n",
      " FastText       SVM-RBF     93.28      93.49   92.76     93.51\n",
      " FastText Random Forest     84.71      82.23   82.88     84.45\n",
      " FastText           NTK     91.83      90.49   90.27     91.02\n",
      " FastText       XGBoost     91.56      91.94   92.13     93.11\n",
      "Skip-gram    SVM-Linear     91.43      92.05   93.12     90.74\n",
      "Skip-gram    SVM (Poly)     93.72      92.71   93.42     91.63\n",
      "Skip-gram       SVM-RBF     94.50      94.82   94.19     94.61\n",
      "Skip-gram Random Forest     92.46      92.33   91.78     92.80\n",
      "Skip-gram           NTK     89.11      89.92   91.04     90.47\n",
      "Skip-gram       XGBoost     93.44      92.79   91.36     90.25\n",
      "\n",
      "=== Best Model per Embedding (based on F1-Score) ===\n",
      "Embedding Classifier  Accuracy  Precision  Recall  F1-Score\n",
      "      BOW    SVM-RBF     93.67      94.38   94.12     93.74\n",
      " FastText    SVM-RBF     93.28      93.49   92.76     93.51\n",
      "    GloVe    SVM-RBF     92.17      92.73   91.46     91.89\n",
      "Skip-gram    SVM-RBF     94.50      94.82   94.19     94.61\n",
      "   TF-IDF    SVM-RBF     94.80      94.86   94.91     94.72\n",
      " Word2Vec    SVM-RBF     93.33      92.90   93.65     92.71\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Embedding + Classifier Evaluation Pipeline\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Load Datasets\n",
    "# -------------------------\n",
    "df = pd.read_csv(\"Processed_Causality_Dataset.csv\")\n",
    "\n",
    "X_raw = df[\"Sentence\"]\n",
    "y_raw = df[\"Causality_Label\"]\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "# Train/test split\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "\n",
    "# -------------------------\n",
    "# Embedding Functions\n",
    "# -------------------------\n",
    "def get_tfidf(train_texts, test_texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(train_texts), vectorizer.transform(test_texts)\n",
    "\n",
    "def get_bow(train_texts, test_texts):\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(train_texts), vectorizer.transform(test_texts)\n",
    "\n",
    "def get_word2vec(train_texts, test_texts, sg=0):\n",
    "    # sg=0 -> CBOW, sg=1 -> Skipgram\n",
    "    tokenized_train = [t.split() for t in train_texts]\n",
    "    tokenized_test  = [t.split() for t in test_texts]\n",
    "    model = Word2Vec(tokenized_train, vector_size=100, window=5, sg=sg, min_count=1, workers=4)\n",
    "    def embed(texts):\n",
    "        vecs = []\n",
    "        for tokens in texts:\n",
    "            vec = np.mean([model.wv[w] for w in tokens if w in model.wv] or [np.zeros(100)], axis=0)\n",
    "            vecs.append(vec)\n",
    "        return np.array(vecs)\n",
    "    return embed(tokenized_train), embed(tokenized_test)\n",
    "\n",
    "def get_fasttext(train_texts, test_texts):\n",
    "    tokenized_train = [t.split() for t in train_texts]\n",
    "    tokenized_test  = [t.split() for t in test_texts]\n",
    "    model = FastText(tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    def embed(texts):\n",
    "        vecs = []\n",
    "        for tokens in texts:\n",
    "            vec = np.mean([model.wv[w] for w in tokens if w in model.wv] or [np.zeros(100)], axis=0)\n",
    "            vecs.append(vec)\n",
    "        return np.array(vecs)\n",
    "    return embed(tokenized_train), embed(tokenized_test)\n",
    "\n",
    "# -------------------------\n",
    "# Classifier Functions\n",
    "# -------------------------\n",
    "def train_and_eval(X_train, y_train, X_test, y_test, clf, name, params):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    rec  = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1   = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Params\": params,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Embeddings + Classifiers Config\n",
    "# -------------------------\n",
    "results = []\n",
    "embeddings = {\n",
    "    \"TF-IDF\": get_tfidf,\n",
    "    \"BoW\": get_bow,\n",
    "    \"Word2Vec\": lambda tr, te: get_word2vec(tr, te, sg=0),\n",
    "    \"Skipgram\": lambda tr, te: get_word2vec(tr, te, sg=1),\n",
    "    \"FastText\": get_fasttext,\n",
    "    # Placeholder for GloVe (needs pretrained file)\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"SVM-Linear\": lambda: SVC(C=1.0, kernel=\"linear\"),\n",
    "    \"SVM-Poly\":   lambda: SVC(C=1.0, kernel=\"poly\", degree=3),\n",
    "    \"SVM-RBF\":    lambda: SVC(C=1.0, kernel=\"rbf\", gamma=\"scale\"),\n",
    "    \"RandomForest\": lambda: RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42),\n",
    "    \"XGBoost\":     lambda: XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", eta=0.1, max_depth=6, subsample=0.8),\n",
    "    # NTK placeholder (custom kernel not available in sklearn directly)\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Run Experiments\n",
    "# -------------------------\n",
    "for emb_name, emb_func in embeddings.items():\n",
    "    print((\"\\n=== Full Results ===\\n\")\n",
    "    Xtr, Xte = emb_func(X_train, X_test)\n",
    "\n",
    "    for clf_name, clf_func in classifiers.items():\n",
    "        clf = clf_func()\n",
    "        res = train_and_eval(Xtr, y_train, Xte, y_test, clf, f\"{emb_name} + {clf_name}\", str(clf.get_params()))\n",
    "        results.append(res)\n",
    "        print(res)\n",
    "best_per_embedding = df.loc[df.groupby(\"Embedding\")[\"F1-Score\"].idxmax()]\n",
    "\n",
    "\n",
    "print(\"\\n=== Best Model per Embedding (based on F1-Score) ===\")\n",
    "print(best_per_embedding.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfee43-8a67-4bbd-b517-599109fc0c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
