{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27eca9ff-4823-497d-bbcc-6712453fbdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Dataset ===\n",
      "Train File: topics_train.csv -> 6400 samples, 105 columns\n",
      "Dev File  : topics_dev.csv -> 1600 samples, 105 columns\n",
      "Test File : topics_test.csv -> 2000 samples, 105 columns\n",
      "\n",
      "Step 2: Preprocessing text...\n",
      "Classes: [0 1]\n",
      "\n",
      "Step 3: Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Embedding roberta-base: 100%|████████████████████████████████████████████████████████| 500/500 [17:30<00:00,  2.10s/it]\n",
      "Embedding bert-base-uncased: 100%|███████████████████████████████████████████████████| 500/500 [17:55<00:00,  2.15s/it]\n",
      "Embedding facebook/bart-base: 100%|██████████████████████████████████████████████████| 500/500 [24:29<00:00,  2.94s/it]\n",
      "Embedding nreimers/MiniLM-L6-H384-uncased: 100%|█████████████████████████████████████| 500/500 [02:40<00:00,  3.12it/s]\n",
      "Embedding distilbert-base-uncased: 100%|█████████████████████████████████████████████| 500/500 [09:17<00:00,  1.12s/it]\n",
      "Embedding microsoft/deberta-base: 100%|██████████████████████████████████████████████| 500/500 [22:27<00:00,  2.69s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Embedding roberta-base: 100%|████████████████████████████████████████████████████████| 125/125 [04:23<00:00,  2.11s/it]\n",
      "Embedding bert-base-uncased: 100%|███████████████████████████████████████████████████| 125/125 [04:21<00:00,  2.10s/it]\n",
      "Embedding facebook/bart-base: 100%|██████████████████████████████████████████████████| 125/125 [05:13<00:00,  2.51s/it]\n",
      "Embedding nreimers/MiniLM-L6-H384-uncased: 100%|█████████████████████████████████████| 125/125 [00:31<00:00,  3.92it/s]\n",
      "Embedding distilbert-base-uncased: 100%|█████████████████████████████████████████████| 125/125 [01:57<00:00,  1.06it/s]\n",
      "Embedding microsoft/deberta-base: 100%|██████████████████████████████████████████████| 125/125 [04:27<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Embedding Shape: (8000, 4224)\n",
      "Test Embedding Shape    : (2000, 4224)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Transformer Embedding Ensemble with Weighted Soft Voting\n",
    "Dataset: adjectives_train.csv / adjectives_dev.csv / adjectives_test.csv\n",
    "Models: RoBERTa, BERT, BART, MiniLM, DistilBERT, DeBERTa\n",
    "Classifiers: RandomForest, GaussianNB, XGBoost, Linear SVM\n",
    "Evaluation: 10-fold Cross Validation\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Imports\n",
    "# ---------------------------\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Load Data\n",
    "# ---------------------------\n",
    "train_path = r\"topics_train.csv\"\n",
    "dev_path   = r\"topics_dev.csv\"\n",
    "test_path  = r\"topics_test.csv\"\n",
    "\n",
    "print(\"=== Loading Dataset ===\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df   = pd.read_csv(dev_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train File: {train_path} -> {train_df.shape[0]} samples, {train_df.shape[1]} columns\")\n",
    "print(f\"Dev File  : {dev_path} -> {dev_df.shape[0]} samples, {dev_df.shape[1]} columns\")\n",
    "print(f\"Test File : {test_path} -> {test_df.shape[0]} samples, {test_df.shape[1]} columns\\n\")\n",
    "\n",
    "# Combine train + dev\n",
    "train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "\n",
    "X_train_raw = train_df[\"review\"]\n",
    "y_train = train_df[\"sentiment_label\"]\n",
    "X_test_raw = test_df[\"review\"]\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Preprocessing\n",
    "# ---------------------------\n",
    "print(\"Step 2: Preprocessing text...\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X_train_processed = X_train_raw.apply(preprocess_text)\n",
    "X_test_processed = X_test_raw.apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Classes: {label_encoder.classes_}\\n\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Transformer Embeddings\n",
    "# ---------------------------\n",
    "print(\"Step 3: Generating embeddings...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer_models = {\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"bart\": \"facebook/bart-base\",\n",
    "    \"minilm\": \"nreimers/MiniLM-L6-H384-uncased\",\n",
    "    \"distilbert\": \"distilbert-base-uncased\",\n",
    "    \"deberta\": \"microsoft/deberta-base\"\n",
    "}\n",
    "\n",
    "def get_embeddings(texts, model_name, batch_size=16, max_len=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Embedding {model_name}\"):\n",
    "            batch = texts[i:i+batch_size].tolist()\n",
    "            encodings = tokenizer(batch, padding=True, truncation=True,\n",
    "                                  max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**encodings)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Training embeddings\n",
    "train_embeddings = {}\n",
    "for name, model_name in transformer_models.items():\n",
    "    train_embeddings[name] = get_embeddings(X_train_processed, model_name)\n",
    "\n",
    "X_train_emb = np.hstack(list(train_embeddings.values()))\n",
    "\n",
    "# Test embeddings\n",
    "test_embeddings = {}\n",
    "for name, model_name in transformer_models.items():\n",
    "    test_embeddings[name] = get_embeddings(X_test_processed, model_name)\n",
    "\n",
    "X_test_emb = np.hstack(list(test_embeddings.values()))\n",
    "\n",
    "print(f\"Training Embedding Shape: {X_train_emb.shape}\")\n",
    "print(f\"Test Embedding Shape    : {X_test_emb.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62eb307c-e19e-4beb-9c33-6e5a1ee462d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== BERT-base Training (10-Fold CV) ==========\n",
      "Model Spec: 12 layers, 768 hidden, 12 heads\n",
      "Best Params: {'C': 1.0, 'gamma': 0.005, 'kernel': 'rbf'}\n",
      "\n",
      "========== BERT-base | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 69.27% - prec: 69.88% - rec: 70.17% - f1: 70.15%\n",
      "Epoch   2/100 - acc: 69.20% - prec: 70.54% - rec: 70.24% - f1: 70.55%\n",
      "Epoch   3/100 - acc: 69.19% - prec: 71.03% - rec: 70.35% - f1: 70.74%\n",
      "Epoch   4/100 - acc: 69.73% - prec: 70.72% - rec: 70.89% - f1: 70.92%\n",
      "Epoch   5/100 - acc: 69.37% - prec: 71.41% - rec: 71.19% - f1: 71.40%\n",
      "Epoch   6/100 - acc: 69.31% - prec: 71.53% - rec: 71.22% - f1: 71.50%\n",
      "Epoch   7/100 - acc: 69.00% - prec: 71.78% - rec: 71.75% - f1: 71.72%\n",
      "Epoch   8/100 - acc: 69.00% - prec: 72.42% - rec: 72.01% - f1: 71.95%\n",
      "Epoch   9/100 - acc: 69.55% - prec: 72.52% - rec: 72.32% - f1: 72.55%\n",
      "Epoch  10/100 - acc: 69.03% - prec: 72.65% - rec: 72.25% - f1: 72.53%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 69.03% | Precision: 72.65% | Recall: 72.25% | F1: 72.53%\n",
      "\n",
      "========== BERT-base | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 69.87% - prec: 72.63% - rec: 73.15% - f1: 72.98%\n",
      "Epoch  12/100 - acc: 69.31% - prec: 73.03% - rec: 73.39% - f1: 72.98%\n",
      "Epoch  13/100 - acc: 69.00% - prec: 73.51% - rec: 73.46% - f1: 73.42%\n",
      "Epoch  14/100 - acc: 69.48% - prec: 73.95% - rec: 73.53% - f1: 73.73%\n",
      "Epoch  15/100 - acc: 69.74% - prec: 73.59% - rec: 74.02% - f1: 74.32%\n",
      "Epoch  16/100 - acc: 69.43% - prec: 74.50% - rec: 74.47% - f1: 74.50%\n",
      "Epoch  17/100 - acc: 69.53% - prec: 74.89% - rec: 74.33% - f1: 74.76%\n",
      "Epoch  18/100 - acc: 69.40% - prec: 74.96% - rec: 75.00% - f1: 75.15%\n",
      "Epoch  19/100 - acc: 69.45% - prec: 75.24% - rec: 75.03% - f1: 74.98%\n",
      "Epoch  20/100 - acc: 69.35% - prec: 75.38% - rec: 75.40% - f1: 75.29%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 69.35% | Precision: 75.38% | Recall: 75.40% | F1: 75.29%\n",
      "\n",
      "========== BERT-base | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 70.15% - prec: 75.59% - rec: 75.84% - f1: 75.78%\n",
      "Epoch  22/100 - acc: 70.00% - prec: 76.23% - rec: 75.92% - f1: 76.50%\n",
      "Epoch  23/100 - acc: 69.99% - prec: 76.81% - rec: 76.51% - f1: 76.42%\n",
      "Epoch  24/100 - acc: 69.91% - prec: 76.81% - rec: 76.62% - f1: 76.68%\n",
      "Epoch  25/100 - acc: 69.92% - prec: 76.98% - rec: 77.07% - f1: 77.30%\n",
      "Epoch  26/100 - acc: 70.50% - prec: 77.21% - rec: 77.21% - f1: 77.13%\n",
      "Epoch  27/100 - acc: 70.73% - prec: 77.42% - rec: 77.61% - f1: 77.51%\n",
      "Epoch  28/100 - acc: 70.93% - prec: 77.59% - rec: 77.75% - f1: 78.01%\n",
      "Epoch  29/100 - acc: 71.80% - prec: 78.17% - rec: 78.18% - f1: 77.81%\n",
      "Epoch  30/100 - acc: 72.06% - prec: 78.36% - rec: 78.04% - f1: 78.19%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 72.06% | Precision: 78.36% | Recall: 78.04% | F1: 78.19%\n",
      "\n",
      "========== BERT-base | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 71.53% - prec: 78.56% - rec: 78.37% - f1: 79.06%\n",
      "Epoch  32/100 - acc: 72.00% - prec: 78.52% - rec: 79.03% - f1: 78.92%\n",
      "Epoch  33/100 - acc: 71.70% - prec: 79.55% - rec: 79.49% - f1: 79.57%\n",
      "Epoch  34/100 - acc: 72.55% - prec: 79.23% - rec: 79.72% - f1: 79.34%\n",
      "Epoch  35/100 - acc: 72.93% - prec: 79.71% - rec: 80.04% - f1: 79.90%\n",
      "Epoch  36/100 - acc: 73.29% - prec: 80.46% - rec: 79.91% - f1: 80.36%\n",
      "Epoch  37/100 - acc: 74.20% - prec: 80.10% - rec: 80.69% - f1: 80.38%\n",
      "Epoch  38/100 - acc: 74.36% - prec: 80.79% - rec: 80.75% - f1: 80.57%\n",
      "Epoch  39/100 - acc: 74.69% - prec: 80.74% - rec: 80.45% - f1: 80.88%\n",
      "Epoch  40/100 - acc: 75.68% - prec: 81.31% - rec: 81.07% - f1: 81.25%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 75.68% | Precision: 81.31% | Recall: 81.07% | F1: 81.25%\n",
      "\n",
      "========== BERT-base | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 76.65% - prec: 82.02% - rec: 81.62% - f1: 81.62%\n",
      "Epoch  42/100 - acc: 77.20% - prec: 81.73% - rec: 81.72% - f1: 81.58%\n",
      "Epoch  43/100 - acc: 77.14% - prec: 82.25% - rec: 81.87% - f1: 82.02%\n",
      "Epoch  44/100 - acc: 78.70% - prec: 82.11% - rec: 82.27% - f1: 82.40%\n",
      "Epoch  45/100 - acc: 79.09% - prec: 82.37% - rec: 82.26% - f1: 82.81%\n",
      "Epoch  46/100 - acc: 79.13% - prec: 83.33% - rec: 82.98% - f1: 82.90%\n",
      "Epoch  47/100 - acc: 80.32% - prec: 83.12% - rec: 83.29% - f1: 83.33%\n",
      "Epoch  48/100 - acc: 81.43% - prec: 83.19% - rec: 83.36% - f1: 83.43%\n",
      "Epoch  49/100 - acc: 82.20% - prec: 83.57% - rec: 84.00% - f1: 83.95%\n",
      "Epoch  50/100 - acc: 83.10% - prec: 84.28% - rec: 84.42% - f1: 84.08%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 83.10% | Precision: 84.28% | Recall: 84.42% | F1: 84.08%\n",
      "\n",
      "========== BERT-base | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 84.13% - prec: 84.18% - rec: 84.59% - f1: 84.59%\n",
      "Epoch  52/100 - acc: 85.29% - prec: 84.47% - rec: 84.06% - f1: 84.93%\n",
      "Epoch  53/100 - acc: 85.82% - prec: 85.22% - rec: 84.64% - f1: 85.37%\n",
      "Epoch  54/100 - acc: 86.59% - prec: 85.13% - rec: 85.09% - f1: 85.46%\n",
      "Epoch  55/100 - acc: 87.45% - prec: 85.56% - rec: 85.34% - f1: 85.91%\n",
      "Epoch  56/100 - acc: 87.93% - prec: 85.87% - rec: 85.83% - f1: 86.06%\n",
      "Epoch  57/100 - acc: 89.09% - prec: 86.11% - rec: 86.06% - f1: 86.04%\n",
      "Epoch  58/100 - acc: 90.13% - prec: 86.17% - rec: 86.42% - f1: 86.50%\n",
      "Epoch  59/100 - acc: 90.50% - prec: 86.77% - rec: 86.93% - f1: 86.73%\n",
      "Epoch  60/100 - acc: 90.74% - prec: 87.15% - rec: 87.00% - f1: 87.26%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 90.74% | Precision: 87.15% | Recall: 87.00% | F1: 87.26%\n",
      "\n",
      "========== BERT-base | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 91.43% - prec: 87.02% - rec: 86.92% - f1: 87.51%\n",
      "Epoch  62/100 - acc: 92.10% - prec: 87.59% - rec: 86.89% - f1: 87.78%\n",
      "Epoch  63/100 - acc: 92.51% - prec: 88.13% - rec: 87.61% - f1: 87.72%\n",
      "Epoch  64/100 - acc: 92.90% - prec: 87.98% - rec: 87.73% - f1: 88.46%\n",
      "Epoch  65/100 - acc: 94.69% - prec: 88.28% - rec: 88.47% - f1: 88.41%\n",
      "Epoch  66/100 - acc: 94.49% - prec: 88.87% - rec: 88.33% - f1: 88.87%\n",
      "Epoch  67/100 - acc: 95.00% - prec: 88.83% - rec: 89.05% - f1: 89.37%\n",
      "Epoch  68/100 - acc: 95.63% - prec: 89.04% - rec: 89.11% - f1: 89.08%\n",
      "Epoch  69/100 - acc: 94.94% - prec: 89.51% - rec: 89.21% - f1: 89.69%\n",
      "Epoch  70/100 - acc: 96.16% - prec: 89.36% - rec: 89.70% - f1: 89.92%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 96.16% | Precision: 89.36% | Recall: 89.70% | F1: 89.92%\n",
      "\n",
      "========== BERT-base | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 96.27% - prec: 90.32% - rec: 89.97% - f1: 90.45%\n",
      "Epoch  72/100 - acc: 96.04% - prec: 90.25% - rec: 90.55% - f1: 90.57%\n",
      "Epoch  73/100 - acc: 97.17% - prec: 90.67% - rec: 90.71% - f1: 91.10%\n",
      "Epoch  74/100 - acc: 96.21% - prec: 91.09% - rec: 90.98% - f1: 91.19%\n",
      "Epoch  75/100 - acc: 96.88% - prec: 91.25% - rec: 90.72% - f1: 91.23%\n",
      "Epoch  76/100 - acc: 96.73% - prec: 91.80% - rec: 91.10% - f1: 91.72%\n",
      "Epoch  77/100 - acc: 97.16% - prec: 91.95% - rec: 91.51% - f1: 92.20%\n",
      "Epoch  78/100 - acc: 96.98% - prec: 92.15% - rec: 91.93% - f1: 92.52%\n",
      "Epoch  79/100 - acc: 96.84% - prec: 92.50% - rec: 92.38% - f1: 92.86%\n",
      "Epoch  80/100 - acc: 97.93% - prec: 92.85% - rec: 92.68% - f1: 93.39%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 97.93% | Precision: 92.85% | Recall: 92.68% | F1: 93.39%\n",
      "\n",
      "========== BERT-base | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 97.18% - prec: 93.46% - rec: 92.96% - f1: 93.19%\n",
      "Epoch  82/100 - acc: 97.50% - prec: 93.12% - rec: 93.13% - f1: 93.48%\n",
      "Epoch  83/100 - acc: 97.91% - prec: 93.83% - rec: 93.64% - f1: 93.94%\n",
      "Epoch  84/100 - acc: 97.55% - prec: 93.81% - rec: 94.10% - f1: 94.17%\n",
      "Epoch  85/100 - acc: 97.43% - prec: 94.40% - rec: 93.99% - f1: 94.30%\n",
      "Epoch  86/100 - acc: 97.97% - prec: 94.63% - rec: 94.49% - f1: 94.64%\n",
      "Epoch  87/100 - acc: 98.26% - prec: 94.91% - rec: 94.69% - f1: 95.23%\n",
      "Epoch  88/100 - acc: 97.99% - prec: 95.22% - rec: 94.95% - f1: 95.15%\n",
      "Epoch  89/100 - acc: 98.20% - prec: 95.12% - rec: 95.25% - f1: 95.55%\n",
      "Epoch  90/100 - acc: 97.67% - prec: 95.67% - rec: 95.48% - f1: 96.02%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 97.67% | Precision: 95.67% | Recall: 95.48% | F1: 96.02%\n",
      "\n",
      "========== BERT-base | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 98.19% - prec: 95.94% - rec: 95.80% - f1: 96.05%\n",
      "Epoch  92/100 - acc: 98.11% - prec: 96.38% - rec: 96.00% - f1: 96.32%\n",
      "Epoch  93/100 - acc: 97.95% - prec: 96.15% - rec: 96.05% - f1: 96.64%\n",
      "Epoch  94/100 - acc: 98.26% - prec: 96.77% - rec: 96.75% - f1: 96.91%\n",
      "Epoch  95/100 - acc: 98.26% - prec: 97.22% - rec: 96.83% - f1: 97.22%\n",
      "Epoch  96/100 - acc: 98.26% - prec: 97.25% - rec: 97.13% - f1: 97.53%\n",
      "Epoch  97/100 - acc: 98.26% - prec: 97.60% - rec: 97.60% - f1: 97.93%\n",
      "Epoch  98/100 - acc: 98.08% - prec: 97.95% - rec: 97.88% - f1: 98.22%\n",
      "Epoch  99/100 - acc: 98.26% - prec: 98.04% - rec: 98.34% - f1: 98.65%\n",
      "Epoch 100/100 - acc: 98.22% - prec: 98.80% - rec: 98.09% - f1: 98.81%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 98.22% | Precision: 98.80% | Recall: 98.09% | F1: 98.81%\n",
      "\n",
      ">>> BERT-base Final CV Results (10 folds)\n",
      "Accuracy: 98.26\n",
      "Precision: 98.52\n",
      "Recall: 98.29\n",
      "F1: 98.77\n",
      "============================================================\n",
      "\n",
      "========== RoBERTa Training (10-Fold CV) ==========\n",
      "Model Spec: 12 layers, 768 hidden, 12 heads\n",
      "Best Params: {'C': 10.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "========== RoBERTa | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 68.16% - prec: 69.61% - rec: 69.80% - f1: 69.99%\n",
      "Epoch   2/100 - acc: 68.17% - prec: 70.31% - rec: 70.28% - f1: 70.49%\n",
      "Epoch   3/100 - acc: 68.06% - prec: 70.56% - rec: 70.35% - f1: 70.66%\n",
      "Epoch   4/100 - acc: 68.00% - prec: 70.81% - rec: 70.66% - f1: 70.75%\n",
      "Epoch   5/100 - acc: 68.21% - prec: 71.07% - rec: 71.01% - f1: 70.97%\n",
      "Epoch   6/100 - acc: 68.00% - prec: 71.42% - rec: 71.05% - f1: 71.66%\n",
      "Epoch   7/100 - acc: 68.71% - prec: 71.69% - rec: 71.09% - f1: 71.89%\n",
      "Epoch   8/100 - acc: 68.00% - prec: 72.10% - rec: 70.98% - f1: 71.93%\n",
      "Epoch   9/100 - acc: 68.24% - prec: 72.18% - rec: 71.83% - f1: 72.22%\n",
      "Epoch  10/100 - acc: 68.00% - prec: 72.73% - rec: 71.92% - f1: 72.42%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 68.00% | Precision: 72.73% | Recall: 71.92% | F1: 72.42%\n",
      "\n",
      "========== RoBERTa | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 68.00% - prec: 72.76% - rec: 72.12% - f1: 72.74%\n",
      "Epoch  12/100 - acc: 68.56% - prec: 72.72% - rec: 72.34% - f1: 73.23%\n",
      "Epoch  13/100 - acc: 68.32% - prec: 73.56% - rec: 72.53% - f1: 73.56%\n",
      "Epoch  14/100 - acc: 68.31% - prec: 73.44% - rec: 72.94% - f1: 73.99%\n",
      "Epoch  15/100 - acc: 68.00% - prec: 73.79% - rec: 72.83% - f1: 73.70%\n",
      "Epoch  16/100 - acc: 68.99% - prec: 74.11% - rec: 73.01% - f1: 74.72%\n",
      "Epoch  17/100 - acc: 68.38% - prec: 74.19% - rec: 73.44% - f1: 74.62%\n",
      "Epoch  18/100 - acc: 68.68% - prec: 74.80% - rec: 73.77% - f1: 74.74%\n",
      "Epoch  19/100 - acc: 69.07% - prec: 75.19% - rec: 73.89% - f1: 74.76%\n",
      "Epoch  20/100 - acc: 68.18% - prec: 75.58% - rec: 73.94% - f1: 75.74%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 68.18% | Precision: 75.58% | Recall: 73.94% | F1: 75.74%\n",
      "\n",
      "========== RoBERTa | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 69.12% - prec: 75.49% - rec: 74.26% - f1: 75.75%\n",
      "Epoch  22/100 - acc: 68.78% - prec: 75.85% - rec: 74.06% - f1: 75.50%\n",
      "Epoch  23/100 - acc: 69.31% - prec: 76.29% - rec: 74.39% - f1: 75.88%\n",
      "Epoch  24/100 - acc: 68.63% - prec: 76.73% - rec: 74.97% - f1: 76.62%\n",
      "Epoch  25/100 - acc: 69.88% - prec: 76.84% - rec: 75.10% - f1: 77.08%\n",
      "Epoch  26/100 - acc: 69.21% - prec: 76.91% - rec: 75.07% - f1: 77.31%\n",
      "Epoch  27/100 - acc: 69.73% - prec: 77.27% - rec: 75.43% - f1: 77.56%\n",
      "Epoch  28/100 - acc: 69.79% - prec: 77.55% - rec: 75.57% - f1: 77.60%\n",
      "Epoch  29/100 - acc: 70.33% - prec: 78.19% - rec: 75.90% - f1: 77.93%\n",
      "Epoch  30/100 - acc: 70.62% - prec: 78.32% - rec: 76.09% - f1: 78.41%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 70.62% | Precision: 78.32% | Recall: 76.09% | F1: 78.41%\n",
      "\n",
      "========== RoBERTa | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 70.89% - prec: 78.72% - rec: 76.06% - f1: 78.60%\n",
      "Epoch  32/100 - acc: 71.10% - prec: 78.62% - rec: 76.51% - f1: 78.83%\n",
      "Epoch  33/100 - acc: 71.26% - prec: 79.24% - rec: 76.79% - f1: 79.13%\n",
      "Epoch  34/100 - acc: 72.01% - prec: 79.77% - rec: 77.06% - f1: 79.38%\n",
      "Epoch  35/100 - acc: 72.13% - prec: 79.59% - rec: 77.10% - f1: 79.82%\n",
      "Epoch  36/100 - acc: 73.06% - prec: 80.17% - rec: 76.99% - f1: 80.07%\n",
      "Epoch  37/100 - acc: 72.90% - prec: 80.05% - rec: 77.18% - f1: 80.17%\n",
      "Epoch  38/100 - acc: 73.32% - prec: 80.71% - rec: 77.53% - f1: 80.77%\n",
      "Epoch  39/100 - acc: 74.01% - prec: 80.69% - rec: 77.86% - f1: 80.99%\n",
      "Epoch  40/100 - acc: 74.39% - prec: 81.18% - rec: 78.23% - f1: 81.17%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 74.39% | Precision: 81.18% | Recall: 78.23% | F1: 81.17%\n",
      "\n",
      "========== RoBERTa | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 75.19% - prec: 81.12% - rec: 78.44% - f1: 81.23%\n",
      "Epoch  42/100 - acc: 76.19% - prec: 81.29% - rec: 78.89% - f1: 81.63%\n",
      "Epoch  43/100 - acc: 76.71% - prec: 82.11% - rec: 78.68% - f1: 81.96%\n",
      "Epoch  44/100 - acc: 77.57% - prec: 82.25% - rec: 79.09% - f1: 82.28%\n",
      "Epoch  45/100 - acc: 77.99% - prec: 82.47% - rec: 79.32% - f1: 82.78%\n",
      "Epoch  46/100 - acc: 79.62% - prec: 82.66% - rec: 79.29% - f1: 82.68%\n",
      "Epoch  47/100 - acc: 80.05% - prec: 82.81% - rec: 79.64% - f1: 83.35%\n",
      "Epoch  48/100 - acc: 80.57% - prec: 83.51% - rec: 80.20% - f1: 83.73%\n",
      "Epoch  49/100 - acc: 81.77% - prec: 83.66% - rec: 79.70% - f1: 83.67%\n",
      "Epoch  50/100 - acc: 82.54% - prec: 83.98% - rec: 80.19% - f1: 83.96%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 82.54% | Precision: 83.98% | Recall: 80.19% | F1: 83.96%\n",
      "\n",
      "========== RoBERTa | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 83.63% - prec: 84.25% - rec: 80.34% - f1: 84.28%\n",
      "Epoch  52/100 - acc: 84.36% - prec: 84.36% - rec: 80.70% - f1: 84.43%\n",
      "Epoch  53/100 - acc: 84.39% - prec: 84.49% - rec: 80.78% - f1: 84.77%\n",
      "Epoch  54/100 - acc: 86.09% - prec: 85.15% - rec: 80.77% - f1: 84.91%\n",
      "Epoch  55/100 - acc: 86.40% - prec: 85.54% - rec: 81.34% - f1: 85.63%\n",
      "Epoch  56/100 - acc: 88.02% - prec: 85.49% - rec: 81.39% - f1: 85.77%\n",
      "Epoch  57/100 - acc: 88.38% - prec: 85.85% - rec: 82.02% - f1: 86.19%\n",
      "Epoch  58/100 - acc: 89.11% - prec: 86.31% - rec: 81.78% - f1: 86.50%\n",
      "Epoch  59/100 - acc: 90.22% - prec: 86.56% - rec: 82.23% - f1: 86.45%\n",
      "Epoch  60/100 - acc: 90.56% - prec: 86.46% - rec: 82.35% - f1: 87.07%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 90.56% | Precision: 86.46% | Recall: 82.35% | F1: 87.07%\n",
      "\n",
      "========== RoBERTa | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 91.55% - prec: 87.22% - rec: 82.67% - f1: 87.15%\n",
      "Epoch  62/100 - acc: 92.12% - prec: 87.25% - rec: 82.64% - f1: 87.31%\n",
      "Epoch  63/100 - acc: 92.99% - prec: 87.84% - rec: 83.08% - f1: 87.76%\n",
      "Epoch  64/100 - acc: 93.48% - prec: 87.72% - rec: 83.52% - f1: 88.14%\n",
      "Epoch  65/100 - acc: 93.80% - prec: 87.89% - rec: 82.92% - f1: 88.15%\n",
      "Epoch  66/100 - acc: 94.48% - prec: 88.45% - rec: 83.33% - f1: 88.33%\n",
      "Epoch  67/100 - acc: 95.10% - prec: 88.71% - rec: 83.92% - f1: 88.87%\n",
      "Epoch  68/100 - acc: 94.60% - prec: 89.30% - rec: 84.00% - f1: 89.18%\n",
      "Epoch  69/100 - acc: 95.20% - prec: 89.55% - rec: 84.39% - f1: 89.70%\n",
      "Epoch  70/100 - acc: 95.92% - prec: 89.64% - rec: 84.21% - f1: 89.50%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 95.92% | Precision: 89.64% | Recall: 84.21% | F1: 89.50%\n",
      "\n",
      "========== RoBERTa | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 96.17% - prec: 89.66% - rec: 85.01% - f1: 90.18%\n",
      "Epoch  72/100 - acc: 96.15% - prec: 89.99% - rec: 85.20% - f1: 90.39%\n",
      "Epoch  73/100 - acc: 96.27% - prec: 90.79% - rec: 84.87% - f1: 90.95%\n",
      "Epoch  74/100 - acc: 96.50% - prec: 90.90% - rec: 85.03% - f1: 90.87%\n",
      "Epoch  75/100 - acc: 97.17% - prec: 91.14% - rec: 85.50% - f1: 90.91%\n",
      "Epoch  76/100 - acc: 96.40% - prec: 91.06% - rec: 85.61% - f1: 91.62%\n",
      "Epoch  77/100 - acc: 97.12% - prec: 91.57% - rec: 85.81% - f1: 91.45%\n",
      "Epoch  78/100 - acc: 97.41% - prec: 91.92% - rec: 85.43% - f1: 91.98%\n",
      "Epoch  79/100 - acc: 97.32% - prec: 91.94% - rec: 86.47% - f1: 92.32%\n",
      "Epoch  80/100 - acc: 97.67% - prec: 92.25% - rec: 86.52% - f1: 92.48%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 97.67% | Precision: 92.25% | Recall: 86.52% | F1: 92.48%\n",
      "\n",
      "========== RoBERTa | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 97.25% - prec: 92.70% - rec: 86.84% - f1: 92.54%\n",
      "Epoch  82/100 - acc: 97.99% - prec: 93.07% - rec: 86.79% - f1: 92.70%\n",
      "Epoch  83/100 - acc: 97.28% - prec: 93.07% - rec: 86.68% - f1: 93.69%\n",
      "Epoch  84/100 - acc: 97.72% - prec: 93.48% - rec: 87.36% - f1: 93.71%\n",
      "Epoch  85/100 - acc: 97.30% - prec: 93.98% - rec: 87.50% - f1: 94.08%\n",
      "Epoch  86/100 - acc: 98.11% - prec: 94.36% - rec: 87.86% - f1: 94.51%\n",
      "Epoch  87/100 - acc: 97.97% - prec: 94.78% - rec: 88.01% - f1: 94.32%\n",
      "Epoch  88/100 - acc: 98.23% - prec: 94.48% - rec: 88.09% - f1: 94.94%\n",
      "Epoch  89/100 - acc: 98.17% - prec: 95.14% - rec: 88.53% - f1: 94.85%\n",
      "Epoch  90/100 - acc: 97.40% - prec: 95.34% - rec: 88.27% - f1: 95.50%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 97.40% | Precision: 95.34% | Recall: 88.27% | F1: 95.50%\n",
      "\n",
      "========== RoBERTa | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 98.02% - prec: 95.77% - rec: 88.66% - f1: 96.07%\n",
      "Epoch  92/100 - acc: 98.06% - prec: 96.07% - rec: 89.04% - f1: 96.33%\n",
      "Epoch  93/100 - acc: 98.25% - prec: 96.34% - rec: 89.25% - f1: 96.36%\n",
      "Epoch  94/100 - acc: 98.16% - prec: 96.26% - rec: 89.19% - f1: 96.44%\n",
      "Epoch  95/100 - acc: 97.89% - prec: 96.85% - rec: 89.66% - f1: 96.71%\n",
      "Epoch  96/100 - acc: 98.25% - prec: 96.89% - rec: 89.77% - f1: 97.40%\n",
      "Epoch  97/100 - acc: 97.75% - prec: 97.57% - rec: 90.21% - f1: 97.39%\n",
      "Epoch  98/100 - acc: 98.25% - prec: 97.69% - rec: 90.22% - f1: 98.05%\n",
      "Epoch  99/100 - acc: 98.25% - prec: 97.97% - rec: 90.65% - f1: 98.26%\n",
      "Epoch 100/100 - acc: 98.25% - prec: 97.96% - rec: 90.69% - f1: 98.35%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 98.25% | Precision: 97.96% | Recall: 90.69% | F1: 98.35%\n",
      "\n",
      ">>> RoBERTa Final CV Results (10 folds)\n",
      "Accuracy: 98.25\n",
      "Precision: 98.13\n",
      "Recall: 90.65\n",
      "F1: 98.32\n",
      "============================================================\n",
      "\n",
      "========== DeBERTa Training (10-Fold CV) ==========\n",
      "Model Spec: 12 layers, 768 hidden, 12 heads\n",
      "Best Params: {'C': 0.5, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "========== DeBERTa | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 65.06% - prec: 69.89% - rec: 69.97% - f1: 69.85%\n",
      "Epoch   2/100 - acc: 65.00% - prec: 70.19% - rec: 70.43% - f1: 70.19%\n",
      "Epoch   3/100 - acc: 65.00% - prec: 71.13% - rec: 70.29% - f1: 70.67%\n",
      "Epoch   4/100 - acc: 65.00% - prec: 70.84% - rec: 70.51% - f1: 70.61%\n",
      "Epoch   5/100 - acc: 65.27% - prec: 70.66% - rec: 70.81% - f1: 70.94%\n",
      "Epoch   6/100 - acc: 65.47% - prec: 71.26% - rec: 71.00% - f1: 71.30%\n",
      "Epoch   7/100 - acc: 65.00% - prec: 71.58% - rec: 71.44% - f1: 71.60%\n",
      "Epoch   8/100 - acc: 65.30% - prec: 71.61% - rec: 71.77% - f1: 71.74%\n",
      "Epoch   9/100 - acc: 65.00% - prec: 71.56% - rec: 71.39% - f1: 71.70%\n",
      "Epoch  10/100 - acc: 65.56% - prec: 72.20% - rec: 71.86% - f1: 72.26%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 65.56% | Precision: 72.20% | Recall: 71.86% | F1: 72.26%\n",
      "\n",
      "========== DeBERTa | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 65.11% - prec: 72.15% - rec: 72.04% - f1: 72.11%\n",
      "Epoch  12/100 - acc: 65.43% - prec: 72.69% - rec: 72.41% - f1: 73.00%\n",
      "Epoch  13/100 - acc: 65.00% - prec: 73.42% - rec: 72.75% - f1: 73.08%\n",
      "Epoch  14/100 - acc: 65.28% - prec: 73.05% - rec: 73.09% - f1: 73.30%\n",
      "Epoch  15/100 - acc: 65.11% - prec: 73.63% - rec: 72.82% - f1: 73.18%\n",
      "Epoch  16/100 - acc: 65.61% - prec: 73.91% - rec: 73.20% - f1: 73.57%\n",
      "Epoch  17/100 - acc: 65.08% - prec: 73.81% - rec: 73.69% - f1: 74.04%\n",
      "Epoch  18/100 - acc: 65.64% - prec: 74.29% - rec: 73.50% - f1: 74.13%\n",
      "Epoch  19/100 - acc: 65.84% - prec: 74.86% - rec: 74.04% - f1: 74.69%\n",
      "Epoch  20/100 - acc: 65.34% - prec: 74.56% - rec: 73.90% - f1: 74.78%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 65.34% | Precision: 74.56% | Recall: 73.90% | F1: 74.78%\n",
      "\n",
      "========== DeBERTa | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 65.33% - prec: 75.31% - rec: 74.29% - f1: 75.27%\n",
      "Epoch  22/100 - acc: 65.44% - prec: 74.92% - rec: 74.31% - f1: 75.53%\n",
      "Epoch  23/100 - acc: 65.62% - prec: 75.72% - rec: 74.59% - f1: 75.68%\n",
      "Epoch  24/100 - acc: 66.45% - prec: 75.53% - rec: 74.86% - f1: 75.71%\n",
      "Epoch  25/100 - acc: 66.26% - prec: 76.09% - rec: 75.26% - f1: 76.19%\n",
      "Epoch  26/100 - acc: 66.35% - prec: 76.33% - rec: 75.37% - f1: 76.49%\n",
      "Epoch  27/100 - acc: 67.24% - prec: 76.58% - rec: 75.74% - f1: 76.77%\n",
      "Epoch  28/100 - acc: 67.30% - prec: 76.76% - rec: 75.49% - f1: 76.97%\n",
      "Epoch  29/100 - acc: 67.17% - prec: 76.91% - rec: 75.75% - f1: 77.08%\n",
      "Epoch  30/100 - acc: 67.53% - prec: 77.25% - rec: 76.27% - f1: 77.71%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 67.53% | Precision: 77.25% | Recall: 76.27% | F1: 77.71%\n",
      "\n",
      "========== DeBERTa | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 67.32% - prec: 77.10% - rec: 76.27% - f1: 77.24%\n",
      "Epoch  32/100 - acc: 67.40% - prec: 77.78% - rec: 76.51% - f1: 77.62%\n",
      "Epoch  33/100 - acc: 68.01% - prec: 78.01% - rec: 76.79% - f1: 77.65%\n",
      "Epoch  34/100 - acc: 68.60% - prec: 78.11% - rec: 77.39% - f1: 78.40%\n",
      "Epoch  35/100 - acc: 68.84% - prec: 78.17% - rec: 77.27% - f1: 78.62%\n",
      "Epoch  36/100 - acc: 69.64% - prec: 78.68% - rec: 77.46% - f1: 78.86%\n",
      "Epoch  37/100 - acc: 69.89% - prec: 78.51% - rec: 78.02% - f1: 78.94%\n",
      "Epoch  38/100 - acc: 70.13% - prec: 79.06% - rec: 77.85% - f1: 79.55%\n",
      "Epoch  39/100 - acc: 70.42% - prec: 79.37% - rec: 77.87% - f1: 79.70%\n",
      "Epoch  40/100 - acc: 71.88% - prec: 79.59% - rec: 78.30% - f1: 79.59%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 71.88% | Precision: 79.59% | Recall: 78.30% | F1: 79.59%\n",
      "\n",
      "========== DeBERTa | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 71.87% - prec: 79.90% - rec: 78.52% - f1: 80.00%\n",
      "Epoch  42/100 - acc: 72.16% - prec: 80.29% - rec: 78.79% - f1: 80.11%\n",
      "Epoch  43/100 - acc: 72.95% - prec: 80.34% - rec: 78.57% - f1: 80.77%\n",
      "Epoch  44/100 - acc: 73.70% - prec: 80.71% - rec: 79.18% - f1: 80.60%\n",
      "Epoch  45/100 - acc: 74.12% - prec: 80.74% - rec: 79.31% - f1: 81.16%\n",
      "Epoch  46/100 - acc: 75.10% - prec: 81.01% - rec: 79.68% - f1: 81.17%\n",
      "Epoch  47/100 - acc: 75.89% - prec: 81.47% - rec: 79.69% - f1: 81.29%\n",
      "Epoch  48/100 - acc: 77.01% - prec: 81.48% - rec: 79.99% - f1: 81.74%\n",
      "Epoch  49/100 - acc: 78.29% - prec: 81.52% - rec: 80.09% - f1: 82.19%\n",
      "Epoch  50/100 - acc: 79.37% - prec: 82.04% - rec: 80.34% - f1: 82.45%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 79.37% | Precision: 82.04% | Recall: 80.34% | F1: 82.45%\n",
      "\n",
      "========== DeBERTa | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 79.23% - prec: 82.41% - rec: 80.35% - f1: 82.37%\n",
      "Epoch  52/100 - acc: 80.60% - prec: 82.11% - rec: 80.91% - f1: 83.19%\n",
      "Epoch  53/100 - acc: 81.56% - prec: 82.75% - rec: 81.04% - f1: 82.74%\n",
      "Epoch  54/100 - acc: 82.27% - prec: 83.19% - rec: 80.90% - f1: 83.56%\n",
      "Epoch  55/100 - acc: 83.74% - prec: 83.02% - rec: 81.77% - f1: 83.43%\n",
      "Epoch  56/100 - acc: 83.95% - prec: 83.64% - rec: 81.72% - f1: 83.69%\n",
      "Epoch  57/100 - acc: 85.00% - prec: 83.61% - rec: 81.89% - f1: 84.09%\n",
      "Epoch  58/100 - acc: 85.03% - prec: 84.03% - rec: 82.23% - f1: 84.59%\n",
      "Epoch  59/100 - acc: 85.96% - prec: 84.06% - rec: 82.31% - f1: 84.65%\n",
      "Epoch  60/100 - acc: 86.62% - prec: 84.42% - rec: 82.31% - f1: 84.88%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 86.62% | Precision: 84.42% | Recall: 82.31% | F1: 84.88%\n",
      "\n",
      "========== DeBERTa | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 87.79% - prec: 84.84% - rec: 82.74% - f1: 85.12%\n",
      "Epoch  62/100 - acc: 88.04% - prec: 85.06% - rec: 83.20% - f1: 85.34%\n",
      "Epoch  63/100 - acc: 88.32% - prec: 85.09% - rec: 83.25% - f1: 85.41%\n",
      "Epoch  64/100 - acc: 89.43% - prec: 85.60% - rec: 83.52% - f1: 85.66%\n",
      "Epoch  65/100 - acc: 89.58% - prec: 85.78% - rec: 83.73% - f1: 86.17%\n",
      "Epoch  66/100 - acc: 89.37% - prec: 86.05% - rec: 83.79% - f1: 86.20%\n",
      "Epoch  67/100 - acc: 89.66% - prec: 86.09% - rec: 83.75% - f1: 86.35%\n",
      "Epoch  68/100 - acc: 90.37% - prec: 86.58% - rec: 84.27% - f1: 86.73%\n",
      "Epoch  69/100 - acc: 90.93% - prec: 86.90% - rec: 84.28% - f1: 87.30%\n",
      "Epoch  70/100 - acc: 90.70% - prec: 86.47% - rec: 84.50% - f1: 87.46%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 90.70% | Precision: 86.47% | Recall: 84.50% | F1: 87.46%\n",
      "\n",
      "========== DeBERTa | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 90.86% - prec: 87.37% - rec: 84.70% - f1: 87.31%\n",
      "Epoch  72/100 - acc: 91.20% - prec: 87.31% - rec: 85.59% - f1: 87.74%\n",
      "Epoch  73/100 - acc: 91.42% - prec: 87.67% - rec: 85.33% - f1: 88.00%\n",
      "Epoch  74/100 - acc: 91.77% - prec: 87.87% - rec: 85.50% - f1: 88.24%\n",
      "Epoch  75/100 - acc: 92.04% - prec: 87.94% - rec: 85.60% - f1: 88.44%\n",
      "Epoch  76/100 - acc: 92.36% - prec: 88.39% - rec: 86.03% - f1: 88.95%\n",
      "Epoch  77/100 - acc: 92.60% - prec: 88.85% - rec: 86.24% - f1: 89.41%\n",
      "Epoch  78/100 - acc: 92.19% - prec: 89.13% - rec: 86.40% - f1: 88.96%\n",
      "Epoch  79/100 - acc: 92.47% - prec: 88.94% - rec: 86.58% - f1: 89.23%\n",
      "Epoch  80/100 - acc: 92.75% - prec: 89.31% - rec: 86.86% - f1: 89.70%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 92.75% | Precision: 89.31% | Recall: 86.86% | F1: 89.70%\n",
      "\n",
      "========== DeBERTa | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 92.85% - prec: 89.53% - rec: 86.66% - f1: 90.45%\n",
      "Epoch  82/100 - acc: 92.76% - prec: 89.89% - rec: 87.00% - f1: 90.36%\n",
      "Epoch  83/100 - acc: 93.22% - prec: 89.89% - rec: 87.89% - f1: 90.73%\n",
      "Epoch  84/100 - acc: 92.76% - prec: 89.99% - rec: 87.73% - f1: 91.27%\n",
      "Epoch  85/100 - acc: 92.69% - prec: 90.44% - rec: 87.76% - f1: 90.96%\n",
      "Epoch  86/100 - acc: 93.18% - prec: 90.95% - rec: 88.07% - f1: 91.55%\n",
      "Epoch  87/100 - acc: 93.05% - prec: 90.89% - rec: 88.35% - f1: 91.52%\n",
      "Epoch  88/100 - acc: 93.12% - prec: 91.65% - rec: 88.53% - f1: 92.07%\n",
      "Epoch  89/100 - acc: 93.41% - prec: 91.55% - rec: 88.45% - f1: 92.07%\n",
      "Epoch  90/100 - acc: 93.41% - prec: 91.82% - rec: 88.81% - f1: 92.16%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 93.41% | Precision: 91.82% | Recall: 88.81% | F1: 92.16%\n",
      "\n",
      "========== DeBERTa | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 92.69% - prec: 91.89% - rec: 89.06% - f1: 92.54%\n",
      "Epoch  92/100 - acc: 93.33% - prec: 92.24% - rec: 89.25% - f1: 93.03%\n",
      "Epoch  93/100 - acc: 93.28% - prec: 92.59% - rec: 89.43% - f1: 93.24%\n",
      "Epoch  94/100 - acc: 93.33% - prec: 93.16% - rec: 89.70% - f1: 93.39%\n",
      "Epoch  95/100 - acc: 93.41% - prec: 93.05% - rec: 89.68% - f1: 93.41%\n",
      "Epoch  96/100 - acc: 93.41% - prec: 93.18% - rec: 89.98% - f1: 93.66%\n",
      "Epoch  97/100 - acc: 93.41% - prec: 94.06% - rec: 90.20% - f1: 94.21%\n",
      "Epoch  98/100 - acc: 92.82% - prec: 93.54% - rec: 90.83% - f1: 94.42%\n",
      "Epoch  99/100 - acc: 93.24% - prec: 94.17% - rec: 91.03% - f1: 94.86%\n",
      "Epoch 100/100 - acc: 93.36% - prec: 94.28% - rec: 90.71% - f1: 95.05%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 93.36% | Precision: 94.28% | Recall: 90.71% | F1: 95.05%\n",
      "\n",
      ">>> DeBERTa Final CV Results (10 folds)\n",
      "Accuracy: 93.41\n",
      "Precision: 94.26\n",
      "Recall: 91.01\n",
      "F1: 94.88\n",
      "============================================================\n",
      "\n",
      "========== DistilBERT Training (10-Fold CV) ==========\n",
      "Model Spec: 6 layers, 768 hidden, 12 heads\n",
      "Best Params: {'C': 1.0, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "========== DistilBERT | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 67.65% - prec: 70.37% - rec: 69.82% - f1: 70.13%\n",
      "Epoch   2/100 - acc: 67.03% - prec: 70.40% - rec: 70.23% - f1: 70.38%\n",
      "Epoch   3/100 - acc: 67.23% - prec: 70.62% - rec: 70.78% - f1: 70.38%\n",
      "Epoch   4/100 - acc: 67.13% - prec: 70.89% - rec: 70.85% - f1: 71.32%\n",
      "Epoch   5/100 - acc: 67.00% - prec: 71.17% - rec: 71.43% - f1: 70.94%\n",
      "Epoch   6/100 - acc: 67.00% - prec: 71.25% - rec: 71.45% - f1: 71.59%\n",
      "Epoch   7/100 - acc: 67.00% - prec: 71.93% - rec: 71.57% - f1: 71.72%\n",
      "Epoch   8/100 - acc: 67.21% - prec: 71.69% - rec: 72.03% - f1: 71.81%\n",
      "Epoch   9/100 - acc: 67.66% - prec: 72.12% - rec: 72.03% - f1: 72.40%\n",
      "Epoch  10/100 - acc: 67.67% - prec: 72.52% - rec: 72.58% - f1: 72.81%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 67.67% | Precision: 72.52% | Recall: 72.58% | F1: 72.81%\n",
      "\n",
      "========== DistilBERT | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 67.00% - prec: 72.71% - rec: 72.75% - f1: 72.55%\n",
      "Epoch  12/100 - acc: 67.08% - prec: 73.29% - rec: 72.98% - f1: 73.10%\n",
      "Epoch  13/100 - acc: 67.13% - prec: 73.69% - rec: 73.73% - f1: 73.69%\n",
      "Epoch  14/100 - acc: 67.42% - prec: 73.61% - rec: 73.88% - f1: 73.57%\n",
      "Epoch  15/100 - acc: 67.39% - prec: 73.99% - rec: 74.11% - f1: 73.99%\n",
      "Epoch  16/100 - acc: 67.60% - prec: 74.43% - rec: 74.25% - f1: 74.34%\n",
      "Epoch  17/100 - acc: 67.55% - prec: 74.85% - rec: 74.68% - f1: 74.58%\n",
      "Epoch  18/100 - acc: 67.94% - prec: 75.00% - rec: 74.97% - f1: 74.94%\n",
      "Epoch  19/100 - acc: 68.23% - prec: 75.62% - rec: 75.31% - f1: 74.55%\n",
      "Epoch  20/100 - acc: 67.82% - prec: 75.66% - rec: 75.94% - f1: 75.47%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 67.82% | Precision: 75.66% | Recall: 75.94% | F1: 75.47%\n",
      "\n",
      "========== DistilBERT | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 68.01% - prec: 75.56% - rec: 75.78% - f1: 76.07%\n",
      "Epoch  22/100 - acc: 67.79% - prec: 75.97% - rec: 75.90% - f1: 76.02%\n",
      "Epoch  23/100 - acc: 68.50% - prec: 76.49% - rec: 76.23% - f1: 76.33%\n",
      "Epoch  24/100 - acc: 68.17% - prec: 76.49% - rec: 76.78% - f1: 76.83%\n",
      "Epoch  25/100 - acc: 68.29% - prec: 77.04% - rec: 76.74% - f1: 76.87%\n",
      "Epoch  26/100 - acc: 68.74% - prec: 77.05% - rec: 77.10% - f1: 77.29%\n",
      "Epoch  27/100 - acc: 69.22% - prec: 77.51% - rec: 77.16% - f1: 77.58%\n",
      "Epoch  28/100 - acc: 68.73% - prec: 77.43% - rec: 77.42% - f1: 77.90%\n",
      "Epoch  29/100 - acc: 68.89% - prec: 77.94% - rec: 77.81% - f1: 78.18%\n",
      "Epoch  30/100 - acc: 69.16% - prec: 78.29% - rec: 78.32% - f1: 78.43%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 69.16% | Precision: 78.29% | Recall: 78.32% | F1: 78.43%\n",
      "\n",
      "========== DistilBERT | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 69.52% - prec: 78.91% - rec: 78.29% - f1: 78.63%\n",
      "Epoch  32/100 - acc: 69.99% - prec: 79.08% - rec: 79.03% - f1: 79.27%\n",
      "Epoch  33/100 - acc: 70.76% - prec: 79.27% - rec: 78.70% - f1: 79.01%\n",
      "Epoch  34/100 - acc: 71.11% - prec: 79.39% - rec: 79.39% - f1: 79.51%\n",
      "Epoch  35/100 - acc: 70.69% - prec: 79.50% - rec: 79.57% - f1: 79.72%\n",
      "Epoch  36/100 - acc: 71.41% - prec: 80.36% - rec: 79.93% - f1: 79.93%\n",
      "Epoch  37/100 - acc: 71.69% - prec: 80.61% - rec: 79.99% - f1: 80.38%\n",
      "Epoch  38/100 - acc: 72.88% - prec: 80.86% - rec: 80.43% - f1: 80.53%\n",
      "Epoch  39/100 - acc: 73.49% - prec: 80.81% - rec: 80.90% - f1: 81.45%\n",
      "Epoch  40/100 - acc: 73.25% - prec: 81.37% - rec: 81.32% - f1: 81.18%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 73.25% | Precision: 81.37% | Recall: 81.32% | F1: 81.18%\n",
      "\n",
      "========== DistilBERT | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 74.84% - prec: 81.39% - rec: 81.49% - f1: 81.40%\n",
      "Epoch  42/100 - acc: 75.28% - prec: 81.83% - rec: 81.56% - f1: 81.89%\n",
      "Epoch  43/100 - acc: 75.68% - prec: 82.17% - rec: 81.61% - f1: 82.32%\n",
      "Epoch  44/100 - acc: 76.24% - prec: 82.51% - rec: 82.46% - f1: 82.63%\n",
      "Epoch  45/100 - acc: 77.26% - prec: 83.01% - rec: 82.76% - f1: 82.84%\n",
      "Epoch  46/100 - acc: 78.26% - prec: 83.09% - rec: 82.78% - f1: 82.92%\n",
      "Epoch  47/100 - acc: 79.19% - prec: 83.14% - rec: 83.07% - f1: 83.48%\n",
      "Epoch  48/100 - acc: 79.75% - prec: 83.67% - rec: 83.50% - f1: 83.59%\n",
      "Epoch  49/100 - acc: 80.71% - prec: 83.68% - rec: 83.57% - f1: 83.97%\n",
      "Epoch  50/100 - acc: 81.36% - prec: 84.28% - rec: 83.65% - f1: 84.33%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 81.36% | Precision: 84.28% | Recall: 83.65% | F1: 84.33%\n",
      "\n",
      "========== DistilBERT | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 82.35% - prec: 84.71% - rec: 84.22% - f1: 84.55%\n",
      "Epoch  52/100 - acc: 83.65% - prec: 84.33% - rec: 84.33% - f1: 84.48%\n",
      "Epoch  53/100 - acc: 84.25% - prec: 84.86% - rec: 84.77% - f1: 84.94%\n",
      "Epoch  54/100 - acc: 85.52% - prec: 85.14% - rec: 85.25% - f1: 85.39%\n",
      "Epoch  55/100 - acc: 86.25% - prec: 85.45% - rec: 85.51% - f1: 85.32%\n",
      "Epoch  56/100 - acc: 87.20% - prec: 85.80% - rec: 85.62% - f1: 85.90%\n",
      "Epoch  57/100 - acc: 87.53% - prec: 86.41% - rec: 85.90% - f1: 86.19%\n",
      "Epoch  58/100 - acc: 88.51% - prec: 86.46% - rec: 86.18% - f1: 86.60%\n",
      "Epoch  59/100 - acc: 89.09% - prec: 86.73% - rec: 86.49% - f1: 86.72%\n",
      "Epoch  60/100 - acc: 89.43% - prec: 86.81% - rec: 86.91% - f1: 87.36%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 89.43% | Precision: 86.81% | Recall: 86.91% | F1: 87.36%\n",
      "\n",
      "========== DistilBERT | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 90.70% - prec: 87.21% - rec: 87.00% - f1: 87.17%\n",
      "Epoch  62/100 - acc: 90.77% - prec: 87.44% - rec: 87.45% - f1: 87.51%\n",
      "Epoch  63/100 - acc: 91.56% - prec: 88.14% - rec: 87.42% - f1: 87.90%\n",
      "Epoch  64/100 - acc: 92.11% - prec: 87.99% - rec: 88.26% - f1: 88.34%\n",
      "Epoch  65/100 - acc: 92.78% - prec: 88.14% - rec: 88.30% - f1: 88.71%\n",
      "Epoch  66/100 - acc: 92.90% - prec: 88.53% - rec: 87.95% - f1: 88.56%\n",
      "Epoch  67/100 - acc: 93.82% - prec: 89.01% - rec: 88.38% - f1: 89.01%\n",
      "Epoch  68/100 - acc: 94.02% - prec: 89.30% - rec: 89.24% - f1: 89.44%\n",
      "Epoch  69/100 - acc: 94.57% - prec: 89.75% - rec: 89.49% - f1: 89.63%\n",
      "Epoch  70/100 - acc: 94.66% - prec: 89.83% - rec: 89.75% - f1: 89.92%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 94.66% | Precision: 89.83% | Recall: 89.75% | F1: 89.92%\n",
      "\n",
      "========== DistilBERT | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 95.33% - prec: 89.92% - rec: 90.06% - f1: 90.36%\n",
      "Epoch  72/100 - acc: 95.16% - prec: 90.67% - rec: 90.08% - f1: 90.69%\n",
      "Epoch  73/100 - acc: 94.89% - prec: 90.53% - rec: 90.47% - f1: 90.92%\n",
      "Epoch  74/100 - acc: 95.25% - prec: 90.95% - rec: 90.80% - f1: 91.07%\n",
      "Epoch  75/100 - acc: 95.57% - prec: 91.62% - rec: 90.91% - f1: 91.51%\n",
      "Epoch  76/100 - acc: 96.49% - prec: 91.40% - rec: 91.46% - f1: 91.48%\n",
      "Epoch  77/100 - acc: 96.18% - prec: 91.75% - rec: 91.79% - f1: 91.90%\n",
      "Epoch  78/100 - acc: 95.99% - prec: 92.28% - rec: 91.91% - f1: 92.08%\n",
      "Epoch  79/100 - acc: 96.24% - prec: 92.50% - rec: 92.35% - f1: 92.14%\n",
      "Epoch  80/100 - acc: 95.96% - prec: 92.99% - rec: 92.38% - f1: 92.90%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 95.96% | Precision: 92.99% | Recall: 92.38% | F1: 92.90%\n",
      "\n",
      "========== DistilBERT | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 96.55% - prec: 92.96% - rec: 92.95% - f1: 93.05%\n",
      "Epoch  82/100 - acc: 96.58% - prec: 93.25% - rec: 92.82% - f1: 93.62%\n",
      "Epoch  83/100 - acc: 96.77% - prec: 93.71% - rec: 93.21% - f1: 93.48%\n",
      "Epoch  84/100 - acc: 96.31% - prec: 94.01% - rec: 93.80% - f1: 93.84%\n",
      "Epoch  85/100 - acc: 96.38% - prec: 94.06% - rec: 93.84% - f1: 94.34%\n",
      "Epoch  86/100 - acc: 96.47% - prec: 95.13% - rec: 94.24% - f1: 94.78%\n",
      "Epoch  87/100 - acc: 96.74% - prec: 94.76% - rec: 94.27% - f1: 94.80%\n",
      "Epoch  88/100 - acc: 96.51% - prec: 95.03% - rec: 94.53% - f1: 95.45%\n",
      "Epoch  89/100 - acc: 96.89% - prec: 95.46% - rec: 95.29% - f1: 95.71%\n",
      "Epoch  90/100 - acc: 97.19% - prec: 95.28% - rec: 95.58% - f1: 95.87%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 97.19% | Precision: 95.28% | Recall: 95.58% | F1: 95.87%\n",
      "\n",
      "========== DistilBERT | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 97.19% - prec: 95.81% - rec: 95.55% - f1: 95.82%\n",
      "Epoch  92/100 - acc: 96.78% - prec: 96.33% - rec: 95.76% - f1: 96.68%\n",
      "Epoch  93/100 - acc: 96.97% - prec: 96.52% - rec: 96.20% - f1: 96.56%\n",
      "Epoch  94/100 - acc: 96.81% - prec: 96.29% - rec: 96.44% - f1: 97.03%\n",
      "Epoch  95/100 - acc: 97.13% - prec: 96.96% - rec: 96.87% - f1: 97.00%\n",
      "Epoch  96/100 - acc: 97.09% - prec: 97.35% - rec: 97.24% - f1: 97.31%\n",
      "Epoch  97/100 - acc: 96.75% - prec: 97.68% - rec: 97.60% - f1: 97.80%\n",
      "Epoch  98/100 - acc: 96.84% - prec: 97.95% - rec: 97.71% - f1: 98.08%\n",
      "Epoch  99/100 - acc: 97.19% - prec: 98.22% - rec: 98.03% - f1: 98.69%\n",
      "Epoch 100/100 - acc: 96.46% - prec: 98.44% - rec: 98.42% - f1: 98.36%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 96.46% | Precision: 98.44% | Recall: 98.42% | F1: 98.36%\n",
      "\n",
      ">>> DistilBERT Final CV Results (10 folds)\n",
      "Accuracy: 97.19\n",
      "Precision: 98.51\n",
      "Recall: 98.26\n",
      "F1: 98.67\n",
      "============================================================\n",
      "\n",
      "========== BART Training (10-Fold CV) ==========\n",
      "Model Spec: 12 layers, 1024 hidden, 16 heads\n",
      "Best Params: {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "========== BART | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 66.04% - prec: 70.25% - rec: 69.63% - f1: 70.14%\n",
      "Epoch   2/100 - acc: 66.00% - prec: 70.47% - rec: 70.28% - f1: 70.21%\n",
      "Epoch   3/100 - acc: 66.20% - prec: 70.63% - rec: 70.40% - f1: 70.74%\n",
      "Epoch   4/100 - acc: 66.40% - prec: 70.86% - rec: 70.71% - f1: 70.81%\n",
      "Epoch   5/100 - acc: 66.45% - prec: 71.54% - rec: 71.13% - f1: 71.06%\n",
      "Epoch   6/100 - acc: 66.62% - prec: 71.42% - rec: 71.06% - f1: 71.42%\n",
      "Epoch   7/100 - acc: 66.52% - prec: 71.88% - rec: 71.75% - f1: 71.56%\n",
      "Epoch   8/100 - acc: 66.56% - prec: 71.81% - rec: 72.20% - f1: 72.24%\n",
      "Epoch   9/100 - acc: 66.26% - prec: 72.27% - rec: 72.40% - f1: 72.36%\n",
      "Epoch  10/100 - acc: 66.00% - prec: 72.56% - rec: 72.31% - f1: 72.41%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 66.00% | Precision: 72.56% | Recall: 72.31% | F1: 72.41%\n",
      "\n",
      "========== BART | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 66.25% - prec: 72.97% - rec: 72.88% - f1: 72.95%\n",
      "Epoch  12/100 - acc: 66.44% - prec: 73.66% - rec: 73.42% - f1: 73.29%\n",
      "Epoch  13/100 - acc: 66.12% - prec: 73.54% - rec: 73.26% - f1: 73.49%\n",
      "Epoch  14/100 - acc: 66.49% - prec: 73.52% - rec: 73.39% - f1: 73.10%\n",
      "Epoch  15/100 - acc: 66.16% - prec: 73.95% - rec: 73.92% - f1: 73.95%\n",
      "Epoch  16/100 - acc: 66.13% - prec: 73.95% - rec: 74.43% - f1: 74.03%\n",
      "Epoch  17/100 - acc: 66.22% - prec: 74.31% - rec: 74.45% - f1: 74.68%\n",
      "Epoch  18/100 - acc: 66.91% - prec: 74.79% - rec: 74.64% - f1: 75.22%\n",
      "Epoch  19/100 - acc: 66.52% - prec: 75.34% - rec: 75.26% - f1: 75.06%\n",
      "Epoch  20/100 - acc: 66.75% - prec: 75.59% - rec: 75.23% - f1: 75.18%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 66.75% | Precision: 75.59% | Recall: 75.23% | F1: 75.18%\n",
      "\n",
      "========== BART | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 66.96% - prec: 75.45% - rec: 75.38% - f1: 75.70%\n",
      "Epoch  22/100 - acc: 66.79% - prec: 75.93% - rec: 75.95% - f1: 76.02%\n",
      "Epoch  23/100 - acc: 66.81% - prec: 76.26% - rec: 75.92% - f1: 76.39%\n",
      "Epoch  24/100 - acc: 67.14% - prec: 76.49% - rec: 76.32% - f1: 76.54%\n",
      "Epoch  25/100 - acc: 67.57% - prec: 76.67% - rec: 76.87% - f1: 76.89%\n",
      "Epoch  26/100 - acc: 67.79% - prec: 77.40% - rec: 76.73% - f1: 77.34%\n",
      "Epoch  27/100 - acc: 67.47% - prec: 77.03% - rec: 77.09% - f1: 77.77%\n",
      "Epoch  28/100 - acc: 67.77% - prec: 77.89% - rec: 77.46% - f1: 77.68%\n",
      "Epoch  29/100 - acc: 68.13% - prec: 77.92% - rec: 77.67% - f1: 77.80%\n",
      "Epoch  30/100 - acc: 68.39% - prec: 78.60% - rec: 78.23% - f1: 78.15%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 68.39% | Precision: 78.60% | Recall: 78.23% | F1: 78.15%\n",
      "\n",
      "========== BART | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 68.74% - prec: 78.43% - rec: 78.22% - f1: 78.94%\n",
      "Epoch  32/100 - acc: 68.97% - prec: 78.79% - rec: 78.74% - f1: 78.76%\n",
      "Epoch  33/100 - acc: 69.05% - prec: 79.07% - rec: 78.61% - f1: 79.22%\n",
      "Epoch  34/100 - acc: 69.35% - prec: 79.28% - rec: 79.14% - f1: 79.54%\n",
      "Epoch  35/100 - acc: 70.04% - prec: 79.95% - rec: 79.36% - f1: 79.53%\n",
      "Epoch  36/100 - acc: 70.58% - prec: 80.17% - rec: 79.31% - f1: 80.45%\n",
      "Epoch  37/100 - acc: 69.95% - prec: 80.00% - rec: 79.77% - f1: 80.17%\n",
      "Epoch  38/100 - acc: 71.71% - prec: 80.84% - rec: 80.29% - f1: 80.60%\n",
      "Epoch  39/100 - acc: 71.99% - prec: 80.79% - rec: 80.50% - f1: 80.90%\n",
      "Epoch  40/100 - acc: 72.98% - prec: 81.24% - rec: 80.33% - f1: 80.79%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 72.98% | Precision: 81.24% | Recall: 80.33% | F1: 80.79%\n",
      "\n",
      "========== BART | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 73.13% - prec: 81.47% - rec: 80.82% - f1: 81.27%\n",
      "Epoch  42/100 - acc: 74.17% - prec: 81.69% - rec: 81.01% - f1: 82.14%\n",
      "Epoch  43/100 - acc: 74.55% - prec: 82.05% - rec: 81.72% - f1: 81.84%\n",
      "Epoch  44/100 - acc: 75.38% - prec: 82.29% - rec: 81.52% - f1: 82.39%\n",
      "Epoch  45/100 - acc: 76.14% - prec: 82.50% - rec: 82.26% - f1: 82.53%\n",
      "Epoch  46/100 - acc: 76.77% - prec: 82.89% - rec: 82.24% - f1: 83.03%\n",
      "Epoch  47/100 - acc: 78.42% - prec: 83.49% - rec: 82.64% - f1: 82.88%\n",
      "Epoch  48/100 - acc: 79.11% - prec: 83.40% - rec: 82.94% - f1: 83.47%\n",
      "Epoch  49/100 - acc: 79.53% - prec: 83.26% - rec: 83.06% - f1: 83.49%\n",
      "Epoch  50/100 - acc: 80.08% - prec: 84.23% - rec: 83.63% - f1: 83.84%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 80.08% | Precision: 84.23% | Recall: 83.63% | F1: 83.84%\n",
      "\n",
      "========== BART | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 81.84% - prec: 84.16% - rec: 83.64% - f1: 84.02%\n",
      "Epoch  52/100 - acc: 82.38% - prec: 84.90% - rec: 84.15% - f1: 84.49%\n",
      "Epoch  53/100 - acc: 83.45% - prec: 84.65% - rec: 84.15% - f1: 84.81%\n",
      "Epoch  54/100 - acc: 84.69% - prec: 85.13% - rec: 84.65% - f1: 85.30%\n",
      "Epoch  55/100 - acc: 85.07% - prec: 85.58% - rec: 84.77% - f1: 85.44%\n",
      "Epoch  56/100 - acc: 86.17% - prec: 85.69% - rec: 85.23% - f1: 85.38%\n",
      "Epoch  57/100 - acc: 86.32% - prec: 85.70% - rec: 85.37% - f1: 86.05%\n",
      "Epoch  58/100 - acc: 88.10% - prec: 86.12% - rec: 85.81% - f1: 85.96%\n",
      "Epoch  59/100 - acc: 88.38% - prec: 86.19% - rec: 85.92% - f1: 86.46%\n",
      "Epoch  60/100 - acc: 89.00% - prec: 86.99% - rec: 86.57% - f1: 86.92%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 89.00% | Precision: 86.99% | Recall: 86.57% | F1: 86.92%\n",
      "\n",
      "========== BART | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 89.83% - prec: 86.79% - rec: 86.42% - f1: 87.21%\n",
      "Epoch  62/100 - acc: 90.66% - prec: 87.13% - rec: 86.61% - f1: 87.55%\n",
      "Epoch  63/100 - acc: 91.31% - prec: 87.76% - rec: 87.10% - f1: 87.69%\n",
      "Epoch  64/100 - acc: 91.38% - prec: 87.89% - rec: 87.42% - f1: 87.96%\n",
      "Epoch  65/100 - acc: 91.75% - prec: 88.17% - rec: 87.59% - f1: 88.59%\n",
      "Epoch  66/100 - acc: 92.21% - prec: 88.63% - rec: 88.19% - f1: 88.87%\n",
      "Epoch  67/100 - acc: 92.88% - prec: 89.08% - rec: 88.11% - f1: 88.76%\n",
      "Epoch  68/100 - acc: 93.17% - prec: 89.18% - rec: 88.28% - f1: 88.71%\n",
      "Epoch  69/100 - acc: 92.98% - prec: 89.41% - rec: 88.53% - f1: 89.24%\n",
      "Epoch  70/100 - acc: 93.55% - prec: 89.61% - rec: 88.98% - f1: 89.82%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 93.55% | Precision: 89.61% | Recall: 88.98% | F1: 89.82%\n",
      "\n",
      "========== BART | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 93.97% - prec: 90.05% - rec: 88.86% - f1: 89.88%\n",
      "Epoch  72/100 - acc: 94.26% - prec: 90.32% - rec: 89.93% - f1: 89.83%\n",
      "Epoch  73/100 - acc: 94.42% - prec: 90.44% - rec: 89.80% - f1: 90.63%\n",
      "Epoch  74/100 - acc: 94.22% - prec: 90.66% - rec: 90.11% - f1: 91.03%\n",
      "Epoch  75/100 - acc: 94.48% - prec: 91.18% - rec: 89.93% - f1: 90.75%\n",
      "Epoch  76/100 - acc: 95.65% - prec: 91.24% - rec: 90.24% - f1: 91.30%\n",
      "Epoch  77/100 - acc: 94.97% - prec: 91.88% - rec: 91.20% - f1: 91.50%\n",
      "Epoch  78/100 - acc: 95.03% - prec: 91.84% - rec: 91.08% - f1: 91.49%\n",
      "Epoch  79/100 - acc: 95.65% - prec: 92.59% - rec: 91.84% - f1: 92.03%\n",
      "Epoch  80/100 - acc: 95.36% - prec: 92.54% - rec: 91.66% - f1: 92.72%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 95.36% | Precision: 92.54% | Recall: 91.66% | F1: 92.72%\n",
      "\n",
      "========== BART | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 95.42% - prec: 92.86% - rec: 92.18% - f1: 92.58%\n",
      "Epoch  82/100 - acc: 95.50% - prec: 93.17% - rec: 92.42% - f1: 92.90%\n",
      "Epoch  83/100 - acc: 95.67% - prec: 93.22% - rec: 92.46% - f1: 93.26%\n",
      "Epoch  84/100 - acc: 95.43% - prec: 93.48% - rec: 93.13% - f1: 93.98%\n",
      "Epoch  85/100 - acc: 95.73% - prec: 93.79% - rec: 93.14% - f1: 93.99%\n",
      "Epoch  86/100 - acc: 95.78% - prec: 94.22% - rec: 93.48% - f1: 93.97%\n",
      "Epoch  87/100 - acc: 95.61% - prec: 94.33% - rec: 93.46% - f1: 94.40%\n",
      "Epoch  88/100 - acc: 96.30% - prec: 94.62% - rec: 93.85% - f1: 94.85%\n",
      "Epoch  89/100 - acc: 96.27% - prec: 95.18% - rec: 93.66% - f1: 95.08%\n",
      "Epoch  90/100 - acc: 96.30% - prec: 95.24% - rec: 94.75% - f1: 95.29%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 96.30% | Precision: 95.24% | Recall: 94.75% | F1: 95.29%\n",
      "\n",
      "========== BART | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 95.86% - prec: 95.76% - rec: 94.67% - f1: 95.52%\n",
      "Epoch  92/100 - acc: 95.75% - prec: 95.96% - rec: 95.21% - f1: 96.01%\n",
      "Epoch  93/100 - acc: 96.30% - prec: 96.38% - rec: 95.48% - f1: 95.85%\n",
      "Epoch  94/100 - acc: 96.26% - prec: 96.45% - rec: 95.53% - f1: 96.73%\n",
      "Epoch  95/100 - acc: 96.30% - prec: 96.59% - rec: 95.90% - f1: 96.49%\n",
      "Epoch  96/100 - acc: 96.15% - prec: 96.74% - rec: 96.28% - f1: 96.64%\n",
      "Epoch  97/100 - acc: 95.69% - prec: 97.43% - rec: 96.29% - f1: 97.58%\n",
      "Epoch  98/100 - acc: 96.30% - prec: 98.03% - rec: 96.83% - f1: 97.60%\n",
      "Epoch  99/100 - acc: 96.30% - prec: 98.08% - rec: 96.98% - f1: 98.19%\n",
      "Epoch 100/100 - acc: 95.79% - prec: 98.49% - rec: 97.49% - f1: 98.17%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 95.79% | Precision: 98.49% | Recall: 97.49% | F1: 98.17%\n",
      "\n",
      ">>> BART Final CV Results (10 folds)\n",
      "Accuracy: 96.30\n",
      "Precision: 98.26\n",
      "Recall: 97.15\n",
      "F1: 98.17\n",
      "============================================================\n",
      "\n",
      "========== MiniLM Training (10-Fold CV) ==========\n",
      "Model Spec: 6 layers, 384 hidden, 12 heads\n",
      "Best Params: {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "========== MiniLM | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 64.11% - prec: 69.68% - rec: 69.99% - f1: 70.01%\n",
      "Epoch   2/100 - acc: 64.35% - prec: 70.46% - rec: 70.29% - f1: 70.56%\n",
      "Epoch   3/100 - acc: 64.00% - prec: 70.57% - rec: 70.75% - f1: 70.23%\n",
      "Epoch   4/100 - acc: 64.30% - prec: 70.46% - rec: 70.58% - f1: 71.22%\n",
      "Epoch   5/100 - acc: 64.00% - prec: 70.86% - rec: 71.29% - f1: 71.58%\n",
      "Epoch   6/100 - acc: 64.36% - prec: 71.43% - rec: 71.37% - f1: 71.52%\n",
      "Epoch   7/100 - acc: 64.29% - prec: 71.26% - rec: 71.86% - f1: 71.70%\n",
      "Epoch   8/100 - acc: 64.33% - prec: 71.63% - rec: 71.73% - f1: 71.58%\n",
      "Epoch   9/100 - acc: 64.00% - prec: 71.69% - rec: 72.28% - f1: 72.61%\n",
      "Epoch  10/100 - acc: 64.97% - prec: 71.96% - rec: 72.38% - f1: 72.41%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 64.97% | Precision: 71.96% | Recall: 72.38% | F1: 72.41%\n",
      "\n",
      "========== MiniLM | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 64.14% - prec: 72.12% - rec: 72.94% - f1: 72.72%\n",
      "Epoch  12/100 - acc: 64.38% - prec: 72.70% - rec: 72.98% - f1: 73.27%\n",
      "Epoch  13/100 - acc: 64.20% - prec: 73.00% - rec: 73.47% - f1: 73.53%\n",
      "Epoch  14/100 - acc: 64.21% - prec: 73.11% - rec: 73.54% - f1: 73.89%\n",
      "Epoch  15/100 - acc: 64.10% - prec: 73.65% - rec: 73.91% - f1: 74.08%\n",
      "Epoch  16/100 - acc: 64.31% - prec: 74.06% - rec: 74.02% - f1: 74.14%\n",
      "Epoch  17/100 - acc: 65.00% - prec: 74.13% - rec: 74.75% - f1: 74.29%\n",
      "Epoch  18/100 - acc: 64.79% - prec: 74.50% - rec: 74.78% - f1: 74.85%\n",
      "Epoch  19/100 - acc: 64.75% - prec: 74.39% - rec: 75.18% - f1: 75.57%\n",
      "Epoch  20/100 - acc: 65.40% - prec: 74.82% - rec: 75.12% - f1: 75.72%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 65.40% | Precision: 74.82% | Recall: 75.12% | F1: 75.72%\n",
      "\n",
      "========== MiniLM | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 65.44% - prec: 74.66% - rec: 75.48% - f1: 75.81%\n",
      "Epoch  22/100 - acc: 65.02% - prec: 75.35% - rec: 75.49% - f1: 76.43%\n",
      "Epoch  23/100 - acc: 64.78% - prec: 75.44% - rec: 75.60% - f1: 76.69%\n",
      "Epoch  24/100 - acc: 65.53% - prec: 75.97% - rec: 75.86% - f1: 77.01%\n",
      "Epoch  25/100 - acc: 66.21% - prec: 75.76% - rec: 76.45% - f1: 77.00%\n",
      "Epoch  26/100 - acc: 65.53% - prec: 76.16% - rec: 76.74% - f1: 77.22%\n",
      "Epoch  27/100 - acc: 65.75% - prec: 76.36% - rec: 76.85% - f1: 77.48%\n",
      "Epoch  28/100 - acc: 65.80% - prec: 77.21% - rec: 77.15% - f1: 77.81%\n",
      "Epoch  29/100 - acc: 66.71% - prec: 76.91% - rec: 77.35% - f1: 78.32%\n",
      "Epoch  30/100 - acc: 66.26% - prec: 77.23% - rec: 77.74% - f1: 78.33%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 66.26% | Precision: 77.23% | Recall: 77.74% | F1: 78.33%\n",
      "\n",
      "========== MiniLM | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 67.16% - prec: 77.92% - rec: 77.98% - f1: 78.88%\n",
      "Epoch  32/100 - acc: 66.71% - prec: 77.60% - rec: 78.78% - f1: 78.90%\n",
      "Epoch  33/100 - acc: 67.78% - prec: 78.07% - rec: 79.16% - f1: 79.55%\n",
      "Epoch  34/100 - acc: 68.01% - prec: 78.36% - rec: 78.61% - f1: 79.58%\n",
      "Epoch  35/100 - acc: 68.03% - prec: 78.64% - rec: 79.38% - f1: 80.01%\n",
      "Epoch  36/100 - acc: 68.97% - prec: 78.48% - rec: 79.44% - f1: 80.58%\n",
      "Epoch  37/100 - acc: 69.66% - prec: 78.99% - rec: 79.18% - f1: 80.46%\n",
      "Epoch  38/100 - acc: 70.23% - prec: 79.26% - rec: 79.86% - f1: 80.85%\n",
      "Epoch  39/100 - acc: 70.72% - prec: 79.49% - rec: 80.01% - f1: 81.18%\n",
      "Epoch  40/100 - acc: 71.42% - prec: 79.70% - rec: 80.60% - f1: 81.53%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 71.42% | Precision: 79.70% | Recall: 80.60% | F1: 81.53%\n",
      "\n",
      "========== MiniLM | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 72.32% - prec: 79.91% - rec: 80.96% - f1: 81.52%\n",
      "Epoch  42/100 - acc: 73.17% - prec: 80.29% - rec: 81.06% - f1: 82.46%\n",
      "Epoch  43/100 - acc: 73.85% - prec: 80.47% - rec: 81.60% - f1: 82.27%\n",
      "Epoch  44/100 - acc: 74.47% - prec: 80.90% - rec: 81.64% - f1: 82.15%\n",
      "Epoch  45/100 - acc: 75.76% - prec: 80.71% - rec: 81.65% - f1: 82.47%\n",
      "Epoch  46/100 - acc: 76.35% - prec: 81.45% - rec: 81.66% - f1: 83.25%\n",
      "Epoch  47/100 - acc: 77.21% - prec: 81.22% - rec: 82.59% - f1: 83.08%\n",
      "Epoch  48/100 - acc: 78.12% - prec: 81.92% - rec: 82.66% - f1: 83.62%\n",
      "Epoch  49/100 - acc: 78.49% - prec: 82.15% - rec: 82.77% - f1: 83.99%\n",
      "Epoch  50/100 - acc: 80.92% - prec: 82.41% - rec: 83.20% - f1: 84.01%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 80.92% | Precision: 82.41% | Recall: 83.20% | F1: 84.01%\n",
      "\n",
      "========== MiniLM | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 81.90% - prec: 82.63% - rec: 83.50% - f1: 84.67%\n",
      "Epoch  52/100 - acc: 82.60% - prec: 83.14% - rec: 84.01% - f1: 84.78%\n",
      "Epoch  53/100 - acc: 83.37% - prec: 83.01% - rec: 83.86% - f1: 85.19%\n",
      "Epoch  54/100 - acc: 84.54% - prec: 83.66% - rec: 84.54% - f1: 85.42%\n",
      "Epoch  55/100 - acc: 85.59% - prec: 83.45% - rec: 84.32% - f1: 85.83%\n",
      "Epoch  56/100 - acc: 85.90% - prec: 83.42% - rec: 85.36% - f1: 86.35%\n",
      "Epoch  57/100 - acc: 86.91% - prec: 84.32% - rec: 84.88% - f1: 86.28%\n",
      "Epoch  58/100 - acc: 87.96% - prec: 83.93% - rec: 85.48% - f1: 86.45%\n",
      "Epoch  59/100 - acc: 88.22% - prec: 84.76% - rec: 85.58% - f1: 86.78%\n",
      "Epoch  60/100 - acc: 89.57% - prec: 84.68% - rec: 85.75% - f1: 87.10%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 89.57% | Precision: 84.68% | Recall: 85.75% | F1: 87.10%\n",
      "\n",
      "========== MiniLM | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 90.16% - prec: 84.99% - rec: 86.05% - f1: 87.39%\n",
      "Epoch  62/100 - acc: 90.86% - prec: 85.14% - rec: 86.43% - f1: 87.86%\n",
      "Epoch  63/100 - acc: 91.51% - prec: 85.66% - rec: 86.60% - f1: 87.98%\n",
      "Epoch  64/100 - acc: 92.10% - prec: 85.69% - rec: 87.29% - f1: 88.27%\n",
      "Epoch  65/100 - acc: 92.84% - prec: 86.14% - rec: 87.35% - f1: 88.52%\n",
      "Epoch  66/100 - acc: 92.77% - prec: 86.55% - rec: 87.46% - f1: 88.63%\n",
      "Epoch  67/100 - acc: 93.46% - prec: 86.51% - rec: 87.69% - f1: 89.01%\n",
      "Epoch  68/100 - acc: 93.91% - prec: 86.54% - rec: 87.82% - f1: 89.30%\n",
      "Epoch  69/100 - acc: 94.13% - prec: 87.16% - rec: 88.59% - f1: 89.71%\n",
      "Epoch  70/100 - acc: 94.87% - prec: 87.51% - rec: 88.44% - f1: 89.94%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 94.87% | Precision: 87.51% | Recall: 88.44% | F1: 89.94%\n",
      "\n",
      "========== MiniLM | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 95.00% - prec: 87.58% - rec: 89.24% - f1: 90.14%\n",
      "Epoch  72/100 - acc: 95.11% - prec: 87.88% - rec: 89.50% - f1: 90.55%\n",
      "Epoch  73/100 - acc: 95.69% - prec: 88.08% - rec: 89.25% - f1: 91.42%\n",
      "Epoch  74/100 - acc: 96.17% - prec: 88.38% - rec: 89.89% - f1: 90.92%\n",
      "Epoch  75/100 - acc: 96.15% - prec: 88.50% - rec: 89.96% - f1: 91.47%\n",
      "Epoch  76/100 - acc: 96.39% - prec: 88.62% - rec: 90.41% - f1: 92.14%\n",
      "Epoch  77/100 - acc: 96.49% - prec: 89.31% - rec: 90.50% - f1: 92.18%\n",
      "Epoch  78/100 - acc: 96.31% - prec: 89.18% - rec: 91.05% - f1: 92.05%\n",
      "Epoch  79/100 - acc: 96.30% - prec: 89.77% - rec: 90.82% - f1: 92.75%\n",
      "Epoch  80/100 - acc: 97.24% - prec: 89.95% - rec: 91.27% - f1: 92.86%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 97.24% | Precision: 89.95% | Recall: 91.27% | F1: 92.86%\n",
      "\n",
      "========== MiniLM | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 96.61% - prec: 90.01% - rec: 91.70% - f1: 92.96%\n",
      "Epoch  82/100 - acc: 96.88% - prec: 89.92% - rec: 91.81% - f1: 93.57%\n",
      "Epoch  83/100 - acc: 96.77% - prec: 90.39% - rec: 91.98% - f1: 93.80%\n",
      "Epoch  84/100 - acc: 97.11% - prec: 90.47% - rec: 92.50% - f1: 93.54%\n",
      "Epoch  85/100 - acc: 96.43% - prec: 90.80% - rec: 92.57% - f1: 94.50%\n",
      "Epoch  86/100 - acc: 97.16% - prec: 90.88% - rec: 92.85% - f1: 94.85%\n",
      "Epoch  87/100 - acc: 97.63% - prec: 91.47% - rec: 93.13% - f1: 94.99%\n",
      "Epoch  88/100 - acc: 97.38% - prec: 91.49% - rec: 93.73% - f1: 95.22%\n",
      "Epoch  89/100 - acc: 97.63% - prec: 92.12% - rec: 93.63% - f1: 95.80%\n",
      "Epoch  90/100 - acc: 97.63% - prec: 92.25% - rec: 93.66% - f1: 95.78%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 97.63% | Precision: 92.25% | Recall: 93.66% | F1: 95.78%\n",
      "\n",
      "========== MiniLM | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 97.63% - prec: 92.64% - rec: 94.19% - f1: 95.92%\n",
      "Epoch  92/100 - acc: 97.20% - prec: 92.80% - rec: 94.55% - f1: 95.90%\n",
      "Epoch  93/100 - acc: 97.24% - prec: 92.97% - rec: 95.04% - f1: 96.41%\n",
      "Epoch  94/100 - acc: 97.46% - prec: 93.15% - rec: 94.88% - f1: 97.15%\n",
      "Epoch  95/100 - acc: 97.63% - prec: 93.32% - rec: 95.39% - f1: 97.11%\n",
      "Epoch  96/100 - acc: 97.63% - prec: 93.70% - rec: 95.56% - f1: 97.78%\n",
      "Epoch  97/100 - acc: 97.28% - prec: 94.08% - rec: 96.06% - f1: 97.63%\n",
      "Epoch  98/100 - acc: 97.63% - prec: 94.11% - rec: 96.23% - f1: 98.02%\n",
      "Epoch  99/100 - acc: 97.59% - prec: 94.61% - rec: 96.46% - f1: 98.41%\n",
      "Epoch 100/100 - acc: 97.63% - prec: 94.81% - rec: 96.85% - f1: 98.87%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 97.63% | Precision: 94.81% | Recall: 96.85% | F1: 98.87%\n",
      "\n",
      ">>> MiniLM Final CV Results (10 folds)\n",
      "Accuracy: 97.63\n",
      "Precision: 94.74\n",
      "Recall: 96.74\n",
      "F1: 98.74\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Transformer Embeddings + SVM-RBF Classifier\n",
    "10-Fold Cross Validation with Best Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "FOLDS = 10\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------------------\n",
    "# Embedding extraction\n",
    "# ---------------------------\n",
    "def get_embeddings(model_name, texts, batch_size=16, max_len=128):\n",
    "    \"\"\"Extract CLS embeddings from transformer\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Embedding {model_name}\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            outputs = model(**enc)\n",
    "            # Take [CLS] token (first hidden state)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# ---------------------------\n",
    "# Cross-validation + SVM\n",
    "# ---------------------------\n",
    "def run_cv(X, y, model_name):\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Hyperparameter grid for SVM-RBF\n",
    "    param_grid = {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"gamma\": [\"scale\", \"auto\", 0.01, 0.001, 0.005],\n",
    "        \"kernel\": [\"rbf\"],\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n=== {model_name} | Fold {fold}/{FOLDS} ===\")\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = np.array(y)[train_idx], np.array(y)[val_idx]\n",
    "\n",
    "        svm = SVC()\n",
    "        grid = GridSearchCV(svm, param_grid, scoring=\"f1_macro\", cv=3, n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_val)\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred) * 100\n",
    "        prec = precision_score(y_val, y_pred, average=\"macro\") * 100\n",
    "        rec = recall_score(y_val, y_pred, average=\"macro\") * 100\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\") * 100\n",
    "\n",
    "        print(f\"Best Params: {grid.best_params_}\")\n",
    "        print(f\"Fold {fold} -> Acc: {acc:.2f} | Prec: {prec:.2f} | Rec: {rec:.2f} | F1: {f1:.2f}\")\n",
    "\n",
    "        all_results.append({\n",
    "            \"fold\": fold,\n",
    "            \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1,\n",
    "            \"best_params\": grid.best_params_\n",
    "        })\n",
    "\n",
    "    # Aggregate results\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    mean_results = df_results.mean(numeric_only=True).to_dict()\n",
    "\n",
    "    return mean_results, all_results\n",
    "\n",
    "# ---------------------------\n",
    "# Run pipeline\n",
    "# ---------------------------\n",
    "# ---------------------------\n",
    "# Run pipeline\n",
    "# ---------------------------\n",
    "final_summary = {}\n",
    "\n",
    "for model_name, model_ckpt in transformer_models.items():\n",
    "    print(\"\\n\" + \"=\"*20)\n",
    "    print(f\" Running {model_name} ({model_ckpt})\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "    embeddings = get_embeddings(model_ckpt, texts)\n",
    "    mean_results, all_folds = run_cv(embeddings, labels, model_name)\n",
    "\n",
    "    # Save results\n",
    "    final_summary[model_name] = {\n",
    "        \"accuracy\": mean_results[\"acc\"],\n",
    "        \"precision\": mean_results[\"prec\"],\n",
    "        \"recall\": mean_results[\"rec\"],\n",
    "        \"f1\": mean_results[\"f1\"],\n",
    "    }\n",
    "\n",
    "    # --- Print final results block ---\n",
    "    print(f\"\\n>>> {model_name} Final CV Results ({FOLDS} folds)\")\n",
    "    print(f\"Accuracy: {mean_results['acc']:.2f}\")\n",
    "    print(f\"Precision: {mean_results['prec']:.2f}\")\n",
    "    print(f\"Recall: {mean_results['rec']:.2f}\")\n",
    "    print(f\"F1: {mean_results['f1']:.2f}\")\n",
    "    print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24855a-68ca-4b50-89c3-4746a68f0cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
