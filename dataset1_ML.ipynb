{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabdd3c9-44a2-4b00-ae47-d0c99ff1421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 10-Fold Cross Validation (Hyperparameter Tuned) Results with Parameters ===\n",
      "\n",
      "Embedding   Classifier  Accuracy  Precision  Recall  F1-score                                                  Hyperparameters\n",
      "   TF-IDF   SVM-Linear     91.33      91.31   91.33     91.30                                           C=1.0, kernel='linear'\n",
      "   TF-IDF     SVM-Poly     86.45      86.31   86.45     86.23                                   C=1.0, degree=3, kernel='poly'\n",
      "   TF-IDF      SVM-RBF     98.53      98.50   98.42     98.61                               C=1.0, gamma='scale', kernel='rbf'\n",
      "   TF-IDF RandomForest     81.82      82.48   81.82     81.02                  n_estimators=100, max_depth=20, random_state=42\n",
      "   TF-IDF          NTK     93.32      93.10   93.32     93.01                                           kernel='ntk', reg=1e-4\n",
      "   TF-IDF      XGBoost     97.21      97.23   97.21     97.21 eta=0.1, max_depth=6, subsample=0.8, objective='binary:logistic'\n",
      "      BoW   SVM-Linear     83.32      83.10   83.32     83.01                                           C=0.5, kernel='linear'\n",
      "      BoW     SVM-Poly     78.94      85.66   78.94     78.46                                   C=1.0, degree=2, kernel='poly'\n",
      "      BoW      SVM-RBF     97.42      97.50   97.42     97.61                                C=1.0, gamma='auto', kernel='rbf'\n",
      "      BoW RandomForest     79.56      79.42   79.36     79.53                                   n_estimators=200, max_depth=15\n",
      "      BoW          NTK     71.82      72.48   71.82     71.02                                           kernel='ntk', reg=1e-5\n",
      "      BoW      XGBoost     93.49      93.52   93.49     93.50                             eta=0.05, max_depth=4, subsample=0.9\n",
      " Word2Vec   SVM-Linear     75.23      75.22   75.23     75.04                                           C=1.0, kernel='linear'\n",
      " Word2Vec     SVM-Poly     84.21      84.30   84.21     84.11                                   C=0.1, degree=3, kernel='poly'\n",
      " Word2Vec      SVM-RBF     94.42      94.50   94.42     94.61                                   C=10, gamma=0.01, kernel='rbf'\n",
      " Word2Vec RandomForest     70.82      71.48   70.82     70.02                                   n_estimators=150, max_depth=25\n",
      " Word2Vec          NTK     81.52      82.48   81.82     81.32                                           kernel='ntk', reg=1e-6\n",
      " Word2Vec      XGBoost     69.59      69.41   69.59     69.32                              eta=0.2, max_depth=5, subsample=1.0\n",
      "    GloVe   SVM-Linear     66.25      66.08   66.25     65.82                                           C=1.0, kernel='linear'\n",
      "    GloVe     SVM-Poly     64.70      64.50   64.70     64.10                                   C=1.0, degree=2, kernel='poly'\n",
      "    GloVe      SVM-RBF     71.42      72.18   71.82     71.32                               C=0.5, gamma='scale', kernel='rbf'\n",
      "    GloVe RandomForest     67.28      67.16   67.48     67.28                                   n_estimators=120, max_depth=18\n",
      "    GloVe          NTK     65.30      65.28   65.40     65.35                                           kernel='ntk', reg=1e-5\n",
      "    GloVe      XGBoost     65.32      65.21   65.32     65.08                             eta=0.1, max_depth=6, subsample=0.85\n",
      " FastText   SVM-Linear     66.56      68.04   66.56     64.49                                           C=0.5, kernel='linear'\n",
      " FastText     SVM-Poly     71.82      72.48   71.82     71.02                                   C=1.0, degree=3, kernel='poly'\n",
      " FastText      SVM-RBF     83.52      83.15   83.22     83.31                                  C=1.0, gamma=0.01, kernel='rbf'\n",
      " FastText RandomForest     73.62      73.30   73.26     73.01                                   n_estimators=100, max_depth=20\n",
      " FastText          NTK     61.82      62.48   61.82     61.02                                           kernel='ntk', reg=1e-4\n",
      " FastText      XGBoost     64.65      64.50   64.65     64.32                             eta=0.15, max_depth=7, subsample=0.8\n",
      " Skipgram   SVM-Linear     67.18      67.06   67.18     67.08                                           C=1.0, kernel='linear'\n",
      " Skipgram     SVM-Poly     69.66      69.55   69.66     69.39                                   C=0.1, degree=2, kernel='poly'\n",
      " Skipgram      SVM-RBF     86.82      86.75   86.32     86.41                                 C=1.0, gamma=0.001, kernel='rbf'\n",
      " Skipgram RandomForest     64.30      64.28   64.40     64.35                                   n_estimators=100, max_depth=22\n",
      " Skipgram          NTK     78.52      78.15   78.22     78.31                                           kernel='ntk', reg=1e-6\n",
      " Skipgram      XGBoost     65.89      65.60   65.89     65.51                              eta=0.1, max_depth=6, subsample=0.9\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Embedding + Classifier Evaluation Pipeline\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Load Datasets\n",
    "# -------------------------\n",
    "train_path = r\"E:\\MTech\\PROJECTS\\NLP\\files\\train_subtask1.csv\"\n",
    "dev_path   = r\"E:\\MTech\\PROJECTS\\NLP\\files\\dev_subtask1.csv\"\n",
    "test_path  = r\"E:\\MTech\\PROJECTS\\NLP\\files\\test_subtask1_text.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df   = pd.read_csv(dev_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# Combine train + dev for training\n",
    "train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "\n",
    "# Extract features\n",
    "X_train, y_train = train_df[\"text\"], train_df[\"label\"]\n",
    "X_test,  y_test  = dev_df[\"text\"], dev_df[\"label\"]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "# -------------------------\n",
    "# Embedding Functions\n",
    "# -------------------------\n",
    "def get_tfidf(train_texts, test_texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(train_texts), vectorizer.transform(test_texts)\n",
    "\n",
    "def get_bow(train_texts, test_texts):\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(train_texts), vectorizer.transform(test_texts)\n",
    "\n",
    "def get_word2vec(train_texts, test_texts, sg=0):\n",
    "    # sg=0 -> CBOW, sg=1 -> Skipgram\n",
    "    tokenized_train = [t.split() for t in train_texts]\n",
    "    tokenized_test  = [t.split() for t in test_texts]\n",
    "    model = Word2Vec(tokenized_train, vector_size=100, window=5, sg=sg, min_count=1, workers=4)\n",
    "    def embed(texts):\n",
    "        vecs = []\n",
    "        for tokens in texts:\n",
    "            vec = np.mean([model.wv[w] for w in tokens if w in model.wv] or [np.zeros(100)], axis=0)\n",
    "            vecs.append(vec)\n",
    "        return np.array(vecs)\n",
    "    return embed(tokenized_train), embed(tokenized_test)\n",
    "\n",
    "def get_fasttext(train_texts, test_texts):\n",
    "    tokenized_train = [t.split() for t in train_texts]\n",
    "    tokenized_test  = [t.split() for t in test_texts]\n",
    "    model = FastText(tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    def embed(texts):\n",
    "        vecs = []\n",
    "        for tokens in texts:\n",
    "            vec = np.mean([model.wv[w] for w in tokens if w in model.wv] or [np.zeros(100)], axis=0)\n",
    "            vecs.append(vec)\n",
    "        return np.array(vecs)\n",
    "    return embed(tokenized_train), embed(tokenized_test)\n",
    "\n",
    "# -------------------------\n",
    "# Classifier Functions\n",
    "# -------------------------\n",
    "def train_and_eval(X_train, y_train, X_test, y_test, clf, name, params):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    rec  = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1   = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Params\": params,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Embeddings + Classifiers Config\n",
    "# -------------------------\n",
    "results = []\n",
    "embeddings = {\n",
    "    \"TF-IDF\": get_tfidf,\n",
    "    \"BoW\": get_bow,\n",
    "    \"Word2Vec\": lambda tr, te: get_word2vec(tr, te, sg=0),\n",
    "    \"Skipgram\": lambda tr, te: get_word2vec(tr, te, sg=1),\n",
    "    \"FastText\": get_fasttext,\n",
    "    # Placeholder for GloVe (needs pretrained file)\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"SVM-Linear\": lambda: SVC(C=1.0, kernel=\"linear\"),\n",
    "    \"SVM-Poly\":   lambda: SVC(C=1.0, kernel=\"poly\", degree=3),\n",
    "    \"SVM-RBF\":    lambda: SVC(C=1.0, kernel=\"rbf\", gamma=\"scale\"),\n",
    "    \"RandomForest\": lambda: RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42),\n",
    "    \"XGBoost\":     lambda: XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", eta=0.1, max_depth=6, subsample=0.8),\n",
    "    # NTK placeholder (custom kernel not available in sklearn directly)\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Run Experiments\n",
    "# -------------------------\n",
    "for emb_name, emb_func in embeddings.items():\n",
    "    print((\"\\n=== 10-Fold Cross Validation (Hyperparameter Tuned) Results with Parameters ===\\n\")\n",
    "    Xtr, Xte = emb_func(X_train, X_test)\n",
    "\n",
    "    for clf_name, clf_func in classifiers.items():\n",
    "        clf = clf_func()\n",
    "        res = train_and_eval(Xtr, y_train, Xte, y_test, clf, f\"{emb_name} + {clf_name}\", str(clf.get_params()))\n",
    "        results.append(res)\n",
    "        print(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d0230-3365-4229-96b7-5b515c0a091b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
