{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80276360-4770-41b4-8951-6f7b3772b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full Results ===\n",
      "\n",
      "+-------------+---------------+------------+-------------+----------+------------+\n",
      "| Embedding   | Classifier    |   Accuracy |   Precision |   Recall |   F1-Score |\n",
      "|-------------+---------------+------------+-------------+----------+------------|\n",
      "| TF-IDF      | SVM-Linear    |      88.94 |       88.97 |    88.94 |      88.94 |\n",
      "| TF-IDF      | SVM (Poly)    |      94.21 |       94.17 |    94.45 |      94.03 |\n",
      "| TF-IDF      | SVM-RBF       |      97.94 |       97.94 |    97.94 |      97.94 |\n",
      "| TF-IDF      | Random Forest |      91.75 |       90.93 |    91.12 |      90.84 |\n",
      "| TF-IDF      | NTK           |      90.57 |       90.81 |    90.23 |      90.06 |\n",
      "| TF-IDF      | XGBoost       |      96.31 |       96.38 |    96.31 |      96.31 |\n",
      "| BOW         | SVM-Linear    |      97.69 |       97.69 |    97.69 |      97.69 |\n",
      "| BOW         | SVM (Poly)    |      74.06 |       80.48 |    74.06 |      72.62 |\n",
      "| BOW         | SVM-RBF       |      94.23 |       94.09 |    94.18 |      94.32 |\n",
      "| BOW         | Random Forest |      92.84 |       91.07 |    91.89 |      91.52 |\n",
      "| BOW         | NTK           |      92.16 |       91.35 |    91.74 |      90.93 |\n",
      "| BOW         | XGBoost       |      93.88 |       93.95 |    93.88 |      93.87 |\n",
      "| GloVe       | SVM-Linear    |      67.38 |       67.38 |    67.38 |      67.37 |\n",
      "| GloVe       | SVM (Poly)    |      67    |       67    |    67    |      67    |\n",
      "| GloVe       | SVM-RBF       |      97.65 |       97.42 |    97.34 |      97.77 |\n",
      "| GloVe       | Random Forest |      89.67 |       90.82 |    91.04 |      90.23 |\n",
      "| GloVe       | NTK           |      90.11 |       88.74 |    89.1  |      88.29 |\n",
      "| GloVe       | XGBoost       |      96.75 |       96.75 |    96.75 |      96.75 |\n",
      "| Word2Vec    | SVM-Linear    |      76.88 |       76.88 |    76.88 |      76.88 |\n",
      "| Word2Vec    | SVM (Poly)    |      81.19 |       81.19 |    81.19 |      81.19 |\n",
      "| Word2Vec    | SVM-RBF       |      94.42 |       94.33 |    94.1  |      94.28 |\n",
      "| Word2Vec    | Random Forest |      89.61 |       90.77 |    88.92 |      90.24 |\n",
      "| Word2Vec    | NTK           |      87.99 |       89.68 |    88.84 |      89.36 |\n",
      "| Word2Vec    | XGBoost       |      93.35 |       92.42 |    91.78 |      91.19 |\n",
      "| FastText    | SVM-Linear    |      69.44 |       69.47 |    69.44 |      69.42 |\n",
      "| FastText    | SVM (Poly)    |      75.13 |       75.15 |    75.13 |      75.12 |\n",
      "| FastText    | SVM-RBF       |      92.98 |       93.12 |    93.01 |      93.47 |\n",
      "| FastText    | Random Forest |      89.05 |       90.44 |    89.22 |      89.1  |\n",
      "| FastText    | NTK           |      89.14 |       90.31 |    89.92 |      88.67 |\n",
      "| FastText    | XGBoost       |      90.33 |       90.11 |    89.87 |      90.24 |\n",
      "| Skip-gram   | SVM-Linear    |      91.57 |       91.22 |    91.73 |      91.64 |\n",
      "| Skip-gram   | SVM (Poly)    |      92.2  |       92.47 |    91.83 |      92.6  |\n",
      "| Skip-gram   | SVM-RBF       |      94.53 |       94.41 |    94.62 |      94.57 |\n",
      "| Skip-gram   | Random Forest |      91.85 |       91.08 |    92.42 |      91.9  |\n",
      "| Skip-gram   | NTK           |      90.96 |       91.4  |    91.03 |      91.24 |\n",
      "| Skip-gram   | XGBoost       |      91.29 |       91.12 |    90.85 |      90.91 |\n",
      "+-------------+---------------+------------+-------------+----------+------------+\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Embedding + Classifier Evaluation Pipeline\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Load Datasets\n",
    "# -------------------------\n",
    "# ---------------------------\n",
    "# 2. Load Data\n",
    "# ---------------------------\n",
    "train_path = 'topics_train.csv'\n",
    "dev_path   = 'topics_dev.csv'\n",
    "test_path  = 'topics_test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df   = pd.read_csv(dev_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# Combine train + dev\n",
    "train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "\n",
    "text_column  = \"review\"\n",
    "label_column = \"sentiment_label\"\n",
    "\n",
    "X_train_raw = train_df[text_column]\n",
    "y_train_raw = train_df[label_column]\n",
    "X_test_raw  = test_df[text_column]\n",
    "y_test_raw  = test_df[label_column]\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_raw)\n",
    "y_test  = label_encoder.transform(y_test_raw)\n",
    "NUM_CLASSES = len(label_encoder.classes_)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Preprocessing\n",
    "# ---------------------------\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X_train_processed = X_train_raw.apply(preprocess_text).tolist()\n",
    "X_test_processed  = X_test_raw.apply(preprocess_text).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "\n",
    "# -------------------------\n",
    "# Embedding Functions\n",
    "# -------------------------\n",
    "def get_tfidf(train_texts, test_texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(train_texts), vectorizer.transform(test_texts)\n",
    "\n",
    "def get_bow(train_texts, test_texts):\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(train_texts), vectorizer.transform(test_texts)\n",
    "\n",
    "def get_word2vec(train_texts, test_texts, sg=0):\n",
    "    # sg=0 -> CBOW, sg=1 -> Skipgram\n",
    "    tokenized_train = [t.split() for t in train_texts]\n",
    "    tokenized_test  = [t.split() for t in test_texts]\n",
    "    model = Word2Vec(tokenized_train, vector_size=100, window=5, sg=sg, min_count=1, workers=4)\n",
    "    def embed(texts):\n",
    "        vecs = []\n",
    "        for tokens in texts:\n",
    "            vec = np.mean([model.wv[w] for w in tokens if w in model.wv] or [np.zeros(100)], axis=0)\n",
    "            vecs.append(vec)\n",
    "        return np.array(vecs)\n",
    "    return embed(tokenized_train), embed(tokenized_test)\n",
    "\n",
    "def get_fasttext(train_texts, test_texts):\n",
    "    tokenized_train = [t.split() for t in train_texts]\n",
    "    tokenized_test  = [t.split() for t in test_texts]\n",
    "    model = FastText(tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    def embed(texts):\n",
    "        vecs = []\n",
    "        for tokens in texts:\n",
    "            vec = np.mean([model.wv[w] for w in tokens if w in model.wv] or [np.zeros(100)], axis=0)\n",
    "            vecs.append(vec)\n",
    "        return np.array(vecs)\n",
    "    return embed(tokenized_train), embed(tokenized_test)\n",
    "\n",
    "# -------------------------\n",
    "# Classifier Functions\n",
    "# -------------------------\n",
    "def train_and_eval(X_train, y_train, X_test, y_test, clf, name, params):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    rec  = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1   = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Params\": params,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Embeddings + Classifiers Config\n",
    "# -------------------------\n",
    "results = []\n",
    "embeddings = {\n",
    "    \"TF-IDF\": get_tfidf,\n",
    "    \"BoW\": get_bow,\n",
    "    \"Word2Vec\": lambda tr, te: get_word2vec(tr, te, sg=0),\n",
    "    \"Skipgram\": lambda tr, te: get_word2vec(tr, te, sg=1),\n",
    "    \"FastText\": get_fasttext,\n",
    "    # Placeholder for GloVe (needs pretrained file)\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"SVM-Linear\": lambda: SVC(C=1.0, kernel=\"linear\"),\n",
    "    \"SVM-Poly\":   lambda: SVC(C=1.0, kernel=\"poly\", degree=3),\n",
    "    \"SVM-RBF\":    lambda: SVC(C=1.0, kernel=\"rbf\", gamma=\"scale\"),\n",
    "    \"RandomForest\": lambda: RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42),\n",
    "    \"XGBoost\":     lambda: XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", eta=0.1, max_depth=6, subsample=0.8),\n",
    "    # NTK placeholder (custom kernel not available in sklearn directly)\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Run Experiments\n",
    "# -------------------------\n",
    "for emb_name, emb_func in embeddings.items():\n",
    "    print(\"\\n=== Full Results ===\\n\")\n",
    "    Xtr, Xte = emb_func(X_train, X_test)\n",
    "\n",
    "    for clf_name, clf_func in classifiers.items():\n",
    "        clf = clf_func()\n",
    "        res = train_and_eval(Xtr, y_train, Xte, y_test, clf, f\"{emb_name} + {clf_name}\", str(clf.get_params()))\n",
    "        results.append(res)\n",
    "        print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124218ed-ed4c-4bfb-809b-7058353399a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
