{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07ba1643-4965-4414-8721-b58095720252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Embedding roberta-base: 100%|██████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  8.62it/s]\n",
      "Embedding bert-base-uncased: 100%|█████████████████████████████████████████████████████| 19/19 [00:02<00:00,  8.80it/s]\n",
      "Embedding facebook/bart-base: 100%|████████████████████████████████████████████████████| 19/19 [00:02<00:00,  6.87it/s]\n",
      "Embedding nreimers/MiniLM-L6-H384-uncased: 100%|███████████████████████████████████████| 19/19 [00:00<00:00, 46.62it/s]\n",
      "Embedding distilbert-base-uncased: 100%|███████████████████████████████████████████████| 19/19 [00:01<00:00, 14.61it/s]\n",
      "Embedding microsoft/deberta-base: 100%|████████████████████████████████████████████████| 19/19 [00:03<00:00,  6.25it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Embedding roberta-base: 100%|████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.05it/s]\n",
      "Embedding bert-base-uncased: 100%|███████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.52it/s]\n",
      "Embedding facebook/bart-base: 100%|██████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.93it/s]\n",
      "Embedding nreimers/MiniLM-L6-H384-uncased: 100%|█████████████████████████████████████████| 5/5 [00:00<00:00, 55.47it/s]\n",
      "Embedding distilbert-base-uncased: 100%|█████████████████████████████████████████████████| 5/5 [00:00<00:00, 16.91it/s]\n",
      "Embedding microsoft/deberta-base: 100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Transformer Embedding Ensemble with Weighted Soft Voting\n",
    "Models: RoBERTa, BERT, BART, MiniLM, DistilBERT, DeBERTa\n",
    "Classifiers: RandomForest, GaussianNB, XGBoost, Linear SVM\n",
    "Evaluation: 10-fold Cross Validation\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Imports\n",
    "# ---------------------------\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Preprocessing\n",
    "# ---------------------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\w+|[^a-zA-Z\\s]\", \"\", text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "X_train = X_train_raw.apply(preprocess)\n",
    "X_test = X_test_raw.apply(preprocess)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Transformer Embeddings\n",
    "# ---------------------------\n",
    "print(\"Step 3: Generating embeddings...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer_models = {\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"bart\": \"facebook/bart-base\",\n",
    "    \"minilm\": \"nreimers/MiniLM-L6-H384-uncased\",\n",
    "    \"distilbert\": \"distilbert-base-uncased\",\n",
    "    \"deberta\": \"microsoft/deberta-base\"\n",
    "}\n",
    "\n",
    "def get_embeddings(texts, model_name, batch_size=16, max_len=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Embedding {model_name}\"):\n",
    "            batch = texts[i:i+batch_size].tolist()\n",
    "            encodings = tokenizer(\n",
    "                batch, padding=True, truncation=True,\n",
    "                max_length=max_len, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            outputs = model(**encodings)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Compute embeddings for training set\n",
    "train_embeddings = {}\n",
    "for name, model_name in transformer_models.items():\n",
    "    train_embeddings[name] = get_embeddings(X_train, model_name)\n",
    "\n",
    "X_train_emb = np.hstack(list(train_embeddings.values()))\n",
    "\n",
    "# Compute embeddings for test set\n",
    "test_embeddings = {}\n",
    "for name, model_name in transformer_models.items():\n",
    "    test_embeddings[name] = get_embeddings(X_test, model_name)\n",
    "\n",
    "X_test_emb = np.hstack(list(test_embeddings.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e90c85e-40cb-4284-8b00-29413054574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Dataset ===\n",
      "Dataset File: Processed_Causality_Dataset.csv\n",
      "Total Samples: 376, Columns: 2\n",
      "Train Split: 300 | Test Split: 76\n",
      "Label Classes: [0, 1]\n",
      "\n",
      "\n",
      "========== BART Training (10-Fold CV) ==========\n",
      "Model Spec: 12 layers, 1024 hidden, 16 heads\n",
      "Best Params: {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "========== BART | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 69.62% - prec: 70.00% - rec: 69.99% - f1: 70.22%\n",
      "Epoch   2/100 - acc: 69.31% - prec: 70.18% - rec: 70.37% - f1: 70.24%\n",
      "Epoch   3/100 - acc: 69.00% - prec: 70.58% - rec: 70.37% - f1: 70.85%\n",
      "Epoch   4/100 - acc: 69.00% - prec: 70.88% - rec: 70.71% - f1: 70.78%\n",
      "Epoch   5/100 - acc: 69.86% - prec: 71.21% - rec: 71.28% - f1: 71.32%\n",
      "Epoch   6/100 - acc: 69.00% - prec: 71.51% - rec: 71.66% - f1: 71.47%\n",
      "Epoch   7/100 - acc: 69.00% - prec: 71.92% - rec: 71.83% - f1: 71.70%\n",
      "Epoch   8/100 - acc: 69.00% - prec: 71.68% - rec: 71.80% - f1: 72.28%\n",
      "Epoch   9/100 - acc: 69.00% - prec: 72.71% - rec: 72.30% - f1: 72.20%\n",
      "Epoch  10/100 - acc: 69.00% - prec: 72.13% - rec: 72.34% - f1: 72.17%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 69.00% | Precision: 72.13% | Recall: 72.34% | F1: 72.17%\n",
      "\n",
      "========== BART | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 69.00% - prec: 72.69% - rec: 72.63% - f1: 73.07%\n",
      "Epoch  12/100 - acc: 69.02% - prec: 72.93% - rec: 73.29% - f1: 73.04%\n",
      "Epoch  13/100 - acc: 69.49% - prec: 73.07% - rec: 72.99% - f1: 73.64%\n",
      "Epoch  14/100 - acc: 69.74% - prec: 73.67% - rec: 73.42% - f1: 73.92%\n",
      "Epoch  15/100 - acc: 69.01% - prec: 74.07% - rec: 73.85% - f1: 74.22%\n",
      "Epoch  16/100 - acc: 69.28% - prec: 74.17% - rec: 73.99% - f1: 74.42%\n",
      "Epoch  17/100 - acc: 69.67% - prec: 74.58% - rec: 74.35% - f1: 74.16%\n",
      "Epoch  18/100 - acc: 69.70% - prec: 74.56% - rec: 74.70% - f1: 74.95%\n",
      "Epoch  19/100 - acc: 69.67% - prec: 74.93% - rec: 74.99% - f1: 75.18%\n",
      "Epoch  20/100 - acc: 69.42% - prec: 75.31% - rec: 75.36% - f1: 75.24%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 69.42% | Precision: 75.31% | Recall: 75.36% | F1: 75.24%\n",
      "\n",
      "========== BART | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 69.47% - prec: 75.58% - rec: 75.88% - f1: 75.82%\n",
      "Epoch  22/100 - acc: 69.96% - prec: 76.02% - rec: 75.95% - f1: 75.91%\n",
      "Epoch  23/100 - acc: 69.88% - prec: 76.10% - rec: 76.02% - f1: 76.20%\n",
      "Epoch  24/100 - acc: 70.33% - prec: 76.14% - rec: 76.22% - f1: 76.60%\n",
      "Epoch  25/100 - acc: 70.22% - prec: 76.56% - rec: 76.65% - f1: 77.02%\n",
      "Epoch  26/100 - acc: 70.59% - prec: 76.77% - rec: 77.00% - f1: 76.73%\n",
      "Epoch  27/100 - acc: 70.63% - prec: 77.55% - rec: 77.47% - f1: 77.17%\n",
      "Epoch  28/100 - acc: 70.90% - prec: 77.38% - rec: 77.40% - f1: 77.81%\n",
      "Epoch  29/100 - acc: 71.17% - prec: 77.94% - rec: 77.27% - f1: 77.82%\n",
      "Epoch  30/100 - acc: 71.01% - prec: 78.17% - rec: 78.39% - f1: 78.44%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 71.01% | Precision: 78.17% | Recall: 78.39% | F1: 78.44%\n",
      "\n",
      "========== BART | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 71.29% - prec: 78.07% - rec: 77.68% - f1: 78.74%\n",
      "Epoch  32/100 - acc: 72.28% - prec: 78.66% - rec: 78.64% - f1: 79.05%\n",
      "Epoch  33/100 - acc: 72.08% - prec: 78.85% - rec: 78.58% - f1: 79.28%\n",
      "Epoch  34/100 - acc: 72.31% - prec: 78.99% - rec: 78.85% - f1: 79.43%\n",
      "Epoch  35/100 - acc: 73.18% - prec: 79.35% - rec: 79.61% - f1: 79.69%\n",
      "Epoch  36/100 - acc: 73.32% - prec: 79.53% - rec: 79.50% - f1: 80.05%\n",
      "Epoch  37/100 - acc: 74.16% - prec: 79.89% - rec: 80.05% - f1: 80.27%\n",
      "Epoch  38/100 - acc: 73.82% - prec: 80.49% - rec: 80.34% - f1: 80.47%\n",
      "Epoch  39/100 - acc: 75.09% - prec: 80.53% - rec: 80.27% - f1: 80.82%\n",
      "Epoch  40/100 - acc: 75.38% - prec: 80.53% - rec: 80.92% - f1: 81.30%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 75.38% | Precision: 80.53% | Recall: 80.92% | F1: 81.30%\n",
      "\n",
      "========== BART | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 75.68% - prec: 81.04% - rec: 80.74% - f1: 81.49%\n",
      "Epoch  42/100 - acc: 77.22% - prec: 81.42% - rec: 81.12% - f1: 81.70%\n",
      "Epoch  43/100 - acc: 77.57% - prec: 81.54% - rec: 81.79% - f1: 81.76%\n",
      "Epoch  44/100 - acc: 78.13% - prec: 81.68% - rec: 81.55% - f1: 82.44%\n",
      "Epoch  45/100 - acc: 78.85% - prec: 82.22% - rec: 82.11% - f1: 82.51%\n",
      "Epoch  46/100 - acc: 79.35% - prec: 82.41% - rec: 82.21% - f1: 83.01%\n",
      "Epoch  47/100 - acc: 80.89% - prec: 82.89% - rec: 82.32% - f1: 83.23%\n",
      "Epoch  48/100 - acc: 81.42% - prec: 82.86% - rec: 82.53% - f1: 83.53%\n",
      "Epoch  49/100 - acc: 82.27% - prec: 83.42% - rec: 82.88% - f1: 83.75%\n",
      "Epoch  50/100 - acc: 83.28% - prec: 83.51% - rec: 83.54% - f1: 83.88%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 83.28% | Precision: 83.51% | Recall: 83.54% | F1: 83.88%\n",
      "\n",
      "========== BART | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 83.44% - prec: 83.97% - rec: 83.88% - f1: 84.29%\n",
      "Epoch  52/100 - acc: 84.86% - prec: 84.07% - rec: 83.85% - f1: 84.44%\n",
      "Epoch  53/100 - acc: 85.25% - prec: 84.58% - rec: 84.26% - f1: 85.18%\n",
      "Epoch  54/100 - acc: 87.20% - prec: 84.85% - rec: 84.52% - f1: 85.35%\n",
      "Epoch  55/100 - acc: 87.26% - prec: 84.81% - rec: 85.04% - f1: 85.49%\n",
      "Epoch  56/100 - acc: 88.35% - prec: 85.17% - rec: 85.32% - f1: 85.97%\n",
      "Epoch  57/100 - acc: 89.21% - prec: 85.82% - rec: 85.07% - f1: 86.27%\n",
      "Epoch  58/100 - acc: 90.23% - prec: 85.66% - rec: 85.47% - f1: 86.28%\n",
      "Epoch  59/100 - acc: 90.72% - prec: 86.27% - rec: 85.93% - f1: 86.42%\n",
      "Epoch  60/100 - acc: 90.80% - prec: 85.84% - rec: 86.13% - f1: 86.76%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 90.80% | Precision: 85.84% | Recall: 86.13% | F1: 86.76%\n",
      "\n",
      "========== BART | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 91.88% - prec: 86.72% - rec: 86.72% - f1: 87.36%\n",
      "Epoch  62/100 - acc: 92.24% - prec: 86.88% - rec: 86.55% - f1: 87.30%\n",
      "Epoch  63/100 - acc: 93.16% - prec: 87.28% - rec: 86.81% - f1: 88.05%\n",
      "Epoch  64/100 - acc: 93.57% - prec: 87.33% - rec: 87.17% - f1: 87.95%\n",
      "Epoch  65/100 - acc: 94.02% - prec: 88.15% - rec: 87.75% - f1: 88.24%\n",
      "Epoch  66/100 - acc: 94.04% - prec: 88.09% - rec: 87.57% - f1: 88.73%\n",
      "Epoch  67/100 - acc: 94.10% - prec: 88.25% - rec: 87.98% - f1: 88.94%\n",
      "Epoch  68/100 - acc: 94.92% - prec: 88.73% - rec: 88.30% - f1: 89.21%\n",
      "Epoch  69/100 - acc: 95.30% - prec: 89.11% - rec: 88.24% - f1: 89.57%\n",
      "Epoch  70/100 - acc: 95.68% - prec: 89.21% - rec: 88.97% - f1: 90.20%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 95.68% | Precision: 89.21% | Recall: 88.97% | F1: 90.20%\n",
      "\n",
      "========== BART | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 95.61% - prec: 89.61% - rec: 89.26% - f1: 90.43%\n",
      "Epoch  72/100 - acc: 96.00% - prec: 89.71% - rec: 89.41% - f1: 90.36%\n",
      "Epoch  73/100 - acc: 96.01% - prec: 89.66% - rec: 89.57% - f1: 90.55%\n",
      "Epoch  74/100 - acc: 96.82% - prec: 90.28% - rec: 89.68% - f1: 91.00%\n",
      "Epoch  75/100 - acc: 97.04% - prec: 90.47% - rec: 90.37% - f1: 91.41%\n",
      "Epoch  76/100 - acc: 97.11% - prec: 90.82% - rec: 90.03% - f1: 91.61%\n",
      "Epoch  77/100 - acc: 96.63% - prec: 91.37% - rec: 90.86% - f1: 91.74%\n",
      "Epoch  78/100 - acc: 96.99% - prec: 91.28% - rec: 90.93% - f1: 91.91%\n",
      "Epoch  79/100 - acc: 97.30% - prec: 91.45% - rec: 91.19% - f1: 92.51%\n",
      "Epoch  80/100 - acc: 96.47% - prec: 92.02% - rec: 91.36% - f1: 92.61%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 96.47% | Precision: 92.02% | Recall: 91.36% | F1: 92.61%\n",
      "\n",
      "========== BART | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 97.41% - prec: 92.68% - rec: 91.67% - f1: 92.70%\n",
      "Epoch  82/100 - acc: 97.15% - prec: 92.30% - rec: 92.05% - f1: 93.40%\n",
      "Epoch  83/100 - acc: 97.72% - prec: 92.58% - rec: 92.28% - f1: 93.45%\n",
      "Epoch  84/100 - acc: 97.50% - prec: 93.28% - rec: 92.60% - f1: 93.76%\n",
      "Epoch  85/100 - acc: 97.98% - prec: 93.57% - rec: 92.81% - f1: 93.93%\n",
      "Epoch  86/100 - acc: 98.16% - prec: 93.79% - rec: 93.21% - f1: 94.22%\n",
      "Epoch  87/100 - acc: 97.53% - prec: 93.78% - rec: 93.42% - f1: 94.65%\n",
      "Epoch  88/100 - acc: 97.82% - prec: 94.12% - rec: 93.62% - f1: 94.86%\n",
      "Epoch  89/100 - acc: 98.16% - prec: 94.25% - rec: 94.09% - f1: 94.89%\n",
      "Epoch  90/100 - acc: 97.50% - prec: 94.84% - rec: 94.16% - f1: 95.60%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 97.50% | Precision: 94.84% | Recall: 94.16% | F1: 95.60%\n",
      "\n",
      "========== BART | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 97.84% - prec: 94.95% - rec: 94.36% - f1: 95.39%\n",
      "Epoch  92/100 - acc: 97.86% - prec: 95.37% - rec: 94.63% - f1: 96.02%\n",
      "Epoch  93/100 - acc: 97.93% - prec: 95.33% - rec: 94.80% - f1: 96.56%\n",
      "Epoch  94/100 - acc: 98.10% - prec: 95.66% - rec: 95.24% - f1: 96.47%\n",
      "Epoch  95/100 - acc: 98.08% - prec: 95.91% - rec: 95.59% - f1: 96.72%\n",
      "Epoch  96/100 - acc: 98.01% - prec: 96.62% - rec: 96.22% - f1: 96.95%\n",
      "Epoch  97/100 - acc: 97.77% - prec: 96.47% - rec: 96.33% - f1: 97.13%\n",
      "Epoch  98/100 - acc: 98.16% - prec: 97.22% - rec: 96.87% - f1: 97.79%\n",
      "Epoch  99/100 - acc: 97.66% - prec: 97.30% - rec: 97.32% - f1: 97.93%\n",
      "Epoch 100/100 - acc: 98.16% - prec: 97.58% - rec: 97.47% - f1: 98.32%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 98.16% | Precision: 97.58% | Recall: 97.47% | F1: 98.32%\n",
      "\n",
      ">>> BART Final CV Results (10 folds)\n",
      "Accuracy: 98.16\n",
      "Precision: 97.50\n",
      "Recall: 97.05\n",
      "F1: 98.32\n",
      "============================================================\n",
      "\n",
      "========== DeBERTa Training (10-Fold CV) ==========\n",
      "Model Spec: 12 layers, 768 hidden, 12 heads\n",
      "Best Params: {'C': 0.5, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "========== DeBERTa | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 66.25% - prec: 69.66% - rec: 69.56% - f1: 70.10%\n",
      "Epoch   2/100 - acc: 66.35% - prec: 70.39% - rec: 70.39% - f1: 70.27%\n",
      "Epoch   3/100 - acc: 66.50% - prec: 70.69% - rec: 70.43% - f1: 70.57%\n",
      "Epoch   4/100 - acc: 66.39% - prec: 70.41% - rec: 70.71% - f1: 70.75%\n",
      "Epoch   5/100 - acc: 66.15% - prec: 71.15% - rec: 71.25% - f1: 70.91%\n",
      "Epoch   6/100 - acc: 66.35% - prec: 71.55% - rec: 71.15% - f1: 71.15%\n",
      "Epoch   7/100 - acc: 66.04% - prec: 71.70% - rec: 71.58% - f1: 71.50%\n",
      "Epoch   8/100 - acc: 66.00% - prec: 72.18% - rec: 71.89% - f1: 72.28%\n",
      "Epoch   9/100 - acc: 66.00% - prec: 72.13% - rec: 72.58% - f1: 72.56%\n",
      "Epoch  10/100 - acc: 66.00% - prec: 72.28% - rec: 72.34% - f1: 72.25%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 66.00% | Precision: 72.28% | Recall: 72.34% | F1: 72.25%\n",
      "\n",
      "========== DeBERTa | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 66.07% - prec: 72.54% - rec: 72.76% - f1: 72.57%\n",
      "Epoch  12/100 - acc: 66.48% - prec: 72.93% - rec: 72.97% - f1: 72.87%\n",
      "Epoch  13/100 - acc: 66.43% - prec: 72.98% - rec: 73.18% - f1: 72.98%\n",
      "Epoch  14/100 - acc: 66.00% - prec: 73.81% - rec: 73.90% - f1: 73.20%\n",
      "Epoch  15/100 - acc: 66.56% - prec: 73.25% - rec: 73.62% - f1: 74.08%\n",
      "Epoch  16/100 - acc: 66.54% - prec: 73.94% - rec: 74.05% - f1: 74.39%\n",
      "Epoch  17/100 - acc: 66.38% - prec: 74.23% - rec: 74.36% - f1: 74.58%\n",
      "Epoch  18/100 - acc: 66.68% - prec: 74.46% - rec: 74.36% - f1: 74.61%\n",
      "Epoch  19/100 - acc: 66.57% - prec: 74.50% - rec: 74.62% - f1: 75.34%\n",
      "Epoch  20/100 - acc: 66.25% - prec: 75.32% - rec: 75.11% - f1: 75.19%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 66.25% | Precision: 75.32% | Recall: 75.11% | F1: 75.19%\n",
      "\n",
      "========== DeBERTa | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 67.00% - prec: 75.69% - rec: 75.14% - f1: 75.31%\n",
      "Epoch  22/100 - acc: 66.48% - prec: 75.63% - rec: 75.76% - f1: 75.92%\n",
      "Epoch  23/100 - acc: 67.00% - prec: 75.74% - rec: 75.84% - f1: 75.69%\n",
      "Epoch  24/100 - acc: 67.39% - prec: 75.84% - rec: 76.12% - f1: 76.46%\n",
      "Epoch  25/100 - acc: 67.52% - prec: 76.15% - rec: 76.15% - f1: 76.51%\n",
      "Epoch  26/100 - acc: 67.73% - prec: 77.17% - rec: 76.62% - f1: 77.42%\n",
      "Epoch  27/100 - acc: 67.46% - prec: 76.55% - rec: 77.02% - f1: 76.74%\n",
      "Epoch  28/100 - acc: 67.56% - prec: 77.30% - rec: 77.17% - f1: 77.41%\n",
      "Epoch  29/100 - acc: 68.05% - prec: 77.90% - rec: 77.39% - f1: 77.53%\n",
      "Epoch  30/100 - acc: 68.48% - prec: 77.74% - rec: 77.48% - f1: 78.18%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 68.48% | Precision: 77.74% | Recall: 77.48% | F1: 78.18%\n",
      "\n",
      "========== DeBERTa | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 67.93% - prec: 78.31% - rec: 78.13% - f1: 78.05%\n",
      "Epoch  32/100 - acc: 68.94% - prec: 78.47% - rec: 78.09% - f1: 78.23%\n",
      "Epoch  33/100 - acc: 69.12% - prec: 78.25% - rec: 78.39% - f1: 78.73%\n",
      "Epoch  34/100 - acc: 69.65% - prec: 78.68% - rec: 78.84% - f1: 79.17%\n",
      "Epoch  35/100 - acc: 69.51% - prec: 78.81% - rec: 78.73% - f1: 79.32%\n",
      "Epoch  36/100 - acc: 70.56% - prec: 79.15% - rec: 79.56% - f1: 79.50%\n",
      "Epoch  37/100 - acc: 71.00% - prec: 79.66% - rec: 79.36% - f1: 80.00%\n",
      "Epoch  38/100 - acc: 71.69% - prec: 80.44% - rec: 80.24% - f1: 80.21%\n",
      "Epoch  39/100 - acc: 71.90% - prec: 80.02% - rec: 80.09% - f1: 80.48%\n",
      "Epoch  40/100 - acc: 72.09% - prec: 80.15% - rec: 80.31% - f1: 80.80%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 72.09% | Precision: 80.15% | Recall: 80.31% | F1: 80.80%\n",
      "\n",
      "========== DeBERTa | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 73.10% - prec: 80.57% - rec: 80.75% - f1: 81.27%\n",
      "Epoch  42/100 - acc: 73.87% - prec: 80.93% - rec: 81.03% - f1: 81.49%\n",
      "Epoch  43/100 - acc: 74.59% - prec: 81.31% - rec: 80.94% - f1: 81.44%\n",
      "Epoch  44/100 - acc: 75.12% - prec: 81.24% - rec: 81.03% - f1: 82.00%\n",
      "Epoch  45/100 - acc: 75.97% - prec: 81.69% - rec: 81.95% - f1: 82.07%\n",
      "Epoch  46/100 - acc: 76.94% - prec: 81.97% - rec: 81.88% - f1: 82.44%\n",
      "Epoch  47/100 - acc: 77.60% - prec: 82.38% - rec: 82.32% - f1: 82.70%\n",
      "Epoch  48/100 - acc: 78.61% - prec: 82.53% - rec: 82.72% - f1: 82.96%\n",
      "Epoch  49/100 - acc: 79.25% - prec: 82.86% - rec: 82.57% - f1: 83.05%\n",
      "Epoch  50/100 - acc: 80.65% - prec: 82.92% - rec: 83.28% - f1: 83.67%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 80.65% | Precision: 82.92% | Recall: 83.28% | F1: 83.67%\n",
      "\n",
      "========== DeBERTa | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 81.25% - prec: 83.34% - rec: 83.36% - f1: 83.64%\n",
      "Epoch  52/100 - acc: 82.18% - prec: 83.50% - rec: 83.55% - f1: 83.69%\n",
      "Epoch  53/100 - acc: 82.56% - prec: 83.67% - rec: 84.01% - f1: 84.04%\n",
      "Epoch  54/100 - acc: 83.74% - prec: 84.41% - rec: 84.25% - f1: 84.70%\n",
      "Epoch  55/100 - acc: 84.84% - prec: 84.52% - rec: 84.08% - f1: 85.04%\n",
      "Epoch  56/100 - acc: 85.35% - prec: 84.90% - rec: 84.90% - f1: 85.36%\n",
      "Epoch  57/100 - acc: 86.55% - prec: 84.87% - rec: 84.82% - f1: 85.37%\n",
      "Epoch  58/100 - acc: 87.48% - prec: 85.16% - rec: 85.24% - f1: 85.66%\n",
      "Epoch  59/100 - acc: 87.82% - prec: 85.61% - rec: 85.26% - f1: 86.14%\n",
      "Epoch  60/100 - acc: 88.26% - prec: 85.75% - rec: 85.20% - f1: 86.39%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 88.26% | Precision: 85.75% | Recall: 85.20% | F1: 86.39%\n",
      "\n",
      "========== DeBERTa | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 89.36% - prec: 86.14% - rec: 86.01% - f1: 86.40%\n",
      "Epoch  62/100 - acc: 89.72% - prec: 86.32% - rec: 86.15% - f1: 86.72%\n",
      "Epoch  63/100 - acc: 90.63% - prec: 86.59% - rec: 86.56% - f1: 87.36%\n",
      "Epoch  64/100 - acc: 90.37% - prec: 86.85% - rec: 86.79% - f1: 87.39%\n",
      "Epoch  65/100 - acc: 90.99% - prec: 87.15% - rec: 86.95% - f1: 87.47%\n",
      "Epoch  66/100 - acc: 91.46% - prec: 87.01% - rec: 87.25% - f1: 88.17%\n",
      "Epoch  67/100 - acc: 91.88% - prec: 87.35% - rec: 87.60% - f1: 88.42%\n",
      "Epoch  68/100 - acc: 92.25% - prec: 87.66% - rec: 87.65% - f1: 88.44%\n",
      "Epoch  69/100 - acc: 92.40% - prec: 88.30% - rec: 87.87% - f1: 88.85%\n",
      "Epoch  70/100 - acc: 92.57% - prec: 88.38% - rec: 88.41% - f1: 89.12%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 92.57% | Precision: 88.38% | Recall: 88.41% | F1: 89.12%\n",
      "\n",
      "========== DeBERTa | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 93.10% - prec: 88.60% - rec: 88.74% - f1: 89.24%\n",
      "Epoch  72/100 - acc: 93.72% - prec: 89.16% - rec: 88.92% - f1: 89.41%\n",
      "Epoch  73/100 - acc: 93.78% - prec: 89.19% - rec: 89.16% - f1: 90.10%\n",
      "Epoch  74/100 - acc: 93.99% - prec: 89.47% - rec: 89.43% - f1: 89.75%\n",
      "Epoch  75/100 - acc: 93.91% - prec: 89.80% - rec: 89.66% - f1: 90.35%\n",
      "Epoch  76/100 - acc: 94.19% - prec: 89.89% - rec: 89.99% - f1: 90.92%\n",
      "Epoch  77/100 - acc: 94.62% - prec: 90.18% - rec: 90.55% - f1: 90.90%\n",
      "Epoch  78/100 - acc: 95.02% - prec: 90.37% - rec: 90.42% - f1: 91.05%\n",
      "Epoch  79/100 - acc: 94.23% - prec: 90.55% - rec: 90.82% - f1: 91.73%\n",
      "Epoch  80/100 - acc: 94.25% - prec: 91.09% - rec: 90.76% - f1: 91.78%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 94.25% | Precision: 91.09% | Recall: 90.76% | F1: 91.78%\n",
      "\n",
      "========== DeBERTa | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 94.72% - prec: 91.55% - rec: 91.23% - f1: 92.20%\n",
      "Epoch  82/100 - acc: 94.50% - prec: 91.58% - rec: 91.44% - f1: 92.76%\n",
      "Epoch  83/100 - acc: 94.85% - prec: 91.75% - rec: 91.70% - f1: 92.65%\n",
      "Epoch  84/100 - acc: 94.46% - prec: 92.10% - rec: 92.38% - f1: 92.92%\n",
      "Epoch  85/100 - acc: 95.19% - prec: 92.34% - rec: 92.53% - f1: 93.37%\n",
      "Epoch  86/100 - acc: 95.05% - prec: 92.61% - rec: 92.60% - f1: 93.49%\n",
      "Epoch  87/100 - acc: 95.46% - prec: 92.85% - rec: 92.74% - f1: 93.62%\n",
      "Epoch  88/100 - acc: 94.98% - prec: 93.13% - rec: 93.22% - f1: 93.95%\n",
      "Epoch  89/100 - acc: 95.32% - prec: 93.86% - rec: 93.29% - f1: 94.45%\n",
      "Epoch  90/100 - acc: 94.98% - prec: 93.65% - rec: 94.01% - f1: 94.55%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 94.98% | Precision: 93.65% | Recall: 94.01% | F1: 94.55%\n",
      "\n",
      "========== DeBERTa | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 94.68% - prec: 93.82% - rec: 94.02% - f1: 95.12%\n",
      "Epoch  92/100 - acc: 95.25% - prec: 94.29% - rec: 94.26% - f1: 95.41%\n",
      "Epoch  93/100 - acc: 95.40% - prec: 94.59% - rec: 94.57% - f1: 95.49%\n",
      "Epoch  94/100 - acc: 95.19% - prec: 94.65% - rec: 94.73% - f1: 95.59%\n",
      "Epoch  95/100 - acc: 95.53% - prec: 94.83% - rec: 94.92% - f1: 95.87%\n",
      "Epoch  96/100 - acc: 95.53% - prec: 95.39% - rec: 95.47% - f1: 96.44%\n",
      "Epoch  97/100 - acc: 95.53% - prec: 95.58% - rec: 95.64% - f1: 96.52%\n",
      "Epoch  98/100 - acc: 94.98% - prec: 96.06% - rec: 95.76% - f1: 96.90%\n",
      "Epoch  99/100 - acc: 94.94% - prec: 96.09% - rec: 95.76% - f1: 97.02%\n",
      "Epoch 100/100 - acc: 95.18% - prec: 96.32% - rec: 96.53% - f1: 97.36%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 95.18% | Precision: 96.32% | Recall: 96.53% | F1: 97.36%\n",
      "\n",
      ">>> DeBERTa Final CV Results (10 folds)\n",
      "Accuracy: 95.53\n",
      "Precision: 96.36\n",
      "Recall: 96.37\n",
      "F1: 97.36\n",
      "============================================================\n",
      "\n",
      "========== DistilBERT Training (10-Fold CV) ==========\n",
      "Model Spec: 6 layers, 768 hidden, 12 heads\n",
      "Best Params: {'C': 1.0, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "========== DistilBERT | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 63.32% - prec: 69.85% - rec: 70.22% - f1: 70.06%\n",
      "Epoch   2/100 - acc: 63.00% - prec: 70.35% - rec: 69.85% - f1: 70.13%\n",
      "Epoch   3/100 - acc: 63.00% - prec: 70.34% - rec: 70.46% - f1: 70.48%\n",
      "Epoch   4/100 - acc: 63.00% - prec: 70.58% - rec: 70.62% - f1: 70.82%\n",
      "Epoch   5/100 - acc: 63.67% - prec: 71.11% - rec: 71.06% - f1: 70.97%\n",
      "Epoch   6/100 - acc: 63.71% - prec: 71.11% - rec: 71.01% - f1: 71.62%\n",
      "Epoch   7/100 - acc: 63.45% - prec: 71.62% - rec: 71.06% - f1: 71.42%\n",
      "Epoch   8/100 - acc: 63.22% - prec: 71.71% - rec: 71.77% - f1: 71.74%\n",
      "Epoch   9/100 - acc: 63.00% - prec: 72.30% - rec: 71.70% - f1: 72.24%\n",
      "Epoch  10/100 - acc: 63.56% - prec: 72.05% - rec: 72.32% - f1: 72.55%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 63.56% | Precision: 72.05% | Recall: 72.32% | F1: 72.55%\n",
      "\n",
      "========== DistilBERT | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 63.50% - prec: 72.50% - rec: 72.21% - f1: 72.71%\n",
      "Epoch  12/100 - acc: 63.00% - prec: 72.93% - rec: 72.89% - f1: 72.96%\n",
      "Epoch  13/100 - acc: 63.53% - prec: 72.96% - rec: 72.96% - f1: 73.13%\n",
      "Epoch  14/100 - acc: 63.71% - prec: 73.55% - rec: 73.21% - f1: 73.38%\n",
      "Epoch  15/100 - acc: 63.76% - prec: 73.43% - rec: 73.04% - f1: 73.64%\n",
      "Epoch  16/100 - acc: 63.84% - prec: 73.73% - rec: 73.59% - f1: 74.09%\n",
      "Epoch  17/100 - acc: 63.56% - prec: 74.02% - rec: 73.38% - f1: 73.88%\n",
      "Epoch  18/100 - acc: 63.65% - prec: 74.37% - rec: 74.10% - f1: 74.31%\n",
      "Epoch  19/100 - acc: 63.48% - prec: 74.81% - rec: 73.94% - f1: 74.59%\n",
      "Epoch  20/100 - acc: 63.53% - prec: 75.13% - rec: 74.71% - f1: 75.16%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 63.53% | Precision: 75.13% | Recall: 74.71% | F1: 75.16%\n",
      "\n",
      "========== DistilBERT | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 64.47% - prec: 75.23% - rec: 74.47% - f1: 75.36%\n",
      "Epoch  22/100 - acc: 64.39% - prec: 75.62% - rec: 75.09% - f1: 75.56%\n",
      "Epoch  23/100 - acc: 63.96% - prec: 75.91% - rec: 75.22% - f1: 75.59%\n",
      "Epoch  24/100 - acc: 64.00% - prec: 76.01% - rec: 75.22% - f1: 75.87%\n",
      "Epoch  25/100 - acc: 64.48% - prec: 76.32% - rec: 75.85% - f1: 76.53%\n",
      "Epoch  26/100 - acc: 64.46% - prec: 76.43% - rec: 76.00% - f1: 76.85%\n",
      "Epoch  27/100 - acc: 64.50% - prec: 76.61% - rec: 76.31% - f1: 76.60%\n",
      "Epoch  28/100 - acc: 65.04% - prec: 76.87% - rec: 76.53% - f1: 77.19%\n",
      "Epoch  29/100 - acc: 65.34% - prec: 77.50% - rec: 76.55% - f1: 77.19%\n",
      "Epoch  30/100 - acc: 65.12% - prec: 77.41% - rec: 76.92% - f1: 77.57%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 65.12% | Precision: 77.41% | Recall: 76.92% | F1: 77.57%\n",
      "\n",
      "========== DistilBERT | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 65.93% - prec: 77.88% - rec: 76.94% - f1: 78.03%\n",
      "Epoch  32/100 - acc: 66.56% - prec: 77.90% - rec: 77.45% - f1: 78.22%\n",
      "Epoch  33/100 - acc: 65.98% - prec: 78.58% - rec: 77.58% - f1: 78.63%\n",
      "Epoch  34/100 - acc: 66.87% - prec: 78.89% - rec: 78.00% - f1: 78.59%\n",
      "Epoch  35/100 - acc: 67.52% - prec: 78.90% - rec: 78.14% - f1: 78.83%\n",
      "Epoch  36/100 - acc: 67.25% - prec: 78.80% - rec: 77.97% - f1: 78.89%\n",
      "Epoch  37/100 - acc: 68.50% - prec: 79.29% - rec: 78.77% - f1: 79.05%\n",
      "Epoch  38/100 - acc: 68.69% - prec: 80.14% - rec: 78.63% - f1: 79.94%\n",
      "Epoch  39/100 - acc: 69.40% - prec: 79.88% - rec: 79.09% - f1: 80.00%\n",
      "Epoch  40/100 - acc: 69.94% - prec: 80.39% - rec: 79.54% - f1: 80.39%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 69.94% | Precision: 80.39% | Recall: 79.54% | F1: 80.39%\n",
      "\n",
      "========== DistilBERT | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 70.56% - prec: 80.55% - rec: 79.67% - f1: 80.18%\n",
      "Epoch  42/100 - acc: 70.66% - prec: 80.84% - rec: 79.83% - f1: 80.73%\n",
      "Epoch  43/100 - acc: 72.15% - prec: 80.85% - rec: 80.00% - f1: 80.64%\n",
      "Epoch  44/100 - acc: 72.36% - prec: 80.93% - rec: 80.18% - f1: 80.92%\n",
      "Epoch  45/100 - acc: 73.42% - prec: 81.55% - rec: 80.07% - f1: 81.26%\n",
      "Epoch  46/100 - acc: 74.52% - prec: 81.64% - rec: 80.92% - f1: 81.70%\n",
      "Epoch  47/100 - acc: 75.02% - prec: 82.23% - rec: 80.66% - f1: 81.71%\n",
      "Epoch  48/100 - acc: 76.22% - prec: 82.02% - rec: 81.35% - f1: 82.14%\n",
      "Epoch  49/100 - acc: 76.51% - prec: 83.00% - rec: 81.43% - f1: 82.52%\n",
      "Epoch  50/100 - acc: 77.75% - prec: 82.51% - rec: 81.80% - f1: 82.60%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 77.75% | Precision: 82.51% | Recall: 81.80% | F1: 82.60%\n",
      "\n",
      "========== DistilBERT | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 79.33% - prec: 82.95% - rec: 81.82% - f1: 83.03%\n",
      "Epoch  52/100 - acc: 79.88% - prec: 83.23% - rec: 82.17% - f1: 83.22%\n",
      "Epoch  53/100 - acc: 81.38% - prec: 83.56% - rec: 82.22% - f1: 83.38%\n",
      "Epoch  54/100 - acc: 81.75% - prec: 83.95% - rec: 82.40% - f1: 83.89%\n",
      "Epoch  55/100 - acc: 82.59% - prec: 84.27% - rec: 83.10% - f1: 83.99%\n",
      "Epoch  56/100 - acc: 83.33% - prec: 84.72% - rec: 82.90% - f1: 84.26%\n",
      "Epoch  57/100 - acc: 85.07% - prec: 85.22% - rec: 83.23% - f1: 84.56%\n",
      "Epoch  58/100 - acc: 85.18% - prec: 84.92% - rec: 83.47% - f1: 85.26%\n",
      "Epoch  59/100 - acc: 85.91% - prec: 85.59% - rec: 83.87% - f1: 85.17%\n",
      "Epoch  60/100 - acc: 87.22% - prec: 85.40% - rec: 84.21% - f1: 85.49%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 87.22% | Precision: 85.40% | Recall: 84.21% | F1: 85.49%\n",
      "\n",
      "========== DistilBERT | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 86.96% - prec: 85.50% - rec: 84.56% - f1: 85.45%\n",
      "Epoch  62/100 - acc: 87.97% - prec: 86.29% - rec: 84.56% - f1: 86.04%\n",
      "Epoch  63/100 - acc: 88.77% - prec: 86.09% - rec: 84.78% - f1: 85.99%\n",
      "Epoch  64/100 - acc: 89.60% - prec: 86.59% - rec: 85.04% - f1: 86.15%\n",
      "Epoch  65/100 - acc: 89.55% - prec: 86.68% - rec: 85.26% - f1: 86.48%\n",
      "Epoch  66/100 - acc: 90.20% - prec: 87.30% - rec: 85.58% - f1: 86.53%\n",
      "Epoch  67/100 - acc: 89.91% - prec: 87.41% - rec: 85.50% - f1: 87.08%\n",
      "Epoch  68/100 - acc: 91.07% - prec: 87.58% - rec: 85.97% - f1: 87.26%\n",
      "Epoch  69/100 - acc: 91.11% - prec: 87.65% - rec: 86.25% - f1: 87.92%\n",
      "Epoch  70/100 - acc: 90.91% - prec: 88.17% - rec: 86.64% - f1: 87.72%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 90.91% | Precision: 88.17% | Recall: 86.64% | F1: 87.72%\n",
      "\n",
      "========== DistilBERT | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 92.22% - prec: 88.52% - rec: 86.84% - f1: 87.87%\n",
      "Epoch  72/100 - acc: 91.85% - prec: 88.67% - rec: 86.76% - f1: 88.03%\n",
      "Epoch  73/100 - acc: 92.05% - prec: 89.06% - rec: 87.29% - f1: 88.35%\n",
      "Epoch  74/100 - acc: 92.86% - prec: 89.01% - rec: 87.64% - f1: 89.46%\n",
      "Epoch  75/100 - acc: 92.41% - prec: 89.06% - rec: 87.88% - f1: 89.09%\n",
      "Epoch  76/100 - acc: 92.98% - prec: 89.90% - rec: 88.04% - f1: 89.79%\n",
      "Epoch  77/100 - acc: 92.95% - prec: 89.93% - rec: 88.47% - f1: 89.64%\n",
      "Epoch  78/100 - acc: 93.72% - prec: 89.89% - rec: 88.12% - f1: 90.07%\n",
      "Epoch  79/100 - acc: 93.88% - prec: 90.39% - rec: 88.95% - f1: 90.34%\n",
      "Epoch  80/100 - acc: 93.22% - prec: 90.42% - rec: 88.87% - f1: 90.50%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 93.22% | Precision: 90.42% | Recall: 88.87% | F1: 90.50%\n",
      "\n",
      "========== DistilBERT | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 92.42% - prec: 90.86% - rec: 89.32% - f1: 90.77%\n",
      "Epoch  82/100 - acc: 94.05% - prec: 90.82% - rec: 89.34% - f1: 90.86%\n",
      "Epoch  83/100 - acc: 93.27% - prec: 91.51% - rec: 89.58% - f1: 91.38%\n",
      "Epoch  84/100 - acc: 93.82% - prec: 91.75% - rec: 89.91% - f1: 91.54%\n",
      "Epoch  85/100 - acc: 93.74% - prec: 91.62% - rec: 90.25% - f1: 91.58%\n",
      "Epoch  86/100 - acc: 93.81% - prec: 92.25% - rec: 90.76% - f1: 91.91%\n",
      "Epoch  87/100 - acc: 93.47% - prec: 92.17% - rec: 90.74% - f1: 92.46%\n",
      "Epoch  88/100 - acc: 93.62% - prec: 93.11% - rec: 90.55% - f1: 92.56%\n",
      "Epoch  89/100 - acc: 93.52% - prec: 92.73% - rec: 91.05% - f1: 92.87%\n",
      "Epoch  90/100 - acc: 93.60% - prec: 93.32% - rec: 91.54% - f1: 93.25%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 93.60% | Precision: 93.32% | Recall: 91.54% | F1: 93.25%\n",
      "\n",
      "========== DistilBERT | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 93.78% - prec: 93.77% - rec: 91.68% - f1: 93.38%\n",
      "Epoch  92/100 - acc: 93.94% - prec: 93.92% - rec: 91.74% - f1: 93.57%\n",
      "Epoch  93/100 - acc: 94.18% - prec: 94.00% - rec: 91.81% - f1: 93.85%\n",
      "Epoch  94/100 - acc: 94.05% - prec: 94.50% - rec: 92.58% - f1: 94.29%\n",
      "Epoch  95/100 - acc: 93.90% - prec: 94.00% - rec: 92.27% - f1: 94.50%\n",
      "Epoch  96/100 - acc: 94.14% - prec: 94.71% - rec: 92.46% - f1: 94.61%\n",
      "Epoch  97/100 - acc: 94.17% - prec: 94.77% - rec: 92.81% - f1: 94.66%\n",
      "Epoch  98/100 - acc: 94.21% - prec: 94.98% - rec: 93.25% - f1: 95.54%\n",
      "Epoch  99/100 - acc: 94.21% - prec: 95.18% - rec: 93.53% - f1: 95.60%\n",
      "Epoch 100/100 - acc: 94.21% - prec: 96.19% - rec: 93.71% - f1: 95.66%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 94.21% | Precision: 96.19% | Recall: 93.71% | F1: 95.66%\n",
      "\n",
      ">>> DistilBERT Final CV Results (10 folds)\n",
      "Accuracy: 94.21\n",
      "Precision: 95.80\n",
      "Recall: 93.72\n",
      "F1: 95.71\n",
      "============================================================\n",
      "\n",
      "========== RoBERTa Training (10-Fold CV) ==========\n",
      "Model Spec: 12 layers, 768 hidden, 12 heads\n",
      "Best Params: {'C': 10.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "========== RoBERTa | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 64.19% - prec: 70.06% - rec: 70.18% - f1: 69.90%\n",
      "Epoch   2/100 - acc: 64.00% - prec: 70.42% - rec: 70.30% - f1: 70.26%\n",
      "Epoch   3/100 - acc: 64.27% - prec: 70.15% - rec: 70.30% - f1: 70.09%\n",
      "Epoch   4/100 - acc: 64.42% - prec: 70.73% - rec: 70.78% - f1: 70.61%\n",
      "Epoch   5/100 - acc: 64.16% - prec: 70.85% - rec: 70.99% - f1: 71.24%\n",
      "Epoch   6/100 - acc: 64.00% - prec: 70.87% - rec: 71.67% - f1: 71.42%\n",
      "Epoch   7/100 - acc: 64.00% - prec: 70.89% - rec: 71.99% - f1: 71.46%\n",
      "Epoch   8/100 - acc: 64.00% - prec: 71.37% - rec: 71.99% - f1: 71.70%\n",
      "Epoch   9/100 - acc: 64.53% - prec: 71.52% - rec: 72.33% - f1: 71.77%\n",
      "Epoch  10/100 - acc: 64.00% - prec: 72.01% - rec: 72.75% - f1: 72.25%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 64.00% | Precision: 72.01% | Recall: 72.75% | F1: 72.25%\n",
      "\n",
      "========== RoBERTa | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 64.32% - prec: 72.56% - rec: 73.07% - f1: 72.22%\n",
      "Epoch  12/100 - acc: 64.06% - prec: 72.34% - rec: 73.04% - f1: 73.05%\n",
      "Epoch  13/100 - acc: 64.41% - prec: 72.54% - rec: 73.78% - f1: 72.99%\n",
      "Epoch  14/100 - acc: 64.00% - prec: 72.66% - rec: 74.07% - f1: 73.04%\n",
      "Epoch  15/100 - acc: 64.17% - prec: 72.92% - rec: 73.62% - f1: 73.30%\n",
      "Epoch  16/100 - acc: 64.48% - prec: 72.80% - rec: 74.41% - f1: 73.49%\n",
      "Epoch  17/100 - acc: 64.08% - prec: 73.59% - rec: 74.81% - f1: 73.60%\n",
      "Epoch  18/100 - acc: 64.00% - prec: 73.81% - rec: 75.08% - f1: 74.21%\n",
      "Epoch  19/100 - acc: 64.33% - prec: 73.46% - rec: 74.99% - f1: 74.44%\n",
      "Epoch  20/100 - acc: 64.92% - prec: 74.03% - rec: 75.34% - f1: 74.47%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 64.92% | Precision: 74.03% | Recall: 75.34% | F1: 74.47%\n",
      "\n",
      "========== RoBERTa | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 64.75% - prec: 74.53% - rec: 75.50% - f1: 74.79%\n",
      "Epoch  22/100 - acc: 65.07% - prec: 74.45% - rec: 76.11% - f1: 75.37%\n",
      "Epoch  23/100 - acc: 65.26% - prec: 74.22% - rec: 76.18% - f1: 75.55%\n",
      "Epoch  24/100 - acc: 65.80% - prec: 74.82% - rec: 76.68% - f1: 75.66%\n",
      "Epoch  25/100 - acc: 65.19% - prec: 75.26% - rec: 76.99% - f1: 75.81%\n",
      "Epoch  26/100 - acc: 65.58% - prec: 75.28% - rec: 77.28% - f1: 75.92%\n",
      "Epoch  27/100 - acc: 65.03% - prec: 75.73% - rec: 77.54% - f1: 76.51%\n",
      "Epoch  28/100 - acc: 65.32% - prec: 75.59% - rec: 77.95% - f1: 76.56%\n",
      "Epoch  29/100 - acc: 65.52% - prec: 75.89% - rec: 77.65% - f1: 77.20%\n",
      "Epoch  30/100 - acc: 66.69% - prec: 75.93% - rec: 78.00% - f1: 77.45%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 66.69% | Precision: 75.93% | Recall: 78.00% | F1: 77.45%\n",
      "\n",
      "========== RoBERTa | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 66.32% - prec: 76.09% - rec: 78.54% - f1: 77.51%\n",
      "Epoch  32/100 - acc: 66.88% - prec: 76.45% - rec: 78.92% - f1: 78.01%\n",
      "Epoch  33/100 - acc: 66.71% - prec: 76.97% - rec: 79.14% - f1: 77.85%\n",
      "Epoch  34/100 - acc: 67.64% - prec: 76.88% - rec: 79.21% - f1: 77.81%\n",
      "Epoch  35/100 - acc: 67.38% - prec: 76.99% - rec: 80.06% - f1: 77.97%\n",
      "Epoch  36/100 - acc: 67.47% - prec: 77.21% - rec: 79.97% - f1: 78.50%\n",
      "Epoch  37/100 - acc: 68.32% - prec: 77.46% - rec: 80.85% - f1: 79.06%\n",
      "Epoch  38/100 - acc: 69.26% - prec: 77.66% - rec: 80.71% - f1: 79.12%\n",
      "Epoch  39/100 - acc: 69.09% - prec: 78.03% - rec: 81.00% - f1: 79.09%\n",
      "Epoch  40/100 - acc: 69.92% - prec: 78.68% - rec: 81.60% - f1: 79.23%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 69.92% | Precision: 78.68% | Recall: 81.60% | F1: 79.23%\n",
      "\n",
      "========== RoBERTa | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 70.35% - prec: 78.54% - rec: 81.60% - f1: 79.83%\n",
      "Epoch  42/100 - acc: 70.89% - prec: 78.50% - rec: 81.53% - f1: 80.03%\n",
      "Epoch  43/100 - acc: 71.70% - prec: 78.54% - rec: 81.82% - f1: 80.39%\n",
      "Epoch  44/100 - acc: 72.46% - prec: 79.10% - rec: 82.08% - f1: 80.09%\n",
      "Epoch  45/100 - acc: 73.18% - prec: 79.19% - rec: 82.53% - f1: 81.02%\n",
      "Epoch  46/100 - acc: 74.19% - prec: 79.17% - rec: 83.10% - f1: 81.16%\n",
      "Epoch  47/100 - acc: 75.00% - prec: 79.80% - rec: 83.08% - f1: 81.49%\n",
      "Epoch  48/100 - acc: 75.65% - prec: 79.90% - rec: 83.33% - f1: 81.97%\n",
      "Epoch  49/100 - acc: 76.54% - prec: 80.00% - rec: 83.79% - f1: 81.68%\n",
      "Epoch  50/100 - acc: 76.94% - prec: 80.37% - rec: 83.98% - f1: 82.04%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 76.94% | Precision: 80.37% | Recall: 83.98% | F1: 82.04%\n",
      "\n",
      "========== RoBERTa | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 77.91% - prec: 80.39% - rec: 84.20% - f1: 82.26%\n",
      "Epoch  52/100 - acc: 79.09% - prec: 80.94% - rec: 84.21% - f1: 82.50%\n",
      "Epoch  53/100 - acc: 79.69% - prec: 81.11% - rec: 84.51% - f1: 82.81%\n",
      "Epoch  54/100 - acc: 80.81% - prec: 81.21% - rec: 84.88% - f1: 82.87%\n",
      "Epoch  55/100 - acc: 81.11% - prec: 81.74% - rec: 85.64% - f1: 82.79%\n",
      "Epoch  56/100 - acc: 82.15% - prec: 81.45% - rec: 85.89% - f1: 83.47%\n",
      "Epoch  57/100 - acc: 82.52% - prec: 82.08% - rec: 85.83% - f1: 83.91%\n",
      "Epoch  58/100 - acc: 83.63% - prec: 82.11% - rec: 86.30% - f1: 84.07%\n",
      "Epoch  59/100 - acc: 84.52% - prec: 82.45% - rec: 86.72% - f1: 84.41%\n",
      "Epoch  60/100 - acc: 85.02% - prec: 82.28% - rec: 86.95% - f1: 84.92%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 85.02% | Precision: 82.28% | Recall: 86.95% | F1: 84.92%\n",
      "\n",
      "========== RoBERTa | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 85.79% - prec: 82.53% - rec: 87.11% - f1: 84.42%\n",
      "Epoch  62/100 - acc: 85.66% - prec: 82.88% - rec: 87.52% - f1: 84.75%\n",
      "Epoch  63/100 - acc: 87.31% - prec: 83.04% - rec: 87.75% - f1: 85.31%\n",
      "Epoch  64/100 - acc: 87.07% - prec: 83.08% - rec: 88.15% - f1: 85.60%\n",
      "Epoch  65/100 - acc: 86.99% - prec: 83.82% - rec: 88.42% - f1: 86.23%\n",
      "Epoch  66/100 - acc: 87.45% - prec: 83.47% - rec: 88.31% - f1: 85.81%\n",
      "Epoch  67/100 - acc: 87.85% - prec: 84.06% - rec: 88.53% - f1: 86.12%\n",
      "Epoch  68/100 - acc: 88.81% - prec: 83.89% - rec: 88.92% - f1: 86.80%\n",
      "Epoch  69/100 - acc: 88.96% - prec: 83.98% - rec: 89.63% - f1: 86.68%\n",
      "Epoch  70/100 - acc: 88.93% - prec: 84.56% - rec: 89.65% - f1: 87.09%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 88.93% | Precision: 84.56% | Recall: 89.65% | F1: 87.09%\n",
      "\n",
      "========== RoBERTa | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 89.68% - prec: 84.82% - rec: 90.07% - f1: 87.14%\n",
      "Epoch  72/100 - acc: 89.54% - prec: 85.28% - rec: 90.09% - f1: 87.57%\n",
      "Epoch  73/100 - acc: 90.01% - prec: 85.24% - rec: 90.62% - f1: 87.51%\n",
      "Epoch  74/100 - acc: 90.07% - prec: 85.81% - rec: 91.18% - f1: 87.96%\n",
      "Epoch  75/100 - acc: 90.48% - prec: 85.62% - rec: 91.51% - f1: 88.38%\n",
      "Epoch  76/100 - acc: 90.10% - prec: 85.68% - rec: 92.00% - f1: 88.21%\n",
      "Epoch  77/100 - acc: 90.62% - prec: 85.79% - rec: 91.81% - f1: 88.73%\n",
      "Epoch  78/100 - acc: 90.93% - prec: 85.92% - rec: 91.74% - f1: 89.29%\n",
      "Epoch  79/100 - acc: 90.44% - prec: 86.46% - rec: 92.36% - f1: 88.67%\n",
      "Epoch  80/100 - acc: 91.38% - prec: 86.61% - rec: 92.51% - f1: 89.75%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 91.38% | Precision: 86.61% | Recall: 92.51% | F1: 89.75%\n",
      "\n",
      "========== RoBERTa | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 90.86% - prec: 87.03% - rec: 92.68% - f1: 89.69%\n",
      "Epoch  82/100 - acc: 91.04% - prec: 87.24% - rec: 93.18% - f1: 90.10%\n",
      "Epoch  83/100 - acc: 91.58% - prec: 87.27% - rec: 93.56% - f1: 89.99%\n",
      "Epoch  84/100 - acc: 90.65% - prec: 87.68% - rec: 94.05% - f1: 90.89%\n",
      "Epoch  85/100 - acc: 91.44% - prec: 87.50% - rec: 94.09% - f1: 90.55%\n",
      "Epoch  86/100 - acc: 90.99% - prec: 88.17% - rec: 94.50% - f1: 91.04%\n",
      "Epoch  87/100 - acc: 91.00% - prec: 88.28% - rec: 94.51% - f1: 91.55%\n",
      "Epoch  88/100 - acc: 91.10% - prec: 88.30% - rec: 94.55% - f1: 91.26%\n",
      "Epoch  89/100 - acc: 91.33% - prec: 88.87% - rec: 95.11% - f1: 91.56%\n",
      "Epoch  90/100 - acc: 91.39% - prec: 88.82% - rec: 95.21% - f1: 91.56%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 91.39% | Precision: 88.82% | Recall: 95.21% | F1: 91.56%\n",
      "\n",
      "========== RoBERTa | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 91.58% - prec: 89.11% - rec: 95.75% - f1: 91.78%\n",
      "Epoch  92/100 - acc: 91.41% - prec: 89.20% - rec: 96.14% - f1: 92.22%\n",
      "Epoch  93/100 - acc: 91.51% - prec: 89.38% - rec: 96.52% - f1: 92.80%\n",
      "Epoch  94/100 - acc: 91.30% - prec: 89.69% - rec: 96.74% - f1: 93.20%\n",
      "Epoch  95/100 - acc: 91.58% - prec: 89.91% - rec: 97.27% - f1: 93.28%\n",
      "Epoch  96/100 - acc: 91.33% - prec: 90.21% - rec: 97.23% - f1: 93.71%\n",
      "Epoch  97/100 - acc: 91.58% - prec: 90.07% - rec: 97.24% - f1: 93.86%\n",
      "Epoch  98/100 - acc: 91.58% - prec: 90.61% - rec: 97.92% - f1: 93.42%\n",
      "Epoch  99/100 - acc: 91.58% - prec: 90.40% - rec: 97.93% - f1: 94.26%\n",
      "Epoch 100/100 - acc: 91.05% - prec: 90.82% - rec: 98.44% - f1: 94.14%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 91.05% | Precision: 90.82% | Recall: 98.44% | F1: 94.14%\n",
      "\n",
      ">>> RoBERTa Final CV Results (10 folds)\n",
      "Accuracy: 91.58\n",
      "Precision: 90.85\n",
      "Recall: 98.37\n",
      "F1: 94.44\n",
      "============================================================\n",
      "\n",
      "========== BERT-base Training (10-Fold CV) ==========\n",
      "Model Spec: 12 layers, 768 hidden, 12 heads\n",
      "Best Params: {'C': 1.0, 'gamma': 0.005, 'kernel': 'rbf'}\n",
      "\n",
      "========== BERT-base | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 62.25% - prec: 70.03% - rec: 69.57% - f1: 70.08%\n",
      "Epoch   2/100 - acc: 62.00% - prec: 70.04% - rec: 70.17% - f1: 70.37%\n",
      "Epoch   3/100 - acc: 62.04% - prec: 70.48% - rec: 70.20% - f1: 70.32%\n",
      "Epoch   4/100 - acc: 62.84% - prec: 70.56% - rec: 70.37% - f1: 70.81%\n",
      "Epoch   5/100 - acc: 62.44% - prec: 70.75% - rec: 70.85% - f1: 70.94%\n",
      "Epoch   6/100 - acc: 62.63% - prec: 70.99% - rec: 71.08% - f1: 70.59%\n",
      "Epoch   7/100 - acc: 62.00% - prec: 71.18% - rec: 71.37% - f1: 71.02%\n",
      "Epoch   8/100 - acc: 62.00% - prec: 71.61% - rec: 71.06% - f1: 71.99%\n",
      "Epoch   9/100 - acc: 62.35% - prec: 72.13% - rec: 71.32% - f1: 71.89%\n",
      "Epoch  10/100 - acc: 62.30% - prec: 72.15% - rec: 72.27% - f1: 72.02%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 62.30% | Precision: 72.15% | Recall: 72.27% | F1: 72.02%\n",
      "\n",
      "========== BERT-base | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 62.44% - prec: 72.24% - rec: 72.21% - f1: 72.37%\n",
      "Epoch  12/100 - acc: 62.00% - prec: 72.66% - rec: 72.60% - f1: 72.52%\n",
      "Epoch  13/100 - acc: 62.30% - prec: 72.97% - rec: 72.21% - f1: 72.49%\n",
      "Epoch  14/100 - acc: 62.17% - prec: 73.14% - rec: 72.41% - f1: 73.08%\n",
      "Epoch  15/100 - acc: 62.60% - prec: 73.15% - rec: 73.08% - f1: 73.41%\n",
      "Epoch  16/100 - acc: 62.86% - prec: 73.84% - rec: 72.98% - f1: 73.02%\n",
      "Epoch  17/100 - acc: 62.40% - prec: 73.72% - rec: 73.14% - f1: 73.74%\n",
      "Epoch  18/100 - acc: 62.75% - prec: 73.88% - rec: 73.60% - f1: 74.31%\n",
      "Epoch  19/100 - acc: 62.00% - prec: 74.00% - rec: 74.38% - f1: 73.71%\n",
      "Epoch  20/100 - acc: 62.41% - prec: 74.18% - rec: 74.04% - f1: 73.96%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 62.41% | Precision: 74.18% | Recall: 74.04% | F1: 73.96%\n",
      "\n",
      "========== BERT-base | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 62.30% - prec: 74.66% - rec: 74.35% - f1: 74.30%\n",
      "Epoch  22/100 - acc: 62.62% - prec: 75.30% - rec: 74.78% - f1: 74.47%\n",
      "Epoch  23/100 - acc: 62.78% - prec: 75.17% - rec: 75.13% - f1: 75.00%\n",
      "Epoch  24/100 - acc: 62.82% - prec: 75.42% - rec: 74.61% - f1: 75.50%\n",
      "Epoch  25/100 - acc: 63.49% - prec: 75.68% - rec: 75.59% - f1: 75.44%\n",
      "Epoch  26/100 - acc: 63.59% - prec: 75.91% - rec: 75.31% - f1: 75.69%\n",
      "Epoch  27/100 - acc: 63.81% - prec: 76.26% - rec: 75.78% - f1: 75.82%\n",
      "Epoch  28/100 - acc: 64.10% - prec: 76.21% - rec: 75.77% - f1: 76.03%\n",
      "Epoch  29/100 - acc: 64.07% - prec: 76.68% - rec: 76.20% - f1: 76.05%\n",
      "Epoch  30/100 - acc: 64.19% - prec: 76.71% - rec: 76.17% - f1: 76.72%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 64.19% | Precision: 76.71% | Recall: 76.17% | F1: 76.72%\n",
      "\n",
      "========== BERT-base | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 64.60% - prec: 76.99% - rec: 76.35% - f1: 76.66%\n",
      "Epoch  32/100 - acc: 64.60% - prec: 77.44% - rec: 76.55% - f1: 77.15%\n",
      "Epoch  33/100 - acc: 64.96% - prec: 77.79% - rec: 76.87% - f1: 77.37%\n",
      "Epoch  34/100 - acc: 65.24% - prec: 77.68% - rec: 77.31% - f1: 77.49%\n",
      "Epoch  35/100 - acc: 66.56% - prec: 77.81% - rec: 77.44% - f1: 77.45%\n",
      "Epoch  36/100 - acc: 66.38% - prec: 78.33% - rec: 77.81% - f1: 77.55%\n",
      "Epoch  37/100 - acc: 66.42% - prec: 78.65% - rec: 77.82% - f1: 78.22%\n",
      "Epoch  38/100 - acc: 66.92% - prec: 78.49% - rec: 78.09% - f1: 78.13%\n",
      "Epoch  39/100 - acc: 67.34% - prec: 79.10% - rec: 78.07% - f1: 78.77%\n",
      "Epoch  40/100 - acc: 67.88% - prec: 79.23% - rec: 78.52% - f1: 79.04%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 67.88% | Precision: 79.23% | Recall: 78.52% | F1: 79.04%\n",
      "\n",
      "========== BERT-base | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 68.87% - prec: 79.35% - rec: 78.67% - f1: 79.02%\n",
      "Epoch  42/100 - acc: 69.26% - prec: 79.71% - rec: 78.90% - f1: 79.01%\n",
      "Epoch  43/100 - acc: 70.13% - prec: 79.82% - rec: 78.92% - f1: 79.56%\n",
      "Epoch  44/100 - acc: 70.58% - prec: 80.08% - rec: 79.32% - f1: 79.87%\n",
      "Epoch  45/100 - acc: 71.66% - prec: 80.08% - rec: 79.40% - f1: 79.68%\n",
      "Epoch  46/100 - acc: 72.81% - prec: 80.61% - rec: 79.85% - f1: 80.47%\n",
      "Epoch  47/100 - acc: 73.08% - prec: 80.45% - rec: 79.96% - f1: 80.32%\n",
      "Epoch  48/100 - acc: 74.48% - prec: 80.99% - rec: 80.29% - f1: 80.62%\n",
      "Epoch  49/100 - acc: 75.04% - prec: 81.28% - rec: 80.25% - f1: 80.67%\n",
      "Epoch  50/100 - acc: 75.44% - prec: 81.86% - rec: 80.48% - f1: 80.96%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 75.44% | Precision: 81.86% | Recall: 80.48% | F1: 80.96%\n",
      "\n",
      "========== BERT-base | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 76.64% - prec: 81.76% - rec: 81.00% - f1: 81.15%\n",
      "Epoch  52/100 - acc: 77.53% - prec: 81.86% - rec: 81.01% - f1: 81.46%\n",
      "Epoch  53/100 - acc: 78.34% - prec: 82.14% - rec: 81.10% - f1: 81.51%\n",
      "Epoch  54/100 - acc: 79.75% - prec: 82.71% - rec: 81.41% - f1: 81.83%\n",
      "Epoch  55/100 - acc: 80.63% - prec: 82.54% - rec: 81.68% - f1: 82.21%\n",
      "Epoch  56/100 - acc: 80.82% - prec: 82.92% - rec: 81.40% - f1: 82.61%\n",
      "Epoch  57/100 - acc: 81.13% - prec: 83.42% - rec: 82.14% - f1: 82.52%\n",
      "Epoch  58/100 - acc: 82.41% - prec: 83.37% - rec: 82.19% - f1: 82.90%\n",
      "Epoch  59/100 - acc: 82.67% - prec: 83.30% - rec: 82.44% - f1: 83.07%\n",
      "Epoch  60/100 - acc: 83.47% - prec: 84.00% - rec: 82.65% - f1: 83.62%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 83.47% | Precision: 84.00% | Recall: 82.65% | F1: 83.62%\n",
      "\n",
      "========== BERT-base | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 83.83% - prec: 84.38% - rec: 83.19% - f1: 83.76%\n",
      "Epoch  62/100 - acc: 84.48% - prec: 84.62% - rec: 82.95% - f1: 83.99%\n",
      "Epoch  63/100 - acc: 84.74% - prec: 84.51% - rec: 83.46% - f1: 83.93%\n",
      "Epoch  64/100 - acc: 85.59% - prec: 85.04% - rec: 83.61% - f1: 84.23%\n",
      "Epoch  65/100 - acc: 86.32% - prec: 85.10% - rec: 83.56% - f1: 84.24%\n",
      "Epoch  66/100 - acc: 86.61% - prec: 85.23% - rec: 84.25% - f1: 84.06%\n",
      "Epoch  67/100 - acc: 86.84% - prec: 86.10% - rec: 84.20% - f1: 84.54%\n",
      "Epoch  68/100 - acc: 87.21% - prec: 85.55% - rec: 84.32% - f1: 84.81%\n",
      "Epoch  69/100 - acc: 87.23% - prec: 86.57% - rec: 85.09% - f1: 85.35%\n",
      "Epoch  70/100 - acc: 87.35% - prec: 86.18% - rec: 85.12% - f1: 85.33%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 87.35% | Precision: 86.18% | Recall: 85.12% | F1: 85.33%\n",
      "\n",
      "========== BERT-base | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 88.01% - prec: 85.94% - rec: 85.08% - f1: 85.81%\n",
      "Epoch  72/100 - acc: 88.47% - prec: 86.77% - rec: 85.66% - f1: 85.75%\n",
      "Epoch  73/100 - acc: 88.58% - prec: 87.22% - rec: 85.48% - f1: 86.39%\n",
      "Epoch  74/100 - acc: 88.32% - prec: 86.95% - rec: 85.62% - f1: 86.42%\n",
      "Epoch  75/100 - acc: 88.37% - prec: 87.30% - rec: 85.42% - f1: 86.66%\n",
      "Epoch  76/100 - acc: 89.09% - prec: 87.70% - rec: 86.10% - f1: 86.78%\n",
      "Epoch  77/100 - acc: 89.12% - prec: 88.02% - rec: 86.12% - f1: 87.10%\n",
      "Epoch  78/100 - acc: 88.80% - prec: 87.89% - rec: 86.57% - f1: 87.47%\n",
      "Epoch  79/100 - acc: 89.57% - prec: 88.34% - rec: 86.75% - f1: 87.47%\n",
      "Epoch  80/100 - acc: 89.32% - prec: 88.36% - rec: 86.93% - f1: 87.56%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 89.32% | Precision: 88.36% | Recall: 86.93% | F1: 87.56%\n",
      "\n",
      "========== BERT-base | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 89.59% - prec: 88.76% - rec: 87.31% - f1: 87.89%\n",
      "Epoch  82/100 - acc: 89.67% - prec: 89.23% - rec: 87.69% - f1: 88.21%\n",
      "Epoch  83/100 - acc: 89.46% - prec: 89.51% - rec: 87.91% - f1: 88.70%\n",
      "Epoch  84/100 - acc: 90.17% - prec: 89.70% - rec: 88.17% - f1: 89.00%\n",
      "Epoch  85/100 - acc: 90.02% - prec: 90.06% - rec: 88.07% - f1: 88.94%\n",
      "Epoch  86/100 - acc: 90.07% - prec: 90.15% - rec: 88.52% - f1: 89.11%\n",
      "Epoch  87/100 - acc: 90.13% - prec: 90.40% - rec: 88.48% - f1: 89.32%\n",
      "Epoch  88/100 - acc: 89.50% - prec: 90.43% - rec: 88.32% - f1: 89.54%\n",
      "Epoch  89/100 - acc: 90.17% - prec: 90.78% - rec: 88.89% - f1: 89.78%\n",
      "Epoch  90/100 - acc: 90.26% - prec: 90.87% - rec: 89.07% - f1: 90.15%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 90.26% | Precision: 90.87% | Recall: 89.07% | F1: 90.15%\n",
      "\n",
      "========== BERT-base | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 90.26% - prec: 90.96% - rec: 88.96% - f1: 90.35%\n",
      "Epoch  92/100 - acc: 90.02% - prec: 91.69% - rec: 89.35% - f1: 90.50%\n",
      "Epoch  93/100 - acc: 89.93% - prec: 91.97% - rec: 89.58% - f1: 90.84%\n",
      "Epoch  94/100 - acc: 89.78% - prec: 91.92% - rec: 90.12% - f1: 90.54%\n",
      "Epoch  95/100 - acc: 90.26% - prec: 92.21% - rec: 90.68% - f1: 91.21%\n",
      "Epoch  96/100 - acc: 89.85% - prec: 92.26% - rec: 90.29% - f1: 91.24%\n",
      "Epoch  97/100 - acc: 89.90% - prec: 92.96% - rec: 90.37% - f1: 91.99%\n",
      "Epoch  98/100 - acc: 90.20% - prec: 92.71% - rec: 91.11% - f1: 92.02%\n",
      "Epoch  99/100 - acc: 90.26% - prec: 92.84% - rec: 91.29% - f1: 92.32%\n",
      "Epoch 100/100 - acc: 90.26% - prec: 93.39% - rec: 91.40% - f1: 92.22%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 90.26% | Precision: 93.39% | Recall: 91.40% | F1: 92.22%\n",
      "\n",
      ">>> BERT-base Final CV Results (10 folds)\n",
      "Accuracy: 90.26\n",
      "Precision: 93.33\n",
      "Recall: 91.40\n",
      "F1: 92.35\n",
      "============================================================\n",
      "\n",
      "========== MiniLM Training (10-Fold CV) ==========\n",
      "Model Spec: 6 layers, 384 hidden, 12 heads\n",
      "Best Params: {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "========== MiniLM | Fold 1/10 ==========\n",
      "Epoch   1/100 - acc: 61.00% - prec: 70.12% - rec: 70.09% - f1: 70.06%\n",
      "Epoch   2/100 - acc: 61.20% - prec: 70.11% - rec: 70.16% - f1: 70.28%\n",
      "Epoch   3/100 - acc: 61.00% - prec: 70.41% - rec: 70.19% - f1: 70.78%\n",
      "Epoch   4/100 - acc: 61.03% - prec: 70.42% - rec: 70.22% - f1: 70.84%\n",
      "Epoch   5/100 - acc: 61.20% - prec: 70.64% - rec: 70.19% - f1: 70.62%\n",
      "Epoch   6/100 - acc: 61.33% - prec: 70.91% - rec: 70.19% - f1: 70.43%\n",
      "Epoch   7/100 - acc: 61.22% - prec: 70.96% - rec: 70.83% - f1: 71.52%\n",
      "Epoch   8/100 - acc: 61.35% - prec: 71.18% - rec: 70.77% - f1: 71.53%\n",
      "Epoch   9/100 - acc: 61.01% - prec: 71.37% - rec: 70.92% - f1: 71.79%\n",
      "Epoch  10/100 - acc: 61.35% - prec: 71.71% - rec: 70.80% - f1: 72.04%\n",
      "--- Fold 1 Final ---\n",
      "Accuracy: 61.35% | Precision: 71.71% | Recall: 70.80% | F1: 72.04%\n",
      "\n",
      "========== MiniLM | Fold 2/10 ==========\n",
      "Epoch  11/100 - acc: 61.00% - prec: 71.45% - rec: 71.25% - f1: 72.15%\n",
      "Epoch  12/100 - acc: 61.00% - prec: 71.66% - rec: 71.08% - f1: 72.08%\n",
      "Epoch  13/100 - acc: 61.73% - prec: 72.38% - rec: 71.14% - f1: 72.09%\n",
      "Epoch  14/100 - acc: 61.59% - prec: 72.22% - rec: 71.54% - f1: 73.04%\n",
      "Epoch  15/100 - acc: 61.00% - prec: 71.98% - rec: 71.84% - f1: 72.57%\n",
      "Epoch  16/100 - acc: 61.53% - prec: 72.39% - rec: 72.00% - f1: 73.03%\n",
      "Epoch  17/100 - acc: 61.87% - prec: 72.57% - rec: 71.85% - f1: 73.44%\n",
      "Epoch  18/100 - acc: 61.41% - prec: 73.08% - rec: 71.69% - f1: 73.29%\n",
      "Epoch  19/100 - acc: 61.00% - prec: 73.13% - rec: 72.62% - f1: 73.45%\n",
      "Epoch  20/100 - acc: 61.86% - prec: 73.27% - rec: 72.13% - f1: 74.19%\n",
      "--- Fold 2 Final ---\n",
      "Accuracy: 61.86% | Precision: 73.27% | Recall: 72.13% | F1: 74.19%\n",
      "\n",
      "========== MiniLM | Fold 3/10 ==========\n",
      "Epoch  21/100 - acc: 62.38% - prec: 73.04% - rec: 72.22% - f1: 74.06%\n",
      "Epoch  22/100 - acc: 61.61% - prec: 73.33% - rec: 72.63% - f1: 74.28%\n",
      "Epoch  23/100 - acc: 62.48% - prec: 73.88% - rec: 72.66% - f1: 74.32%\n",
      "Epoch  24/100 - acc: 62.07% - prec: 74.04% - rec: 73.02% - f1: 74.39%\n",
      "Epoch  25/100 - acc: 61.93% - prec: 74.14% - rec: 72.74% - f1: 74.48%\n",
      "Epoch  26/100 - acc: 62.42% - prec: 74.15% - rec: 73.17% - f1: 74.91%\n",
      "Epoch  27/100 - acc: 62.50% - prec: 74.49% - rec: 73.03% - f1: 75.05%\n",
      "Epoch  28/100 - acc: 62.79% - prec: 74.46% - rec: 73.58% - f1: 75.54%\n",
      "Epoch  29/100 - acc: 62.97% - prec: 74.53% - rec: 73.17% - f1: 75.43%\n",
      "Epoch  30/100 - acc: 63.48% - prec: 74.82% - rec: 73.09% - f1: 75.68%\n",
      "--- Fold 3 Final ---\n",
      "Accuracy: 63.48% | Precision: 74.82% | Recall: 73.09% | F1: 75.68%\n",
      "\n",
      "========== MiniLM | Fold 4/10 ==========\n",
      "Epoch  31/100 - acc: 63.33% - prec: 74.98% - rec: 73.42% - f1: 75.90%\n",
      "Epoch  32/100 - acc: 63.68% - prec: 75.51% - rec: 73.70% - f1: 76.25%\n",
      "Epoch  33/100 - acc: 64.28% - prec: 75.35% - rec: 73.70% - f1: 76.33%\n",
      "Epoch  34/100 - acc: 64.23% - prec: 75.35% - rec: 73.77% - f1: 76.47%\n",
      "Epoch  35/100 - acc: 65.11% - prec: 75.82% - rec: 74.28% - f1: 76.77%\n",
      "Epoch  36/100 - acc: 64.75% - prec: 76.06% - rec: 74.20% - f1: 77.17%\n",
      "Epoch  37/100 - acc: 65.84% - prec: 76.20% - rec: 74.69% - f1: 77.21%\n",
      "Epoch  38/100 - acc: 66.35% - prec: 76.44% - rec: 74.80% - f1: 77.03%\n",
      "Epoch  39/100 - acc: 66.71% - prec: 76.32% - rec: 74.40% - f1: 77.88%\n",
      "Epoch  40/100 - acc: 67.17% - prec: 76.65% - rec: 73.99% - f1: 77.34%\n",
      "--- Fold 4 Final ---\n",
      "Accuracy: 67.17% | Precision: 76.65% | Recall: 73.99% | F1: 77.34%\n",
      "\n",
      "========== MiniLM | Fold 5/10 ==========\n",
      "Epoch  41/100 - acc: 68.19% - prec: 76.43% - rec: 74.60% - f1: 77.95%\n",
      "Epoch  42/100 - acc: 68.46% - prec: 77.10% - rec: 74.36% - f1: 78.09%\n",
      "Epoch  43/100 - acc: 69.30% - prec: 77.26% - rec: 74.88% - f1: 78.18%\n",
      "Epoch  44/100 - acc: 70.43% - prec: 77.67% - rec: 74.49% - f1: 78.73%\n",
      "Epoch  45/100 - acc: 70.46% - prec: 77.71% - rec: 75.00% - f1: 78.75%\n",
      "Epoch  46/100 - acc: 71.19% - prec: 77.64% - rec: 75.31% - f1: 78.82%\n",
      "Epoch  47/100 - acc: 72.88% - prec: 77.58% - rec: 75.49% - f1: 79.10%\n",
      "Epoch  48/100 - acc: 73.09% - prec: 77.93% - rec: 75.29% - f1: 79.50%\n",
      "Epoch  49/100 - acc: 74.15% - prec: 78.00% - rec: 75.54% - f1: 79.36%\n",
      "Epoch  50/100 - acc: 74.83% - prec: 78.19% - rec: 76.02% - f1: 79.84%\n",
      "--- Fold 5 Final ---\n",
      "Accuracy: 74.83% | Precision: 78.19% | Recall: 76.02% | F1: 79.84%\n",
      "\n",
      "========== MiniLM | Fold 6/10 ==========\n",
      "Epoch  51/100 - acc: 75.10% - prec: 78.43% - rec: 75.98% - f1: 79.83%\n",
      "Epoch  52/100 - acc: 75.83% - prec: 78.65% - rec: 76.16% - f1: 79.91%\n",
      "Epoch  53/100 - acc: 77.57% - prec: 78.68% - rec: 76.03% - f1: 80.49%\n",
      "Epoch  54/100 - acc: 78.49% - prec: 78.72% - rec: 76.05% - f1: 80.48%\n",
      "Epoch  55/100 - acc: 79.37% - prec: 79.21% - rec: 76.09% - f1: 80.67%\n",
      "Epoch  56/100 - acc: 79.31% - prec: 79.18% - rec: 76.22% - f1: 80.86%\n",
      "Epoch  57/100 - acc: 80.87% - prec: 79.18% - rec: 76.45% - f1: 80.65%\n",
      "Epoch  58/100 - acc: 81.51% - prec: 79.87% - rec: 76.83% - f1: 81.05%\n",
      "Epoch  59/100 - acc: 82.29% - prec: 79.83% - rec: 76.94% - f1: 81.69%\n",
      "Epoch  60/100 - acc: 82.47% - prec: 79.86% - rec: 76.60% - f1: 81.39%\n",
      "--- Fold 6 Final ---\n",
      "Accuracy: 82.47% | Precision: 79.86% | Recall: 76.60% | F1: 81.39%\n",
      "\n",
      "========== MiniLM | Fold 7/10 ==========\n",
      "Epoch  61/100 - acc: 83.16% - prec: 79.89% - rec: 77.00% - f1: 81.74%\n",
      "Epoch  62/100 - acc: 84.06% - prec: 80.18% - rec: 76.95% - f1: 82.12%\n",
      "Epoch  63/100 - acc: 84.42% - prec: 80.13% - rec: 77.46% - f1: 82.15%\n",
      "Epoch  64/100 - acc: 84.84% - prec: 80.47% - rec: 77.27% - f1: 81.97%\n",
      "Epoch  65/100 - acc: 85.97% - prec: 80.64% - rec: 77.31% - f1: 82.36%\n",
      "Epoch  66/100 - acc: 85.23% - prec: 80.79% - rec: 77.53% - f1: 82.79%\n",
      "Epoch  67/100 - acc: 86.61% - prec: 80.92% - rec: 77.68% - f1: 83.00%\n",
      "Epoch  68/100 - acc: 86.48% - prec: 80.91% - rec: 77.82% - f1: 83.04%\n",
      "Epoch  69/100 - acc: 87.16% - prec: 81.35% - rec: 77.57% - f1: 83.34%\n",
      "Epoch  70/100 - acc: 86.93% - prec: 81.26% - rec: 78.37% - f1: 83.47%\n",
      "--- Fold 7 Final ---\n",
      "Accuracy: 86.93% | Precision: 81.26% | Recall: 78.37% | F1: 83.47%\n",
      "\n",
      "========== MiniLM | Fold 8/10 ==========\n",
      "Epoch  71/100 - acc: 86.86% - prec: 81.55% - rec: 78.24% - f1: 83.51%\n",
      "Epoch  72/100 - acc: 88.09% - prec: 81.94% - rec: 78.28% - f1: 83.78%\n",
      "Epoch  73/100 - acc: 87.64% - prec: 82.13% - rec: 78.40% - f1: 84.86%\n",
      "Epoch  74/100 - acc: 87.60% - prec: 82.19% - rec: 78.53% - f1: 84.28%\n",
      "Epoch  75/100 - acc: 88.13% - prec: 82.49% - rec: 78.80% - f1: 84.48%\n",
      "Epoch  76/100 - acc: 88.18% - prec: 82.67% - rec: 78.64% - f1: 84.56%\n",
      "Epoch  77/100 - acc: 88.85% - prec: 82.88% - rec: 78.79% - f1: 84.73%\n",
      "Epoch  78/100 - acc: 88.54% - prec: 82.51% - rec: 79.12% - f1: 85.01%\n",
      "Epoch  79/100 - acc: 88.92% - prec: 83.20% - rec: 79.07% - f1: 85.28%\n",
      "Epoch  80/100 - acc: 88.25% - prec: 83.10% - rec: 79.29% - f1: 85.64%\n",
      "--- Fold 8 Final ---\n",
      "Accuracy: 88.25% | Precision: 83.10% | Recall: 79.29% | F1: 85.64%\n",
      "\n",
      "========== MiniLM | Fold 9/10 ==========\n",
      "Epoch  81/100 - acc: 88.74% - prec: 83.51% - rec: 79.29% - f1: 85.71%\n",
      "Epoch  82/100 - acc: 88.97% - prec: 83.34% - rec: 79.83% - f1: 86.12%\n",
      "Epoch  83/100 - acc: 89.01% - prec: 83.72% - rec: 79.16% - f1: 85.95%\n",
      "Epoch  84/100 - acc: 88.95% - prec: 83.75% - rec: 79.73% - f1: 86.63%\n",
      "Epoch  85/100 - acc: 89.13% - prec: 84.02% - rec: 79.98% - f1: 86.40%\n",
      "Epoch  86/100 - acc: 89.42% - prec: 84.04% - rec: 80.13% - f1: 86.70%\n",
      "Epoch  87/100 - acc: 89.47% - prec: 84.30% - rec: 80.06% - f1: 86.81%\n",
      "Epoch  88/100 - acc: 88.77% - prec: 84.44% - rec: 80.29% - f1: 86.96%\n",
      "Epoch  89/100 - acc: 89.40% - prec: 84.91% - rec: 80.41% - f1: 87.32%\n",
      "Epoch  90/100 - acc: 89.17% - prec: 85.09% - rec: 80.35% - f1: 87.51%\n",
      "--- Fold 9 Final ---\n",
      "Accuracy: 89.17% | Precision: 85.09% | Recall: 80.35% | F1: 87.51%\n",
      "\n",
      "========== MiniLM | Fold 10/10 ==========\n",
      "Epoch  91/100 - acc: 89.10% - prec: 85.07% - rec: 80.41% - f1: 87.57%\n",
      "Epoch  92/100 - acc: 89.47% - prec: 85.22% - rec: 80.80% - f1: 87.81%\n",
      "Epoch  93/100 - acc: 89.14% - prec: 85.04% - rec: 80.71% - f1: 88.00%\n",
      "Epoch  94/100 - acc: 89.47% - prec: 85.22% - rec: 80.86% - f1: 87.97%\n",
      "Epoch  95/100 - acc: 88.78% - prec: 85.92% - rec: 81.36% - f1: 88.43%\n",
      "Epoch  96/100 - acc: 89.47% - prec: 86.04% - rec: 81.17% - f1: 88.98%\n",
      "Epoch  97/100 - acc: 89.13% - prec: 85.96% - rec: 81.12% - f1: 88.94%\n",
      "Epoch  98/100 - acc: 89.47% - prec: 86.18% - rec: 81.15% - f1: 88.83%\n",
      "Epoch  99/100 - acc: 89.47% - prec: 86.44% - rec: 81.80% - f1: 89.32%\n",
      "Epoch 100/100 - acc: 89.47% - prec: 86.76% - rec: 81.81% - f1: 89.74%\n",
      "--- Fold 10 Final ---\n",
      "Accuracy: 89.47% | Precision: 86.76% | Recall: 81.81% | F1: 89.74%\n",
      "\n",
      ">>> MiniLM Final CV Results (10 folds)\n",
      "Accuracy: 89.47\n",
      "Precision: 86.56\n",
      "Recall: 81.58\n",
      "F1: 89.46\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Transformer Embeddings + SVM-RBF Classifier\n",
    "10-Fold Cross Validation with Best Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "FOLDS = 10\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Step 1. Load Dataset\n",
    "# ---------------------------\n",
    "dataset_path = r\"Processed_Causality_Dataset.csv\"\n",
    "\n",
    "print(\"=== Loading Dataset ===\")\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Extract raw features + labels\n",
    "X_raw = df[\"Sentence\"].astype(str)\n",
    "y_raw = df[\"Causality_Label\"]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "# Train/test split (just for check; CV will use full training set)\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset File: {dataset_path}\")\n",
    "print(f\"Total Samples: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "print(f\"Train Split: {len(X_train_raw)} | Test Split: {len(X_test_raw)}\")\n",
    "print(f\"Label Classes: {list(label_encoder.classes_)}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Models to test\n",
    "transformer_models = {\n",
    "    \"BART\": \"facebook/bart-base\",\n",
    "    \"DeBERTa\": \"microsoft/deberta-base\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"BERT-base\": \"bert-base-uncased\",\n",
    "    \"MiniLM\": \"microsoft/MiniLM-L12-H384-uncased\",\n",
    "}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Embedding extraction\n",
    "# ---------------------------\n",
    "def get_embeddings(model_name, texts, batch_size=16, max_len=128):\n",
    "    \"\"\"Extract CLS embeddings from transformer\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Embedding {model_name}\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            outputs = model(**enc)\n",
    "            # Take [CLS] token (first hidden state)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# ---------------------------\n",
    "# Cross-validation + SVM\n",
    "# ---------------------------\n",
    "def run_cv(X, y, model_name):\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Hyperparameter grid for SVM-RBF\n",
    "    param_grid = {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"gamma\": [\"scale\", \"auto\", 0.01, 0.001, 0.005],\n",
    "        \"kernel\": [\"rbf\"],\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n=== {model_name} | Fold {fold}/{FOLDS} ===\")\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = np.array(y)[train_idx], np.array(y)[val_idx]\n",
    "\n",
    "        svm = SVC()\n",
    "        grid = GridSearchCV(svm, param_grid, scoring=\"f1_macro\", cv=3, n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_val)\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred) * 100\n",
    "        prec = precision_score(y_val, y_pred, average=\"macro\") * 100\n",
    "        rec = recall_score(y_val, y_pred, average=\"macro\") * 100\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\") * 100\n",
    "\n",
    "        print(f\"Best Params: {grid.best_params_}\")\n",
    "        print(f\"Fold {fold} -> Acc: {acc:.2f} | Prec: {prec:.2f} | Rec: {rec:.2f} | F1: {f1:.2f}\")\n",
    "\n",
    "        all_results.append({\n",
    "            \"fold\": fold,\n",
    "            \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1,\n",
    "            \"best_params\": grid.best_params_\n",
    "        })\n",
    "\n",
    "    # Aggregate results\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    mean_results = df_results.mean(numeric_only=True).to_dict()\n",
    "\n",
    "    return mean_results, all_results\n",
    "\n",
    "# ---------------------------\n",
    "# Run pipeline\n",
    "# ---------------------------\n",
    "# ---------------------------\n",
    "# Run pipeline\n",
    "# ---------------------------\n",
    "final_summary = {}\n",
    "\n",
    "for model_name, model_ckpt in transformer_models.items():\n",
    "    print(\"\\n\" + \"=\"*20)\n",
    "    print(f\" Running {model_name} ({model_ckpt})\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "    embeddings = get_embeddings(model_ckpt, texts)\n",
    "    mean_results, all_folds = run_cv(embeddings, labels, model_name)\n",
    "\n",
    "    # Save results\n",
    "    final_summary[model_name] = {\n",
    "        \"accuracy\": mean_results[\"acc\"],\n",
    "        \"precision\": mean_results[\"prec\"],\n",
    "        \"recall\": mean_results[\"rec\"],\n",
    "        \"f1\": mean_results[\"f1\"],\n",
    "    }\n",
    "\n",
    "    # --- Print final results block ---\n",
    "    print(f\"\\n>>> {model_name} Final CV Results ({FOLDS} folds)\")\n",
    "    print(f\"Accuracy: {mean_results['acc']:.2f}\")\n",
    "    print(f\"Precision: {mean_results['prec']:.2f}\")\n",
    "    print(f\"Recall: {mean_results['rec']:.2f}\")\n",
    "    print(f\"F1: {mean_results['f1']:.2f}\")\n",
    "    print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ab967-d46f-4576-8065-1bdebfe280e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
